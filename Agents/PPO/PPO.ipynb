{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def init_layer(layer, gain = np.sqrt(2)):\n",
    "  nn.init.orthogonal_(layer.weight, gain)\n",
    "  nn.init.constant_(layer.bias, 0)\n",
    "  return layer\n",
    "\n",
    "# Policy network (MLP)\n",
    "class MLPPolicyNetwork(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super(MLPPolicyNetwork, self).__init__()\n",
    "\n",
    "        # Actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            init_layer(nn.Linear(in_states, h1_nodes)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(h1_nodes, h1_nodes)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(h1_nodes, out_actions), gain = 0.01)\n",
    "        )\n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            init_layer(nn.Linear(in_states, h1_nodes)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(h1_nodes, h1_nodes)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(h1_nodes, 1), gain = 1.0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = x.flatten(start_dim=1)\n",
    "      logits = self.actor(x)\n",
    "      value = self.critic(x)\n",
    "      return logits, value\n",
    "\n",
    "\n",
    "class CNNPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CNNPolicyNetwork, self).__init__()\n",
    "        stack, height, width = input_shape\n",
    "\n",
    "        self.shared_conv = nn.Sequential(\n",
    "            nn.Conv2d(stack, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test = torch.zeros(1, stack, height, width)\n",
    "            find_conv_size = self.shared_conv(test)\n",
    "            conv_size = find_conv_size.numel()\n",
    "\n",
    "        self.actor_fc = init_layer(nn.Linear(conv_size, num_actions), gain = 0.01)\n",
    "        self.critic_fc = init_layer(nn.Linear(conv_size, 1), gain = 1.)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "      feats = self.shared_conv(x)\n",
    "      feats = torch.flatten(feats, start_dim=1)\n",
    "\n",
    "      logits = self.actor_fc(feats)\n",
    "      value = self.critic_fc(feats)\n",
    "\n",
    "      return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData:\n",
    "  def __init__(self):\n",
    "    self.states = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "    self.values = []\n",
    "    self.next_states = []\n",
    "    self.dones = []\n",
    "    self.log_probs = []\n",
    "\n",
    "  def clear(self):\n",
    "    self.__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from metrics import Metrics\n",
    "\n",
    "class PPOAgent():\n",
    "  def __init__(self, params):\n",
    "\n",
    "    self.device = params.get(\"device\", torch.device(\"cpu\"))\n",
    "    self.policy = params.get(\"policy\", \"CnnPolicy\")\n",
    "    self.params = params\n",
    "\n",
    "    # Learn\n",
    "    self.num_iterations = params.get(\"num_iterations\", 200)\n",
    "\n",
    "    # Collect data\n",
    "    self.num_steps = params.get(\"num_steps\", 20)\n",
    "    self.step_reward = params.get(\"step_reward\", 0.0)\n",
    "\n",
    "    # GAE\n",
    "    self.gamma = params.get(\"gamma\", 0.99)\n",
    "    self.lamda = params.get(\"lamda\", 0.95)\n",
    "\n",
    "\n",
    "    # Update Policy\n",
    "    self.clip_epsilon = params.get(\"clip_epsilon\", 0.2)\n",
    "    self.loss_coeff = params.get(\"loss_coeff\", 0.5)\n",
    "\n",
    "    self.epochs = params.get(\"epochs\", 5)\n",
    "    self.batch_size = params.get(\"batch_size\", 32)\n",
    "\n",
    "    self.learning_rate = params.get(\"learning_rate\", 3e-4)\n",
    "    self.discount = params.get(\"discount\", 0.2) \n",
    "    self.entropy_coeff = params.get(\"entropy_coeff\", 0.01)\n",
    "\n",
    "    self.to_save_model = params.get(\"save_model\", False)\n",
    "\n",
    "    self.policy_net = None\n",
    "    self.num_actions = None\n",
    "    self.train_data = TrainData()\n",
    "\n",
    "    # Metrics\n",
    "    use_metrics = params.get(\"use_metrics\", False)\n",
    "    save_params = params.get(\"save_params\", False)\n",
    "\n",
    "    self.metrics = Metrics(self.policy, \"training_results\", use_metrics)\n",
    "    if save_params:\n",
    "        self.metrics.save_params(params)\n",
    "\n",
    "\n",
    "  def create_network(self, env):\n",
    "    self.num_actions = env.action_space.n\n",
    "\n",
    "    if self.policy == \"CnnPolicy\":\n",
    "        self.create_CNN(env)\n",
    "\n",
    "    if self.policy == \"MlpPolicy\":\n",
    "        self.create_MLP_Network(env)\n",
    "\n",
    "    self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "  def create_CNN(self, env):\n",
    "        self.num_states = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.policy_net = CNNPolicyNetwork(self.num_states, self.num_actions).to(self.device)\n",
    "\n",
    "  def create_MLP_Network(self, env):\n",
    "        self.num_states = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.policy_net = MLPPolicyNetwork(self.num_states, self.num_states, self.num_actions).to(self.device)\n",
    "\n",
    "  def collect_data(self, num_steps, env):\n",
    "        '''\n",
    "        Collect data from the environment for num_steps times, if episode ends during for loop, reset the environment.\n",
    "        '''\n",
    "        state = env.reset()[0]\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        state = state.unsqueeze(0)\n",
    "\n",
    "        done = False\n",
    "        episode_reward = []\n",
    "        total_reward = 0.0\n",
    "        num_episodes = 0\n",
    "        episode_len = 0\n",
    "        total_steps = 0\n",
    "\n",
    "        self.train_data.clear()\n",
    "\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "\n",
    "            action, log_prob, value = self.get_action_value(state, eval_mode=False)\n",
    "\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            next_state = torch.tensor(np.array([next_state]), dtype=torch.float32, device=self.device)\n",
    "\n",
    "            reward += self.step_reward\n",
    "\n",
    "            self.train_data.states.append(state)\n",
    "            self.train_data.actions.append(action)\n",
    "            self.train_data.log_probs.append(log_prob.item())\n",
    "            self.train_data.values.append(value.item())\n",
    "            self.train_data.rewards.append(reward)\n",
    "            self.train_data.dones.append(done)\n",
    "\n",
    "            episode_reward.append(reward)\n",
    "            episode_len += 1\n",
    "\n",
    "            state = next_state\n",
    "            if done or truncated:\n",
    "                state, _ = env.reset()\n",
    "                state = torch.tensor(np.array([state]), dtype=torch.float32, device=self.device)\n",
    "                done = False\n",
    "                truncated = False\n",
    "\n",
    "                num_episodes += 1\n",
    "                total_steps +=  episode_len\n",
    "                total_reward += sum(episode_reward) / len(episode_reward)\n",
    "                episode_len = 0\n",
    "                episode_reward = []\n",
    "\n",
    "        try:\n",
    "          avg_episode_len = total_steps / num_episodes\n",
    "          avg_episode_reward = total_reward / num_episodes\n",
    "        except:\n",
    "          avg_episode_len = 0\n",
    "          avg_episode_reward = 0\n",
    "\n",
    "        return avg_episode_reward, avg_episode_len\n",
    "\n",
    "  def gae(self, next_value=0.0):\n",
    "      \"\"\"\n",
    "      Compute generalized advantage estimation. If last step of the episode, bootstrapping.\n",
    "      \"\"\"\n",
    "      num_steps = len(self.train_data.rewards)\n",
    "      advantages = [0] * num_steps\n",
    "      gae = 0.0\n",
    "\n",
    "      for i in reversed(range(num_steps)):\n",
    "          if i == num_steps - 1:\n",
    "              next_state_value = next_value\n",
    "          else:\n",
    "              next_state_value = self.train_data.values[i+1]\n",
    "\n",
    "          mask = 1.0 - float(self.train_data.dones[i]) # if not done, mask = 1.0\n",
    "          delta = self.train_data.rewards[i] + self.gamma * next_state_value * mask - self.train_data.values[i]\n",
    "          gae = delta + self.gamma * self.lamda * mask * gae\n",
    "          advantages[i] = gae\n",
    "\n",
    "      returns = [v + a for v, a in zip(self.train_data.values, advantages)]\n",
    "      return advantages, returns\n",
    "\n",
    "\n",
    "  def update_policy(self, advantages, returns):\n",
    "    '''\n",
    "    Update policy network based on the collected data.\n",
    "    '''\n",
    "\n",
    "    states = torch.stack(self.train_data.states).to(self.device) # Stack the list of tensors\n",
    "    states = states.squeeze(1)\n",
    "    actions = torch.LongTensor(self.train_data.actions).to(self.device)\n",
    "    old_log_probs = torch.FloatTensor(self.train_data.log_probs).to(self.device)\n",
    "    advantages = torch.FloatTensor(advantages).to(self.device)\n",
    "    returns = torch.FloatTensor(returns).to(self.device)\n",
    "\n",
    "    # Normalise advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    num_states = len(states)\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(self.epochs):\n",
    "        indexs = np.random.permutation(num_states)\n",
    "        for i in range(0, num_states, self.batch_size):\n",
    "            batch_index = indexs[i : i + self.batch_size]\n",
    "\n",
    "            batch_states = states[batch_index]\n",
    "            batch_actions = actions[batch_index]\n",
    "            batch_old_log_probs = old_log_probs[batch_index]\n",
    "            batch_advantages = advantages[batch_index]\n",
    "            batch_returns = returns[batch_index]\n",
    "\n",
    "            logits, value = self.policy_net(batch_states)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            new_log_probs = dist.log_prob(batch_actions)\n",
    "\n",
    "            # ratio\n",
    "            ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "\n",
    "            # clipped surrogate\n",
    "            #L_CLIP: equation 7 in \"Proximal Policy Optimization Algorithms\"\n",
    "            policy_loss = -torch.min(ratio * batch_advantages, torch.clip(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon)* batch_advantages).mean()\n",
    "\n",
    "\n",
    "            # value loss\n",
    "            value_loss = (value.squeeze() - batch_returns).pow(2).mean()\n",
    "\n",
    "            # entropy\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            loss = policy_loss + self.loss_coeff * value_loss - self.entropy_coeff * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.metrics.add(\"update_policy/policy_loss\", policy_loss.item(), step)\n",
    "            self.metrics.add(\"update_policy/value_loss\", value_loss.item(), step)\n",
    "            self.metrics.add(\"update_policy/entropy\", entropy.item(), step)\n",
    "            self.metrics.add(\"update_policy/total_loss\", loss.item(), step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "\n",
    "  def learn(self, env):\n",
    "    self.create_network(env)\n",
    "\n",
    "    for iteration in tqdm(range(self.num_iterations), desc=\"Training Model\"):\n",
    "      #TODO: change loop to run A actors for T times, and compute average adavantage (currently run 1 agent for num_steps times, if device == cuda use mp\n",
    "      episode_reward, avg_episode_len = self.collect_data(self.num_steps, env)\n",
    "\n",
    "      if not self.train_data.dones[-1]: # If it's the last step of the episod, bootstrapping.\n",
    "          last_state = self.train_data.states[-1]\n",
    "\n",
    "          with torch.no_grad():\n",
    "              _, last_value_t = self.policy_net(last_state)\n",
    "          next_value = last_value_t.item()\n",
    "      else:\n",
    "          next_value = 0.0\n",
    "\n",
    "      advantages, returns = self.gae(next_value)\n",
    "      self.update_policy(advantages, returns)\n",
    "      \n",
    "      self.metrics.add(\"collect_data/episode_reward\", episode_reward, iteration)\n",
    "      self.metrics.add(\"collect_data/avg_episode_len\", avg_episode_len, iteration)\n",
    "\n",
    "      if iteration % 5 == 0:\n",
    "        self.evaluate(env, 10, iteration)\n",
    "\n",
    "    if self.to_save_model:\n",
    "          self.save_model()\n",
    "\n",
    "    self.metrics.close\n",
    "\n",
    "  def get_action(self, state, eval_mode=False):\n",
    "    \"\"\"\n",
    "    if eval_mode, return the best action\n",
    "    else, return the action based on the policy\n",
    "    \"\"\"\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float32, device=self.device)\n",
    "    if len(state.shape) == 1:\n",
    "        state = state.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      logits, _ = self.policy_net(state)\n",
    "      dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "      if eval_mode:\n",
    "          action = torch.argmax(dist.probs, dim=1)\n",
    "          action = action.item()\n",
    "      else:\n",
    "          action_sample = dist.sample()\n",
    "          action = action_sample.item()\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "  def get_action_value(self, state, eval_mode=False):\n",
    "    \"\"\"\n",
    "    return action log_prob and value\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float32, device=self.device)\n",
    "    if len(state.shape) == 1:\n",
    "        state = state.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, value = self.policy_net(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "        if eval_mode: # select the best action\n",
    "            action_tensor = torch.argmax(dist.probs, dim=1)\n",
    "        else: # sample action from policy\n",
    "            action_tensor = dist.sample()\n",
    "        log_prob = dist.log_prob(action_tensor)\n",
    "\n",
    "    action = action_tensor.item()\n",
    "\n",
    "    return action, log_prob, value.squeeze(1)\n",
    "\n",
    "\n",
    "  def evaluate(self, env, episode_num, iteration = -1):\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    r = tqdm(range(episode_num), desc=\"Evaluating Agent\") if iteration == -1 else range(episode_num)\n",
    "\n",
    "    for episode in r:\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        episode_reward = []\n",
    "        steps = 0\n",
    "\n",
    "        while (not done and not truncated):\n",
    "            # Select best action\n",
    "            action, _, _ = self.get_action_value(state, eval_mode=True)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            episode_reward.append(reward)\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            env.render()\n",
    "\n",
    "        total_reward += sum(episode_reward) / len(episode_reward)\n",
    "        total_steps += steps\n",
    "\n",
    "        if iteration == -1:\n",
    "            self.metrics.add(f\"evaluate/episode/episode_reward(episode_reward/episode)\", sum(episode_reward) / len(episode_reward), episode)\n",
    "            self.metrics.add(f\"evaluate/episode/episode_steps(steps/episode)\", steps, episode)\n",
    "\n",
    "    avg_reward = total_reward / episode_num\n",
    "    avg_steps = total_steps / episode_num\n",
    "\n",
    "    if iteration != -1:\n",
    "      self.metrics.add(f\"evaluate/iteration/iteration_avg_reward(avg_reward/iteration)\",avg_reward, iteration)\n",
    "      self.metrics.add(f\"evaluate/iteration/iteration_avg_steps(avg_steps/iteration)\",avg_steps, iteration)\n",
    "\n",
    "    if iteration == -1: print(f\"\\nEvaluation after training: Num of Episodes: {episode_num}, Avg Reward: {avg_reward:.2f}, Avg Steps: {avg_steps:.2f}\")\n",
    "    return avg_reward, avg_steps\n",
    "\n",
    "\n",
    "  def create_folder(self, directory_name):\n",
    "    try:\n",
    "        os.mkdir(directory_name)\n",
    "        print(f\"Directory '{directory_name}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        return\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "  def save_model(self):\n",
    "      folder_name = self.policy + \"_save_models\"\n",
    "      self.create_folder(folder_name)\n",
    "      new_model_num = str(len(os.listdir(\"./\" +folder_name)) + 1)\n",
    "      file_name = f'{folder_name}/PPO_{new_model_num}_{self.time}.pth'\n",
    "      state = {\n",
    "          \"policy_net\": self.policy_net.state_dict(),\n",
    "          \"optimizer\": self.optimizer.state_dict()\n",
    "      }\n",
    "      torch.save(state, file_name)\n",
    "\n",
    "      self.file_name = f\"PPO_{new_model_num}_{self.time}\"\n",
    "      print(f\"Model saved to {file_name}\")\n",
    "\n",
    "  def load_model(self, env, file_name):\n",
    "      folder_name = self.policy + \"_save_models\"\n",
    "\n",
    "      filename = folder_name + \"/\" + file_name + \".pth\"\n",
    "      self.create_network(env)\n",
    "\n",
    "      models = torch.load(filename, map_location=self.device)\n",
    "\n",
    "      self.policy_net.load_state_dict(models[\"policy_net\"])\n",
    "      self.optimizer.load_state_dict(models[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "# choose a policy\n",
    "policy = \"CnnPolicy\"\n",
    "# policy = \"MlpPolicy\"\n",
    "\n",
    "if policy == \"CnnPolicy\":\n",
    "    config={\n",
    "        \"lanes_count\" : 3,\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 64),\n",
    "            \"stack_size\": 4,\n",
    "            \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion keep this conversion this is in the highway env page\n",
    "            \"scaling\": 1.75,\n",
    "        },\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"lanes_count\" : 3,\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"vehicles_count\": 5,\n",
    "            \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "            \"features_range\": {\n",
    "                \"x\": [-100, 100],\n",
    "                \"y\": [-100, 100],\n",
    "                \"vx\": [-20, 20],\n",
    "                \"vy\": [-20, 20]\n",
    "            },\n",
    "            \"absolute\": False,\n",
    "            \"order\": \"sorted\"\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'policy': policy,\n",
    "    'device': torch.device(\"mps\"),\n",
    "\n",
    "    'num_iterations': 20,\n",
    "    'num_steps': 1024,\n",
    "    \"step_reward\" : 0.0,\n",
    "    'gamma': 0.99,\n",
    "    'lamda': 0.95,\n",
    "    'clip_epsilon': 0.2,\n",
    "    'loss_coeff': 0.5,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 3e-4,\n",
    "    'entropy_coeff': 0.02,\n",
    "    'use_metrics': True,\n",
    "    'save_model': True,\n",
    "    'save_params': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 72 # Our group number\n",
    "for i in range(1):\n",
    "    seed += i\n",
    "    torch.manual_seed(seed)\n",
    "    ppo = PPOAgent(params)\n",
    "    env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "    ppo.learn(env)\n",
    "    ppo.evaluate(env, 10)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "ppo = PPOAgent(params)\n",
    "ppo.load_model(env, \"DQN_1_20250101000000\")\n",
    "ppo.evaluate(env, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-786a0208295f14a7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-786a0208295f14a7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6013;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir training_results --host localhost --port 6013"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
