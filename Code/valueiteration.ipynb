{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import highway_env\n",
    "# import finite-mdp\n",
    "# from finite_mdp.envs import finite_mdp_env\n",
    "\n",
    "%load_ext tensorboard\n",
    "import sys\n",
    "from tqdm.notebook import trange\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# doesn't work cause not linux\n",
    "# !apt-get install -y xvfb ffmpeg\n",
    "# !git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "# !git clone https://github.com/eleurent/finite-mdp.git 2> /dev/null\n",
    "sys.path.insert(0, '/content/HighwayEnv/scripts/')\n",
    "# from utils import record_videos, show_videos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# import random\n",
    "# import highway_env\n",
    "# from collections import defaultdict\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "\n",
    "# # render_mode=\"rgb_array_list\"\n",
    "# env = gym.make(\"highway-fast-v0\", render_mode = \"rgb_array\", config={\n",
    "# #    \"observation\": {\n",
    "# #     \"type\": \"Kinematics\",\n",
    "# #     \"vehicles_count\": 5,\n",
    "# #     \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\"],\n",
    "# #     \"features_range\": {\n",
    "# #         \"x\": [-100, 100],\n",
    "# #         \"y\": [-100, 100],\n",
    "# #         \"vx\": [-20, 20],\n",
    "# #         \"vy\": [-20, 20]\n",
    "# #     },\n",
    "# #     \"grid_size\": [[-27.5, 27.5], [-27.5, 27.5]],\n",
    "# #     \"grid_step\": [5, 5],\n",
    "# #     \"absolute\": False,\n",
    "# # },\n",
    "# # Vehicle behaviour. defense vehicle\n",
    "#  \"observation\": {\n",
    "#     \"type\": \"TimeToCollision\",\n",
    "#     \"horizon\": 2,\n",
    "#     \"lanes_count\":2,\n",
    "#     \"other_vehicles_type\": \"highway_env.vehicle.behavior.DefenseVehicle\",\n",
    "#     # \"lanes_count\":2,\n",
    "# },\n",
    "# #   \"action\":{\n",
    "# #     \"type\": \"DiscreteAction\",\n",
    "# # }\n",
    "# }\n",
    "# )\n",
    "\n",
    "# # obs, info = env.reset()\n",
    "\n",
    "\n",
    "# class FiniteMarkovDecision():\n",
    "#     def __init__(self,env):\n",
    "#         self.finite_mdp = self.is_finite_mdp(env)\n",
    "#         self.states = env.observation_space\n",
    "#         if self.finite_mdp:\n",
    "#             self.mpd = env.mdp\n",
    "#         elif not self.finite_mdp:\n",
    "#             try:\n",
    "#                 self.mdp = env.unwrapped.to_finite_mdp()\n",
    "#             except AttributeError:\n",
    "#                 raise TypeError(\"not finite mdp\")\n",
    "            \n",
    "#         self.env = env\n",
    "#         self.obs, self.info = env.reset() \n",
    "#         self.state_action_value = env.action_space\n",
    "#         self.load_dictionary()\n",
    "\n",
    "\n",
    "\n",
    "#     def train(self):\n",
    "#         gamma = 0.9  # Discount factor\n",
    "#         episodes = 1000\n",
    "\n",
    "#         # print(self.state_action_value.n)\n",
    "#         for _ in range(episodes):  # Loop through each episode\n",
    "#             done = False\n",
    "#             total_reward = 0\n",
    "#             episode_count = 0\n",
    "\n",
    "#             # Initialize the environment and get the initial state\n",
    "#             self.obs = self.env.reset()  # Assuming `reset` initializes the environment\n",
    "#             state = str(self.obs)\n",
    "\n",
    "#             while not done:\n",
    "#                 # Select a random action\n",
    "#                 action = random.randrange(0, self.state_action_value.n)\n",
    "\n",
    "#                 # Take the action in the environment\n",
    "#                 next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "#                 # Accumulate discounted rewards\n",
    "#                 total_reward += reward * (gamma ** episode_count)\n",
    "#                 episode_count += 1\n",
    "\n",
    "#                 # Update policy if the current action is better\n",
    "#             if state not in self.policy or self.policy[state]['reward'] < total_reward:\n",
    "#                 self.policy[state] = {'action': action, 'reward': total_reward}\n",
    "\n",
    "#                 # Update state for the next iteration\n",
    "#                 # self.obs = next_obs\n",
    "#                 # state = str(self.obs)\n",
    "#             # env.render()\n",
    "            \n",
    "\n",
    "#             self.save_policy()\n",
    "\n",
    "\n",
    "            \n",
    "#     def load_dictionary(self):\n",
    "#         file_path = \"policies.json\"\n",
    "#         if os.path.exists(file_path):\n",
    "#             try:\n",
    "#                 with open(file_path, 'r') as file:  # Open in text mode for JSON\n",
    "#                     self.policy = json.load(file)\n",
    "#                     # print(\"Dictionary loaded successfully.\")\n",
    "#                     return True\n",
    "#             except (json.JSONDecodeError, IOError) as e:\n",
    "#                 print(f\"Error loading dictionary: {e}\")\n",
    "#                 self.policy = {}  # Use a regular dictionary as fallback\n",
    "#                 return False\n",
    "#         else:\n",
    "#             self.policy = {}  # Use a regular dictionary if file doesn't exist\n",
    "#             print(\"No existing policy found. Starting with an empty dictionary.\")\n",
    "#             return False\n",
    "\n",
    "#     def save_policy(self):\n",
    "#         file_path = \"policies.json\"\n",
    "#         try:\n",
    "#             # Ensure self.policy is serializable\n",
    "#             if isinstance(self.policy, defaultdict):\n",
    "#                 self.policy = dict(self.policy)  # Convert defaultdict to dict\n",
    "\n",
    "#             with open(file_path, 'w') as file:  # Open in write mode\n",
    "#                 json.dump(self.policy, file, indent=4)  # Save with pretty printing\n",
    "#                 # print(\"Dictionary saved successfully.\")\n",
    "#                 return True\n",
    "#         except IOError as e:\n",
    "#             print(f\"Error saving dictionary: {e}\")\n",
    "#             return False\n",
    "                \n",
    "#     def evaluate(self, env, episode_num):\n",
    "#         # add camera here\n",
    "#         for _ in range(episode_num):\n",
    "#             state = env.reset()[0]  \n",
    "#             done = False      \n",
    "#             truncated = False \n",
    "#             # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "#             while(not done and not truncated):  \n",
    "#                 # Select best action\n",
    "#                 try:   \n",
    "#                     action = self.policy[str(state)]\n",
    "#                 except KeyError:\n",
    "#                     action = random.randrange(0, env.action_space.n)\n",
    "#                 next_state, reward, done, truncated, info = env.step(action)\n",
    "#                 state = next_state\n",
    "#                 env.render()\n",
    "\n",
    "\n",
    "\n",
    "#     def is_finite_mdp(self,env):\n",
    "#         try:\n",
    "#             finite_mdp = __import__(\"finite_mdp.envs.finite_mdp_env\")\n",
    "#             if isinstance(env.unwrapped, finite_mdp.envs.finite_mdp_env.FiniteMDPEnv):\n",
    "#                 return True\n",
    "#         except (ModuleNotFoundError, TypeError):\n",
    "#             return False\n",
    "\n",
    "\n",
    "\n",
    "# finite_mdp = FiniteMarkovDecision(env)\n",
    "# finite_mdp.train()\n",
    "# finite_mdp.evaluate(env, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "    \n",
    "# https://stackoverflow.com/questions/50916422/python-typeerror-object-of-type-int64-is-not-json-serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import json\n",
    "import random\n",
    "import highway_env\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "# import numpy\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, exploration_decay=0.99):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.q_table = defaultdict()\n",
    "        self.q_table_path = \"q_table.json\"\n",
    "        self.load_q_table()\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.env.action_space.sample()  # Explore: random action\n",
    "        else:\n",
    "            try:\n",
    "                return max(self.q_table[state])\n",
    "            except:\n",
    "                return self.env.action_space.sample()\n",
    "\n",
    "\n",
    "    def train(self, episodes):\n",
    "        for episode in range(episodes):\n",
    "            state = str(self.env.reset()[0])  # Convert state to string for indexing\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = str(self.choose_action(state))\n",
    "\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "                next_state = str(next_obs)\n",
    "\n",
    "\n",
    "                if state not in self.q_table:\n",
    "                    self.q_table[state] = { str(i): 0 for i in range(0, self.action_space)}\n",
    "\n",
    "\n",
    "                if next_state not in self.q_table:\n",
    "                    self.q_table[next_state] = { str(i): 0 for i in range(0, self.action_space)}\n",
    "\n",
    "\n",
    "                best_next_action = str(max(self.q_table[next_state], key = self.q_table[next_state].get))\n",
    "                # max(d, key = d.get)\n",
    "\n",
    "\n",
    "                # try:\n",
    "                #     self.q_table[next_state][best_next_action]\n",
    "                # except:\n",
    "                #     print(type(self.q_table[next_state].keys().get))\n",
    "\n",
    "                \n",
    "                td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "\n",
    "                td_error = td_target - self.q_table[state][action]\n",
    "\n",
    "\n",
    "                self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "\n",
    "\n",
    "            self.exploration_rate *= self.exploration_decay\n",
    "\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "        self.save_q_table()\n",
    "\n",
    "    def evaluate(self, episodes=10):\n",
    "        for episode in range(episodes):\n",
    "            state = str(self.env.reset()[0])\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                try:\n",
    "                    action = max(self.q_table[state])\n",
    "                except:\n",
    "                    action = str(0)\n",
    "\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "                state = str(next_obs)\n",
    "                total_reward += reward\n",
    "                # self.env.render()\n",
    "\n",
    "            print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "    def load_q_table(self):\n",
    "        if os.path.exists(self.q_table_path):\n",
    "            try:\n",
    "                with open(self.q_table_path, 'r') as file:\n",
    "                # self.q_table = np.load(self.q_table_path, allow_pickle=True).item()\n",
    "                    loaded =  json.load(file)\n",
    "                    self.q_table.update(loaded)\n",
    "                    # print(type(self.q_table))\n",
    "                    print(\"Q-table loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading Q-table: {e}\")\n",
    "\n",
    "    def save_q_table(self):\n",
    "        try:\n",
    "            with open(self.q_table_path, 'w') as file:  \n",
    "                json.dump(self.q_table, file, indent=4)\n",
    "                print(\"Q-table saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving Q-table: {e}\")\n",
    "\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode = \"rgb_array\", config={\n",
    "\n",
    " \"observation\": {\n",
    "    \"type\": \"TimeToCollision\",\n",
    "    \"horizon\": 5,\n",
    "    \"lanes_count\":3,\n",
    "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.DefenseVehicle\",\n",
    "    # \"lanes_count\":2,\n",
    "},\n",
    "}\n",
    ")\n",
    "\n",
    "# Initialize the Q-learning agent\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "agent.train(episodes=10000)\n",
    "\n",
    "# Evaluate the agent\n",
    "# agent.evaluate(episodes=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
