{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import highway_env\n",
    "# import finite-mdp\n",
    "# from finite_mdp.envs import finite_mdp_env\n",
    "\n",
    "%load_ext tensorboard\n",
    "import sys\n",
    "from tqdm.notebook import trange\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# doesn't work cause not linux\n",
    "# !apt-get install -y xvfb ffmpeg\n",
    "# !git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "# !git clone https://github.com/eleurent/finite-mdp.git 2> /dev/null\n",
    "sys.path.insert(0, '/content/HighwayEnv/scripts/')\n",
    "# from utils import record_videos, show_videos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.]], dtype=float32), array([[0. , 0. , 1. , 1. , 0.5],\n",
      "       [0. , 0. , 0. , 0. , 0. ],\n",
      "       [0. , 0. , 0. , 0. , 0. ]], dtype=float32))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 94\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# obs, info = env.reset()\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# print(obs)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# x = env.to_finite_mdp()\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# print(env.__dict__.keys())\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# print(env.action_space)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m finite_mdp \u001b[38;5;241m=\u001b[39m FiniteMarkovDecision(env)\n\u001b[0;32m---> 94\u001b[0m \u001b[43mfinite_mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbellman\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 72\u001b[0m, in \u001b[0;36mFiniteMarkovDecision.bellman\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m total_reward:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs] \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     74\u001b[0m episodes \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import highway_env\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", config={\n",
    "#    \"observation\": {\n",
    "#     \"type\": \"Kinematics\",\n",
    "#     \"vehicles_count\": 5,\n",
    "#     \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\"],\n",
    "#     \"features_range\": {\n",
    "#         \"x\": [-100, 100],\n",
    "#         \"y\": [-100, 100],\n",
    "#         \"vx\": [-20, 20],\n",
    "#         \"vy\": [-20, 20]\n",
    "#     },\n",
    "#     \"grid_size\": [[-27.5, 27.5], [-27.5, 27.5]],\n",
    "#     \"grid_step\": [5, 5],\n",
    "#     \"absolute\": False,\n",
    "# },\n",
    " \"observation\": {\n",
    "    \"type\": \"TimeToCollision\",\n",
    "    \"horizon\": 5,\n",
    "    # \"lanes_count\":2,\n",
    "    # \"lanes_count\":2,\n",
    "},\n",
    "#   \"action\":{\n",
    "#     \"type\": \"DiscreteAction\",\n",
    "# }\n",
    "}\n",
    ")\n",
    "\n",
    "# obs, info = env.reset()\n",
    "\n",
    "\n",
    "class FiniteMarkovDecision():\n",
    "    def __init__(self,env):\n",
    "        self.finite_mdp = self.is_finite_mdp(env)\n",
    "        self.states = env.observation_space\n",
    "        if self.finite_mdp:\n",
    "            self.mpd = env.mdp\n",
    "        elif not self.finite_mdp:\n",
    "            try:\n",
    "                self.mdp = env.unwrapped.to_finite_mdp()\n",
    "            except AttributeError:\n",
    "                raise TypeError(\"not finite mdp\")\n",
    "            \n",
    "        self.env = env\n",
    "        self.obs, self.info = env.reset() \n",
    "        self.state_action_value = env.action_space\n",
    "        self.policy = defaultdict()\n",
    "\n",
    "\n",
    "    def bellman(self):\n",
    "\n",
    "        # print(self.state_action_value.n)\n",
    "        gamma = 0.9\n",
    "        episodes = 2\n",
    "        episode_count = 0\n",
    "        while episodes > 0:\n",
    "            done = True\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                i = random.randrange(0, self.state_action_value.n)\n",
    "                next_state, reward, done, truncated, info = self.env.step(i)\n",
    "                total_reward += (reward*gamma*episode_count)\n",
    "                episode_count += 1\n",
    "            state = tuple(self.obs)\n",
    "            if self.policy[state] < total_reward:\n",
    "                self.policy[self.obs] = i\n",
    "            episodes -= 1\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    def is_finite_mdp(self,env):\n",
    "        try:\n",
    "            finite_mdp = __import__(\"finite_mdp.envs.finite_mdp_env\")\n",
    "            if isinstance(env.unwrapped, finite_mdp.envs.finite_mdp_env.FiniteMDPEnv):\n",
    "                return True\n",
    "        except (ModuleNotFoundError, TypeError):\n",
    "            return False\n",
    "\n",
    "\n",
    "# obs, info = env.reset()\n",
    "# print(obs)\n",
    "# x = env.to_finite_mdp()\n",
    "# print(env.__dict__.keys())\n",
    "# print(env.action_space)\n",
    "finite_mdp = FiniteMarkovDecision(env)\n",
    "finite_mdp.bellman()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
