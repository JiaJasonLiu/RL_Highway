{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:19:10.110768Z",
     "start_time": "2024-11-27T10:19:10.107277Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/deep-rl-dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running it without Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 16:24:05.748 python[2021:12877892] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-12-17 16:24:05.748 python[2021:12877892] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# env = gym.make('highway-v0', render_mode='rgb_array')\n",
    "# env.reset()\n",
    "# for _ in range(5):\n",
    "# # while True:\n",
    "#     action = env.unwrapped.action_type.actions_indexes[\"IDLE\"]\n",
    "#     obs, reward, done, truncated, info = env.step(action)\n",
    "#     env.render()\n",
    "\n",
    "# plt.imshow(env.render())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Their DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:19:24.572478Z",
     "start_time": "2024-11-27T10:19:24.554652Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "\n",
    "# Visualization utils\n",
    "%load_ext tensorboard\n",
    "import sys\n",
    "from tqdm.notebook import trange\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# doesn't work cause not linux\n",
    "# !apt-get install -y xvfb ffmpeg\n",
    "# !git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "# sys.path.insert(0, '/content/HighwayEnv/scripts/')\n",
    "# from utils import record_videos, show_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning using existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'observation': {'type': 'Kinematics', 'vehicles_count': 15, 'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'], 'features_range': {'x': [-100, 100], 'y': [-100, 100], 'vx': [-20, 20], 'vy': [-20, 20]}, 'absolute': False, 'order': 'sorted'}, 'action': {'type': 'DiscreteMetaAction'}, 'simulation_frequency': 5, 'policy_frequency': 1, 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle', 'screen_width': 600, 'screen_height': 150, 'centering_position': [0.3, 0.5], 'scaling': 5.5, 'show_trajectories': False, 'render_agent': True, 'offscreen_rendering': False, 'manual_control': False, 'real_time_rendering': False, 'lanes_count': 3, 'vehicles_count': 20, 'controlled_vehicles': 1, 'initial_lane_id': None, 'duration': 30, 'ego_spacing': 1.5, 'vehicles_density': 1, 'collision_reward': -1, 'right_lane_reward': 0.1, 'high_speed_reward': 0.4, 'lane_change_reward': 0, 'reward_speed_range': [20, 30], 'normalize_reward': True, 'offroad_terminal': False}\n",
      "Using mps device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to highway_dqn/DQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.78     |\n",
      "|    exploration_rate | 0.591    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 45       |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 43       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 8.16     |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 90       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | 8.45     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 139      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.4     |\n",
      "|    ep_rew_mean      | 8.57     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 46       |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 182      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | 8.64     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 42       |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 226      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0622   |\n",
      "|    n_updates        | 25       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.8     |\n",
      "|    ep_rew_mean      | 9.13     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 40       |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 282      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0433   |\n",
      "|    n_updates        | 81       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.3     |\n",
      "|    ep_rew_mean      | 8.88     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 40       |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 317      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0318   |\n",
      "|    n_updates        | 116      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | 8.27     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 39       |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 337      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.0549   |\n",
      "|    n_updates        | 136      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10       |\n",
      "|    ep_rew_mean      | 7.87     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 39       |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 360      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.153    |\n",
      "|    n_updates        | 159      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.7      |\n",
      "|    ep_rew_mean      | 7.65     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 38       |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 388      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.146    |\n",
      "|    n_updates        | 187      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.98     |\n",
      "|    ep_rew_mean      | 7.91     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 38       |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 439      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.245    |\n",
      "|    n_updates        | 238      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.65     |\n",
      "|    ep_rew_mean      | 7.64     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 38       |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 463      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.305    |\n",
      "|    n_updates        | 262      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.35     |\n",
      "|    ep_rew_mean      | 7.39     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 37       |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 486      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 285      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.84     |\n",
      "|    ep_rew_mean      | 7.71     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 37       |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 551      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.301    |\n",
      "|    n_updates        | 350      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.67     |\n",
      "|    ep_rew_mean      | 7.61     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 37       |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 580      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.35     |\n",
      "|    n_updates        | 379      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.69     |\n",
      "|    ep_rew_mean      | 7.62     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 620      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.324    |\n",
      "|    n_updates        | 419      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10       |\n",
      "|    ep_rew_mean      | 7.88     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 683      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.502    |\n",
      "|    n_updates        | 482      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.97     |\n",
      "|    ep_rew_mean      | 7.83     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 718      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.281    |\n",
      "|    n_updates        | 517      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.83     |\n",
      "|    ep_rew_mean      | 7.74     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 747      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.196    |\n",
      "|    n_updates        | 546      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.86     |\n",
      "|    ep_rew_mean      | 7.74     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 789      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.538    |\n",
      "|    n_updates        | 588      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.92     |\n",
      "|    ep_rew_mean      | 7.78     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 833      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.185    |\n",
      "|    n_updates        | 632      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.1     |\n",
      "|    ep_rew_mean      | 7.9      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 887      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.273    |\n",
      "|    n_updates        | 686      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.89     |\n",
      "|    ep_rew_mean      | 7.76     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 910      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.293    |\n",
      "|    n_updates        | 709      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.8      |\n",
      "|    ep_rew_mean      | 7.69     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 941      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.311    |\n",
      "|    n_updates        | 740      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.8      |\n",
      "|    ep_rew_mean      | 7.71     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 36       |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 980      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0005   |\n",
      "|    loss             | 0.508    |\n",
      "|    n_updates        | 779      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x334cc4550>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for MLP\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 15,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-20, 20],\n",
    "            \"vy\": [-20, 20]\n",
    "        },\n",
    "        \"absolute\": False,\n",
    "        \"order\": \"sorted\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # For CNN\n",
    "# config = {\n",
    "#     \"observation\": {\n",
    "#         \"type\": \"GrayscaleObservation\",\n",
    "#         \"observation_shape\": (128, 64),\n",
    "#         \"stack_size\": 4,\n",
    "#         \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "#         \"scaling\": 1.75,\n",
    "#     },\n",
    "#     \"policy_frequency\": 2\n",
    "# }\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode='rgb_array', config=config)\n",
    "print(env.unwrapped.config)\n",
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "model = DQN('MlpPolicy', env,\n",
    "              policy_kwargs=dict(net_arch=[256, 256]),\n",
    "              learning_rate=5e-4,\n",
    "              buffer_size=15000,\n",
    "              learning_starts=200,\n",
    "              batch_size=32,\n",
    "              gamma=0.8,\n",
    "              device=mps_device,\n",
    "              train_freq=1,\n",
    "              gradient_steps=1,\n",
    "              target_update_interval=50,\n",
    "              verbose=1,\n",
    "              tensorboard_log=\"highway_dqn/\")\n",
    "\n",
    "model.learn(int(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"highway_dqn/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [307]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (done \u001b[38;5;129;01mor\u001b[39;00m truncated):\n\u001b[1;32m      9\u001b[0m   action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m   obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:280\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    278\u001b[0m         frame \u001b[38;5;241m<\u001b[39m frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    279\u001b[0m     ):  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:340\u001b[0m, in \u001b[0;36mAbstractEnv._automatic_rendering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:303\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m EnvViewer(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39moffscreen:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/graphics.py:160\u001b[0m, in \u001b[0;36mEnvViewer.display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal_time_rendering\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSAVE_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory:\n\u001b[1;32m    163\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_surface,\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighway-env_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load and test saved model\n",
    "model = DQN.load(\"highway_dqn/model\")\n",
    "\n",
    "# while True:\n",
    "for i in range(100):\n",
    "  done = truncated = False\n",
    "  obs, info = env.reset()\n",
    "  while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Own Implementations\n",
    "\n",
    "### DQN\n",
    "\n",
    "Chose to first make naive work and then make the normal work\n",
    "\n",
    "\n",
    "The following is the pesudocode that will be followed when creating the DQN\n",
    "\n",
    "Useful: https://www.youtube.com/watch?v=RVMpm86equc&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=2\n",
    "\n",
    "https://github.com/saashanair/rl-series/tree/master/dqn\n",
    "\n",
    "https://github.com/johnnycode8/gym_solutions/blob/main/frozen_lake_dql.py\n",
    "\n",
    "<img src=\"DQN.png\" style=\"width: 900px;\" align=\"left\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "\n",
    "# Define model\n",
    "class MLQNetwork(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super(MLQNetwork, self).__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # output layer w\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         \n",
    "        # x = self.flatten(x)         \n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        filename = filename + \".pth\"\n",
    "        torch.save(self.state_dict(), filename)\n",
    "    \n",
    "    def load_model(self, filename, device):\n",
    "        filename = filename + \".pth\"\n",
    "        self.load_state_dict(torch.load(filename, map_location=device))\n",
    "        \n",
    "# class CNNNetwork(nn.Module):\n",
    "#     def __init__(self, in_states, h1_nodes, out_actions):\n",
    "#         super(MLQNetwork, self).__init__()\n",
    "\n",
    "#         # Define network layers\n",
    "#         self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "#         self.out = nn.Linear(h1_nodes, out_actions) # output layer w\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "#         x = self.out(x)         \n",
    "#         # x = self.flatten(x)         \n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "    \n",
    "#     def save_model(self, filename):\n",
    "#         filename = filename + \".pth\"\n",
    "#         torch.save(self.state_dict(), filename)\n",
    "    \n",
    "#     def load_model(self, filename, device):\n",
    "#         filename = filename + \".pth\"\n",
    "#         self.load_state_dict(torch.load(filename), map_location=device)\n",
    "        \n",
    "\n",
    "    \n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity\n",
    "        self.buffer_state = []\n",
    "        self.buffer_action = []\n",
    "        self.buffer_next_state = []\n",
    "        self.buffer_reward = []\n",
    "        self.buffer_done = []\n",
    "        self.idx = 0\n",
    "        self.memory = []\n",
    "        self.device = device\n",
    "    \n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        if len(self.buffer_state) < self.capacity:\n",
    "            self.buffer_state.append(state)\n",
    "            self.buffer_action.append(action)\n",
    "            self.buffer_next_state.append(next_state)\n",
    "            self.buffer_reward.append(reward)\n",
    "            self.buffer_done.append(done)\n",
    "        else:\n",
    "            self.buffer_state[self.idx] = state\n",
    "            self.buffer_action[self.idx] = action\n",
    "            self.buffer_next_state[self.idx] = next_state\n",
    "            self.buffer_reward[self.idx] = reward\n",
    "            self.buffer_done[self.idx] = done\n",
    "\n",
    "        self.idx = (self.idx+1)%self.capacity # for circular memory\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size >  len(self.buffer_state):\n",
    "            batch_size = len(self.buffer_state)\n",
    "        \n",
    "        indices_to_sample = random.sample(range(len(self.buffer_state)), batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.array(self.buffer_state)[indices_to_sample]).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(self.buffer_action)[indices_to_sample]).to(self.device)\n",
    "        next_states = torch.from_numpy(np.array(self.buffer_next_state)[indices_to_sample]).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(self.buffer_reward)[indices_to_sample]).float().to(self.device)\n",
    "        done = torch.from_numpy(np.array(self.buffer_done)[indices_to_sample]).to(self.device)\n",
    "\n",
    "        return states, actions, next_states, rewards, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, memory_capacity=5000):\n",
    "        # weights\n",
    "        self.w1 = []\n",
    "        \n",
    "        self.q_net = {}\n",
    "        self.w2 = []\n",
    "        self.q_target_net = self.q_net # the same\n",
    "        self.policy = {}\n",
    "\n",
    "        self.epsilon = 0.2\n",
    "        self.discount = 0.01\n",
    "        self.batch_size = 2\n",
    "        \n",
    "        # the weights of q1 and q2 should be the same\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.memory = {} # this is the memory buffer -> setting a limit\n",
    "        self.device = torch.device(\"mps\")\n",
    "    \n",
    "    def initialize_neural_networks(self, env):\n",
    "        # the lanes\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        # print(env.action_space.sample())\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = MLQNetwork(self.num_states, self.num_states, self.num_actions).to(self.device)\n",
    "        self.q_target_net = MLQNetwork(self.num_states, self.num_states, self.num_actions).to(self.device)\n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        # make the weights and biases the same\n",
    "        self.q_target_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n",
    "    def learn(self, env, policy):\n",
    "        print(policy)\n",
    "        self.initialize_neural_networks(env)\n",
    "        \n",
    "        self.memory = ReplayMemory(capacity=self.memory_capacity, device=self.device)\n",
    "\n",
    "        env.reset()\n",
    "        num_episodes = 2\n",
    "        prefill_num = 2\n",
    "\n",
    "        self.prefill_memory(env, prefill_num)\n",
    "\n",
    "        print(\"training...\")\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            \n",
    "            # True when agent reaches the end states (colliding or passing the time)\n",
    "            done = False \n",
    "            # True when agent takes more than 100 actions    \n",
    "            truncated = False  \n",
    "            while(not done and not truncated):\n",
    "            # for i in range(2):\n",
    "                # choose best action\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state=state, action=action, next_state=next_state, reward=reward, done=done)\n",
    "                self.experience_replay()\n",
    "                \n",
    "                self.update_target_network()\n",
    "                \n",
    "                state = next_state\n",
    "                env.render()\n",
    "        \n",
    "    # either the policies are able to get miltuple actions and into the NN or the input of NN should be able to handle all of these\n",
    "    # output (one of): {0: 'LANE_LEFT', 1: 'IDLE', 2: 'LANE_RIGHT', 3: 'FASTER', 4: 'SLOWER'}\n",
    "    def get_action(self, state):\n",
    "        if random.random() <= self.epsilon: # amount of exploration reduces with the epsilon value\n",
    "            return random.randrange(self.num_actions)\n",
    "\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(np.array([state]), dtype=torch.float32).to(self.device)[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "        # pick the action with maximum Q-value in the Q-network \n",
    "            actions = self.q_net(state)\n",
    "            actions = actions.mean(dim=1, keepdim=True)\n",
    "        return torch.argmax(actions).item() \n",
    "\n",
    "    def experience_replay(self):\n",
    "        states, actions, next_states, rewards, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        q_pred = self.q_net(states)\n",
    "        q_pred = q_pred.mean(dim=2, keepdim=True)\n",
    "        # q value of the action taken\n",
    "        q_pred = q_pred.gather(1, actions.view(-1, 1, 1)) \n",
    "        q_pred = q_pred.squeeze(1)\n",
    "        \n",
    "        q_target = self.q_target_net(next_states)\n",
    "        q_target = q_target.mean(dim=2, keepdim=True)\n",
    "        q_target = q_target.gather(1, actions.view(-1, 1, 1))\n",
    "        q_target = q_target.squeeze(1)\n",
    "        \n",
    "        # setting Q(s',a') to 0 when the current state is a terminal state\n",
    "        q_target[dones] = 0.0\n",
    "        \n",
    "        y_j = rewards + (self.discount * q_target)\n",
    "        y_j = y_j.mean(dim=0, keepdim=True)\n",
    "        y_j = y_j.view(-1, 1)\n",
    "        \n",
    "        # calculate the loss as the mean-squared error of yj and qpred\n",
    "        self.q_net.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(y_j, q_pred).mean()\n",
    "        loss.backward()\n",
    "        self.q_net.optimizer.step()\n",
    "        \n",
    "    def prefill_memory(self, env, prefill_num):\n",
    "        print(\"prefilling memory...\")\n",
    "        for _ in range(prefill_num):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            while not done and not truncated:\n",
    "            # for _ in range(2):\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                dqn_agent.memory.store(state=state, \n",
    "                                    action=action, \n",
    "                                    next_state=next_state, \n",
    "                                    reward=reward, \n",
    "                                    done=done)\n",
    "    def save_model(self, filename):\n",
    "        self.q_net.save_model(filename)\n",
    "        \n",
    "    def load_model(self, env, filename):\n",
    "        self.initialize_neural_networks(env)\n",
    "        self.q_net.load_model(filename, self.device)\n",
    "        # turn into evaluation model for test\n",
    "        self.q_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpPolicy\n",
      "prefilling memory...\n",
      "training...\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "# policy = \"CnnPolicy\"\n",
    "policy = \"MlpPolicy\"\n",
    "\n",
    "if policy == \"CnnPolicy\":\n",
    "    config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 64),\n",
    "            \"stack_size\": 4,\n",
    "            \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "            \"scaling\": 1.75,\n",
    "        },\n",
    "        \"policy_frequency\": 2\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"vehicles_count\": 5,\n",
    "            \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "            \"features_range\": {\n",
    "                \"x\": [-100, 100],\n",
    "                \"y\": [-100, 100],\n",
    "                \"vx\": [-20, 20],\n",
    "                \"vy\": [-20, 20]\n",
    "            },\n",
    "            \"absolute\": False,\n",
    "            \"order\": \"sorted\"\n",
    "        }\n",
    "    }\n",
    "dqn_agent = DQNAgent()\n",
    "env = gym.make('highway-fast-v0', render_mode='rgb_array')\n",
    "dqn_agent.learn(env, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.save_model(\"highway_dqn/test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "MPS Available: True\n",
      "Device Count: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "print(\"Device Count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/m_r158w50cqfn3ydymrpm7rc0000gn/T/ipykernel_83387/1191197686.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(filename, map_location=device))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [309]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated):  \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Select best action   \u001b[39;00m\n\u001b[1;32m     13\u001b[0m     action \u001b[38;5;241m=\u001b[39m dqn_agent_test\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[0;32m---> 14\u001b[0m     next_state, reward, done, truncated, info  \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:280\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    278\u001b[0m         frame \u001b[38;5;241m<\u001b[39m frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    279\u001b[0m     ):  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:340\u001b[0m, in \u001b[0;36mAbstractEnv._automatic_rendering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:303\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m EnvViewer(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39moffscreen:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/graphics.py:160\u001b[0m, in \u001b[0;36mEnvViewer.display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal_time_rendering\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSAVE_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory:\n\u001b[1;32m    163\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_surface,\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighway-env_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('highway-v0', render_mode='rgb_array')\n",
    "dqn_agent_test = DQNAgent()\n",
    "dqn_agent_test.load_model(env, \"highway_dqn_model\")\n",
    "\n",
    "for i in range(100):\n",
    "    state = env.reset()[0]  # Initialize to state 0\n",
    "    done = False      # True when agent falls in hole or reached goal\n",
    "    truncated = False       # True when agent takes more than 200 actions            \n",
    "\n",
    "    # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "    while(not done and not truncated):  \n",
    "        # Select best action   \n",
    "        action = dqn_agent_test.get_action(state)\n",
    "        next_state, reward, done, truncated, info  = env.step(action)\n",
    "        env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with other model (also their DQN)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
