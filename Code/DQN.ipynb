{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:19:10.110768Z",
     "start_time": "2024-11-27T10:19:10.107277Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/deep-rl-dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running it without Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 16:24:05.748 python[2021:12877892] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-12-17 16:24:05.748 python[2021:12877892] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# env = gym.make('highway-v0', render_mode='rgb_array')\n",
    "# env.reset()\n",
    "# for _ in range(5):\n",
    "# # while True:\n",
    "#     action = env.unwrapped.action_type.actions_indexes[\"IDLE\"]\n",
    "#     obs, reward, done, truncated, info = env.step(action)\n",
    "#     env.render()\n",
    "\n",
    "# plt.imshow(env.render())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Their DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:19:24.572478Z",
     "start_time": "2024-11-27T10:19:24.554652Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "\n",
    "# Visualization utils\n",
    "%load_ext tensorboard\n",
    "import sys\n",
    "from tqdm.notebook import trange\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# doesn't work cause not linux\n",
    "# !apt-get install -y xvfb ffmpeg\n",
    "# !git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "# sys.path.insert(0, '/content/HighwayEnv/scripts/')\n",
    "# from utils import record_videos, show_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning using existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'observation': {'type': 'GrayscaleObservation', 'observation_shape': (128, 64), 'stack_size': 4, 'weights': [0.2989, 0.587, 0.114], 'scaling': 1.75}, 'action': {'type': 'DiscreteMetaAction'}, 'simulation_frequency': 5, 'policy_frequency': 2, 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle', 'screen_width': 600, 'screen_height': 150, 'centering_position': [0.3, 0.5], 'scaling': 5.5, 'show_trajectories': False, 'render_agent': True, 'offscreen_rendering': False, 'manual_control': False, 'real_time_rendering': False, 'lanes_count': 3, 'vehicles_count': 20, 'controlled_vehicles': 1, 'initial_lane_id': None, 'duration': 30, 'ego_spacing': 1.5, 'vehicles_density': 1, 'collision_reward': -1, 'right_lane_reward': 0.1, 'high_speed_reward': 0.4, 'lane_change_reward': 0, 'reward_speed_range': [20, 30], 'normalize_reward': True, 'offroad_terminal': False}\n",
      "Using mps device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to highway_dqn/DQN_9\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 100      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 78       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 110      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 149      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x3069ea070>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for MLP\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 15,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-20, 20],\n",
    "            \"vy\": [-20, 20]\n",
    "        },\n",
    "        \"absolute\": False,\n",
    "        \"order\": \"sorted\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# For CNN\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"GrayscaleObservation\",\n",
    "        \"observation_shape\": (128, 64),\n",
    "        \"stack_size\": 4,\n",
    "        \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "        \"scaling\": 1.75,\n",
    "    },\n",
    "    \"policy_frequency\": 2\n",
    "}\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode='rgb_array', config=config)\n",
    "print(env.unwrapped.config)\n",
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "model = DQN('CnnPolicy', env,\n",
    "              policy_kwargs=dict(net_arch=[256, 256]),\n",
    "              learning_rate=5e-4,\n",
    "              buffer_size=15000,\n",
    "              learning_starts=200,\n",
    "              batch_size=32,\n",
    "              gamma=0.8,\n",
    "              device=mps_device,\n",
    "              train_freq=1,\n",
    "              gradient_steps=1,\n",
    "              target_update_interval=50,\n",
    "              verbose=1,\n",
    "              tensorboard_log=\"highway_dqn/\")\n",
    "\n",
    "model.learn(int(2e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighway_dqn/model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"highway_dqn/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test saved model\n",
    "model = DQN.load(\"highway_dqn/model\")\n",
    "\n",
    "# while True:\n",
    "for i in range(10):\n",
    "  done = truncated = False\n",
    "  obs, info = env.reset()\n",
    "  while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Own Implementations\n",
    "\n",
    "### DQN\n",
    "\n",
    "Chose to first make naive work and then make the normal work\n",
    "\n",
    "\n",
    "The following is the pesudocode that will be followed when creating the DQN\n",
    "\n",
    "Useful: https://www.youtube.com/watch?v=RVMpm86equc&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=2\n",
    "\n",
    "https://github.com/saashanair/rl-series/tree/master/dqn\n",
    "\n",
    "https://github.com/johnnycode8/gym_solutions/blob/main/frozen_lake_dql.py\n",
    "\n",
    "<img src=\"DQN.png\" style=\"width: 900px;\" align=\"left\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefilling memory\n",
      "prefilling memory done\n",
      "0.27380601576997743\n",
      "0\n",
      "0.6473449764390355\n",
      "4\n",
      "0.734225787617938\n",
      "0\n",
      "0.841293428297414\n",
      "3\n",
      "0.8448108303210403\n",
      "3\n",
      "0.4744495081221871\n",
      "0\n",
      "0.4232309275560028\n",
      "0\n",
      "0.2508689617484974\n",
      "0\n",
      "0.6264961377968485\n",
      "4\n",
      "0.9910431037019017\n",
      "0\n",
      "0.6320532048588275\n",
      "0\n",
      "0.015513337256023552\n",
      "0\n",
      "0.5848186649866909\n",
      "0\n",
      "0.9780627005664049\n",
      "3\n",
      "0.5241173948086301\n",
      "0\n",
      "0.6746921725558953\n",
      "0\n",
      "0.7147176418267616\n",
      "0\n",
      "0.20677687624541108\n",
      "0\n",
      "0.735827875555674\n",
      "0\n",
      "0.6680463245148117\n",
      "0\n",
      "0.9066301795489136\n",
      "0\n",
      "0.6433628290430311\n",
      "0\n",
      "0.4263533297842509\n",
      "0\n",
      "0.730611514532331\n",
      "0\n",
      "0.16890957487976788\n",
      "0\n",
      "0.06595292130928965\n",
      "0\n",
      "0.16958175135517595\n",
      "0\n",
      "0.585601786457758\n",
      "0\n",
      "0.38487476104284246\n",
      "4\n",
      "0.9926282828473887\n",
      "0\n",
      "0.2957917421574152\n",
      "0\n",
      "0.32755972368877606\n",
      "0\n",
      "0.8360239483343422\n",
      "4\n",
      "0.09467573536323626\n",
      "0\n",
      "0.6123068179078854\n",
      "0\n",
      "0.17915599291917117\n",
      "0\n",
      "0.3077712730749379\n",
      "0\n",
      "0.9708241000056307\n",
      "0\n",
      "0.9958705499425806\n",
      "0\n",
      "0.36426352141387597\n",
      "0\n",
      "0.3283602559273415\n",
      "2\n",
      "0.04134654842893437\n",
      "0\n",
      "0.4608859985495646\n",
      "2\n",
      "0.2002758043148688\n",
      "0\n",
      "0.28965881584679787\n",
      "0\n",
      "0.23291169169263826\n",
      "0\n",
      "0.6138603481610454\n",
      "0\n",
      "0.35023385324384027\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [146]\u001b[0m, in \u001b[0;36m<cell line: 203>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m dqn_agent \u001b[38;5;241m=\u001b[39m DQNAgent()\n\u001b[1;32m    201\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighway-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [146]\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    128\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[0;32m--> 130\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstore(state\u001b[38;5;241m=\u001b[39mstate, action\u001b[38;5;241m=\u001b[39maction, next_state\u001b[38;5;241m=\u001b[39mnext_state, reward\u001b[38;5;241m=\u001b[39mreward, done\u001b[38;5;241m=\u001b[39mdone)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience_replay()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:271\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    260\u001b[0m     action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    268\u001b[0m ):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/road.py:464\u001b[0m, in \u001b[0;36mRoad.act\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[0;32m--> 464\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/vehicle/behavior.py:115\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    110\u001b[0m action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\n\u001b[1;32m    111\u001b[0m     action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m front_vehicle, rear_vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane_index\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(\n\u001b[1;32m    119\u001b[0m     ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, front_vehicle\u001b[38;5;241m=\u001b[39mfront_vehicle, rear_vehicle\u001b[38;5;241m=\u001b[39mrear_vehicle\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/road.py:504\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[0;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    501\u001b[0m         v, Landmark\n\u001b[1;32m    502\u001b[0m     ):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    506\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/lane.py:212\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[0;34m(self, position)\u001b[0m\n\u001b[1;32m    210\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[1;32m    211\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[0;32m--> 212\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[0;32m<__array_function__ internals>:4\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "\n",
    "# Define model\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # output layer w\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "    \n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer_state = []\n",
    "        self.buffer_action = []\n",
    "        self.buffer_next_state = []\n",
    "        self.buffer_reward = []\n",
    "        self.buffer_done = []\n",
    "        self.idx = 0\n",
    "        self.memory = []\n",
    "    \n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        if len(self.buffer_state) < self.capacity:\n",
    "            self.buffer_state.append(state)\n",
    "            self.buffer_action.append(action)\n",
    "            self.buffer_next_state.append(next_state)\n",
    "            self.buffer_reward.append(reward)\n",
    "            self.buffer_done.append(done)\n",
    "        else:\n",
    "            self.buffer_state[self.idx] = state\n",
    "            self.buffer_action[self.idx] = action\n",
    "            self.buffer_next_state[self.idx] = next_state\n",
    "            self.buffer_reward[self.idx] = reward\n",
    "            self.buffer_done[self.idx] = done\n",
    "\n",
    "        self.idx = (self.idx+1)%self.capacity # for circular memory\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size >  len(self.buffer_state):\n",
    "            batch_size = len(self.buffer_state)\n",
    "        \n",
    "        indices_to_sample = random.sample(range(len(self.buffer_state)), batch_size)\n",
    "        states = torch.from_numpy(np.array(self.buffer_state)[indices_to_sample]).float()\n",
    "        actions = torch.from_numpy(np.array(self.buffer_action)[indices_to_sample])\n",
    "        next_states = torch.from_numpy(np.array(self.buffer_next_state)[indices_to_sample]).float()\n",
    "        rewards = torch.from_numpy(np.array(self.buffer_reward)[indices_to_sample]).float()\n",
    "        done = torch.from_numpy(np.array(self.buffer_done)[indices_to_sample])\n",
    "\n",
    "        return states, actions, next_states, rewards, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, memory_capacity=5000):\n",
    "        # weights\n",
    "        self.w1 = []\n",
    "        \n",
    "        # TODO: create the convolution network that takes the state and returns the possible state-actions\n",
    "        self.q_net = {}\n",
    "        self.w2 = []\n",
    "        self.q_target_net = self.q_net # the same\n",
    "        self.policy = {}\n",
    "\n",
    "        self.epsilon = 0.2\n",
    "        self.discount = 0.01\n",
    "        self.batch_size = 1\n",
    "        \n",
    "        # the weights of q1 and q2 should be the same\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.memory = {} # this is the memory buffer -> setting a limit\n",
    "        self.device = torch.device(\"mps\")\n",
    "    \n",
    "    def initialize_neural_networks(self, env):\n",
    "        # the lanes\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        # print(env.action_space.sample())\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = DQNNetwork(self.num_states, self.num_states, self.num_actions)\n",
    "        self.q_target_net = DQNNetwork(self.num_states, self.num_states, self.num_actions)\n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        # make the weights and biases the same\n",
    "        self.q_target_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n",
    "    def learn(self, env):\n",
    "        self.initialize_neural_networks(env)\n",
    "        \n",
    "        self.memory = ReplayMemory(capacity=self.memory_capacity)\n",
    "\n",
    "        env.reset()\n",
    "        num_episodes = 100\n",
    "        prefill_num = 2\n",
    "\n",
    "        self.prefill_memory(env, prefill_num)\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            \n",
    "            # True when agent reaches the end states (colliding or passing the time)\n",
    "            done = False \n",
    "            # True when agent takes more than 100 actions    \n",
    "            truncated = False  \n",
    "            while(not done and not truncated):\n",
    "            # for i in range(2):\n",
    "                # choose best action\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state=state, action=action, next_state=next_state, reward=reward, done=done)\n",
    "                self.experience_replay()\n",
    "                \n",
    "                self.update_target_network()\n",
    "                \n",
    "                state = next_state\n",
    "                # env.render()\n",
    "\n",
    "\n",
    "    # either the policies are able to get miltuple actions and into the NN or the input of NN should be able to handle all of these\n",
    "    # output (one of): {0: 'LANE_LEFT', 1: 'IDLE', 2: 'LANE_RIGHT', 3: 'FASTER', 4: 'SLOWER'}\n",
    "    def get_action(self, state):\n",
    "        # print(state)\n",
    "        print(random.random())\n",
    "        if random.random() <= self.epsilon: # amount of exploration reduces with the epsilon value\n",
    "            return random.randrange(self.num_actions)\n",
    "\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(np.array([state]), dtype=torch.float32)[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "        # pick the action with maximum Q-value in the Q-network \n",
    "            action = self.q_net(state)\n",
    "        # these actions are discrete, return index that has highest Q\n",
    "        # TODO: understand which we should be returning\n",
    "        return torch.argmax(action[0]).item() \n",
    "\n",
    "    def experience_replay(self):\n",
    "        states, actions, next_states, rewards, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        q_pred = self.q_net(states[0])\n",
    "        # q value of the action taken\n",
    "        q_pred = q_pred.gather(1, actions.view(-1, 1)) \n",
    "        \n",
    "        q_target = self.q_target_net(next_states)[0]\n",
    "        q_target = q_target.gather(1, actions.view(-1, 1))\n",
    "        \n",
    "        # setting Q(s',a') to 0 when the current state is a terminal state\n",
    "        q_target[dones] = 0.0 \n",
    "        \n",
    "        y_j = rewards + (self.discount * q_target)\n",
    "        y_j = y_j.view(-1, 1)\n",
    "        \n",
    "        # calculate the loss as the mean-squared error of yj and qpred\n",
    "        self.q_net.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(y_j, q_pred).mean()\n",
    "        loss.backward()\n",
    "        self.q_net.optimizer.step()\n",
    "        \n",
    "    def prefill_memory(self, env, prefill_num):\n",
    "        print(\"prefilling memory\")\n",
    "        for _ in range(prefill_num):\n",
    "            done = False\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            while not done:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                dqn_agent.memory.store(state=state, \n",
    "                                    action=action, \n",
    "                                    next_state=next_state, \n",
    "                                    reward=reward, \n",
    "                                    done=done)\n",
    "        print(\"prefilling memory done\")\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        print(\"gradient descent\")\n",
    "\n",
    "\n",
    "dqn_agent = DQNAgent()\n",
    "env = gym.make('highway-v0', render_mode='rgb_array')\n",
    "\n",
    "dqn_agent.learn(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with other model (also their DQN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # ouptut layer w\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "    \n",
    "    def store(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# FrozeLake Deep Q-Learning\n",
    "class FrozenLakeDQL():\n",
    "    # Hyperparameters (adjustable)\n",
    "    learning_rate_a = 0.001         # learning rate (alpha)\n",
    "    discount_factor_g = 0.9         # discount rate (gamma)    \n",
    "    network_sync_rate = 10          # number of steps the agent takes before syncing the policy and target network\n",
    "    replay_memory_size = 1000       # size of replay memory\n",
    "    mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
    "\n",
    "    # Neural Network\n",
    "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "    optimizer = None                # NN Optimizer. Initialize later.\n",
    "\n",
    "    ACTIONS = ['L','D','R','U']     # for printing 0,1,2,3 => L(eft),D(own),R(ight),U(p)\n",
    "\n",
    "    # Train the FrozeLake environment\n",
    "    def train(self, episodes, render=False, is_slippery=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human' if render else None)\n",
    "        num_states = env.observation_space.n\n",
    "        num_actions = env.action_space.n\n",
    "        \n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
    "        target_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
    "\n",
    "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        print('Policy (random, before training):')\n",
    "        self.print_dqn(policy_dqn)\n",
    "\n",
    "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else. \n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
    "        rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count=0\n",
    "            \n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions    \n",
    "\n",
    "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if random.random() < epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
    "                else:\n",
    "                    # select best action            \n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "                # Save experience into memory\n",
    "                memory.append((state, action, new_state, reward, terminated)) \n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "                # Increment step counter\n",
    "                step_count+=1\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            if reward == 1:\n",
    "                rewards_per_episode[i] = 1\n",
    "\n",
    "            # Check if enough experience has been collected and if at least 1 reward has been collected\n",
    "            if len(memory)>self.mini_batch_size and np.sum(rewards_per_episode)>0:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)        \n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1/episodes, 0)\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count=0\n",
    "\n",
    "        # Close environment\n",
    "        env.close()\n",
    "\n",
    "        # Save policy\n",
    "        torch.save(policy_dqn.state_dict(), \"frozen_lake_dql.pt\")\n",
    "\n",
    "        # Create new graph \n",
    "        plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        sum_rewards = np.zeros(episodes)\n",
    "        for x in range(episodes):\n",
    "            sum_rewards[x] = np.sum(rewards_per_episode[max(0, x-100):(x+1)])\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        plt.plot(sum_rewards)\n",
    "        \n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        plt.plot(epsilon_history)\n",
    "        \n",
    "        # Save plots\n",
    "        plt.savefig('frozen_lake_dql.png')\n",
    "\n",
    "    # Optimize policy network\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        # Get number of input nodes\n",
    "        num_states = policy_dqn.fc1.in_features\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "\n",
    "            if terminated: \n",
    "                # Agent either reached goal (reward=1) or fell into hole (reward=0)\n",
    "                # When in a terminated state, target q value should be set to the reward.\n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate target q value \n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state, num_states)).max()\n",
    "                    )\n",
    "\n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state, num_states))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state, num_states)) \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "                \n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    '''\n",
    "    Converts an state (int) to a tensor representation.\n",
    "    For example, the FrozenLake 4x4 map has 4x4=16 states numbered from 0 to 15. \n",
    "\n",
    "    Parameters: state=1, num_states=16\n",
    "    Return: tensor([0., 1., 0. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    '''\n",
    "    def state_to_dqn_input(self, state:int, num_states:int)->torch.Tensor:\n",
    "        input_tensor = torch.zeros(num_states)\n",
    "        input_tensor[state] = 1\n",
    "        return input_tensor\n",
    "\n",
    "    # Run the FrozeLake environment with the learned policy\n",
    "    def test(self, episodes, is_slippery=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human')\n",
    "        num_states = env.observation_space.n\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions) \n",
    "        policy_dqn.load_state_dict(torch.load(\"frozen_lake_dql.pt\"))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        print('Policy (trained):')\n",
    "        self.print_dqn(policy_dqn)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions            \n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):  \n",
    "                # Select best action   \n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    # Print DQN: state, best action, q values\n",
    "    def print_dqn(self, dqn):\n",
    "        # Get number of input nodes\n",
    "        num_states = dqn.fc1.in_features\n",
    "\n",
    "        # Loop each state and print policy to console\n",
    "        for s in range(num_states):\n",
    "            #  Format q values for printing\n",
    "            q_values = ''\n",
    "            for q in dqn(self.state_to_dqn_input(s, num_states)).tolist():\n",
    "                q_values += \"{:+.2f}\".format(q)+' '  # Concatenate q values, format to 2 decimals\n",
    "            q_values=q_values.rstrip()              # Remove space at the end\n",
    "\n",
    "            # Map the best action to L D R U\n",
    "            best_action = self.ACTIONS[dqn(self.state_to_dqn_input(s, num_states)).argmax()]\n",
    "\n",
    "            # Print policy in the format of: state, action, q values\n",
    "            # The printed layout matches the FrozenLake map.\n",
    "            print(f'{s:02},{best_action},[{q_values}]', end=' ')         \n",
    "            if (s+1)%4==0:\n",
    "                print() # Print a newline every 4 states\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    frozen_lake = FrozenLakeDQL()\n",
    "    is_slippery = False\n",
    "    frozen_lake.train(1000, is_slippery=is_slippery)\n",
    "    frozen_lake.test(10, is_slippery=is_slippery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
