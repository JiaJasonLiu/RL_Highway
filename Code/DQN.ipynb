{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:19:10.110768Z",
     "start_time": "2024-11-27T10:19:10.107277Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/deep-rl-dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running it without Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 16:24:05.748 python[2021:12877892] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-12-17 16:24:05.748 python[2021:12877892] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# env = gym.make('highway-v0', render_mode='rgb_array')\n",
    "# env.reset()\n",
    "# for _ in range(5):\n",
    "# # while True:\n",
    "#     action = env.unwrapped.action_type.actions_indexes[\"IDLE\"]\n",
    "#     obs, reward, done, truncated, info = env.step(action)\n",
    "#     env.render()\n",
    "\n",
    "# plt.imshow(env.render())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Their DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T10:19:24.572478Z",
     "start_time": "2024-11-27T10:19:24.554652Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "\n",
    "# Visualization utils\n",
    "%load_ext tensorboard\n",
    "import sys\n",
    "from tqdm.notebook import trange\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# doesn't work cause not linux\n",
    "# !apt-get install -y xvfb ffmpeg\n",
    "# !git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "# sys.path.insert(0, '/content/HighwayEnv/scripts/')\n",
    "# from utils import record_videos, show_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning using existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'observation': {'type': 'GrayscaleObservation', 'observation_shape': (128, 64), 'stack_size': 4, 'weights': [0.2989, 0.587, 0.114], 'scaling': 1.75}, 'action': {'type': 'DiscreteMetaAction'}, 'simulation_frequency': 5, 'policy_frequency': 2, 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle', 'screen_width': 600, 'screen_height': 150, 'centering_position': [0.3, 0.5], 'scaling': 5.5, 'show_trajectories': False, 'render_agent': True, 'offscreen_rendering': False, 'manual_control': False, 'real_time_rendering': False, 'lanes_count': 3, 'vehicles_count': 20, 'controlled_vehicles': 1, 'initial_lane_id': None, 'duration': 30, 'ego_spacing': 1.5, 'vehicles_density': 1, 'collision_reward': -1, 'right_lane_reward': 0.1, 'high_speed_reward': 0.4, 'lane_change_reward': 0, 'reward_speed_range': [20, 30], 'normalize_reward': True, 'offroad_terminal': False}\n",
      "Using mps device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to highway_dqn/DQN_9\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 100      |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 78       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 110      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 149      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x3069ea070>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for MLP\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 15,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-20, 20],\n",
    "            \"vy\": [-20, 20]\n",
    "        },\n",
    "        \"absolute\": False,\n",
    "        \"order\": \"sorted\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# For CNN\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"GrayscaleObservation\",\n",
    "        \"observation_shape\": (128, 64),\n",
    "        \"stack_size\": 4,\n",
    "        \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "        \"scaling\": 1.75,\n",
    "    },\n",
    "    \"policy_frequency\": 2\n",
    "}\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode='rgb_array', config=config)\n",
    "print(env.unwrapped.config)\n",
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "model = DQN('CnnPolicy', env,\n",
    "              policy_kwargs=dict(net_arch=[256, 256]),\n",
    "              learning_rate=5e-4,\n",
    "              buffer_size=15000,\n",
    "              learning_starts=200,\n",
    "              batch_size=32,\n",
    "              gamma=0.8,\n",
    "              device=mps_device,\n",
    "              train_freq=1,\n",
    "              gradient_steps=1,\n",
    "              target_update_interval=50,\n",
    "              verbose=1,\n",
    "              tensorboard_log=\"highway_dqn/\")\n",
    "\n",
    "model.learn(int(2e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighway_dqn/model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"highway_dqn/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test saved model\n",
    "model = DQN.load(\"highway_dqn/model\")\n",
    "\n",
    "# while True:\n",
    "for i in range(10):\n",
    "  done = truncated = False\n",
    "  obs, info = env.reset()\n",
    "  while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Own Implementations\n",
    "\n",
    "### DQN\n",
    "\n",
    "Chose to first make naive work and then make the normal work\n",
    "\n",
    "\n",
    "The following is the pesudocode that will be followed when creating the DQN\n",
    "\n",
    "Useful: https://www.youtube.com/watch?v=RVMpm86equc&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=2\n",
    "\n",
    "https://github.com/saashanair/rl-series/tree/master/dqn\n",
    "\n",
    "https://github.com/johnnycode8/gym_solutions/blob/main/frozen_lake_dql.py\n",
    "\n",
    "<img src=\"DQN.png\" style=\"width: 900px;\" align=\"left\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "\n",
    "# Define model\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # output layer w\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "    \n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer_state = []\n",
    "        self.buffer_action = []\n",
    "        self.buffer_next_state = []\n",
    "        self.buffer_reward = []\n",
    "        self.buffer_done = []\n",
    "        self.idx = 0\n",
    "        self.memory = []\n",
    "    \n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        if len(self.buffer_state) < self.capacity:\n",
    "            self.buffer_state.append(state)\n",
    "            self.buffer_action.append(action)\n",
    "            self.buffer_next_state.append(next_state)\n",
    "            self.buffer_reward.append(reward)\n",
    "            self.buffer_done.append(done)\n",
    "        else:\n",
    "            self.buffer_state[self.idx] = state\n",
    "            self.buffer_action[self.idx] = action\n",
    "            self.buffer_next_state[self.idx] = next_state\n",
    "            self.buffer_reward[self.idx] = reward\n",
    "            self.buffer_done[self.idx] = done\n",
    "\n",
    "        self.idx = (self.idx+1)%self.capacity # for circular memory\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size >  len(self.buffer_state):\n",
    "            batch_size = len(self.buffer_state)\n",
    "        \n",
    "        indices_to_sample = random.sample(range(len(self.buffer_state)), batch_size)\n",
    "        states = torch.from_numpy(np.array(self.buffer_state)[indices_to_sample]).float()\n",
    "        actions = torch.from_numpy(np.array(self.buffer_action)[indices_to_sample])\n",
    "        next_states = torch.from_numpy(np.array(self.buffer_next_state)[indices_to_sample]).float()\n",
    "        rewards = torch.from_numpy(np.array(self.buffer_reward)[indices_to_sample]).float()\n",
    "        done = torch.from_numpy(np.array(self.buffer_done)[indices_to_sample])\n",
    "\n",
    "        return states, actions, next_states, rewards, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, memory_capacity=5000):\n",
    "        # weights\n",
    "        self.w1 = []\n",
    "        \n",
    "        # TODO: create the convolution network that takes the state and returns the possible state-actions\n",
    "        self.q_net = {}\n",
    "        self.w2 = []\n",
    "        self.q_target_net = self.q_net # the same\n",
    "        self.policy = {}\n",
    "\n",
    "        self.epsilon = 0.00005\n",
    "        self.discount = 0.01\n",
    "        self.batch_size = 1\n",
    "        \n",
    "        # the weights of q1 and q2 should be the same\n",
    "        self.memory_capacity = memory_capacity\n",
    "        self.memory = {} # this is the memory buffer -> setting a limit\n",
    "        self.device = torch.device(\"mps\")\n",
    "    \n",
    "    def initialize_neural_networks(self, env):\n",
    "        # the lanes\n",
    "        self.num_states = env.observation_space.shape[0]\n",
    "        # print(env.action_space.sample())\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = DQNNetwork(self.num_states, self.num_states, self.num_actions)\n",
    "        self.q_target_net = DQNNetwork(self.num_states, self.num_states, self.num_actions)\n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        # make the weights and biases the same\n",
    "        self.q_target_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n",
    "    def learn(self, env):\n",
    "        self.initialize_neural_networks(env)\n",
    "        \n",
    "        self.memory = ReplayMemory(capacity=self.memory_capacity)\n",
    "\n",
    "        env.reset()\n",
    "        num_episodes = 1\n",
    "        time_step = 2\n",
    "        prefill_num = 2\n",
    "\n",
    "        # self.prefill_memory(env, prefill_num)\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            \n",
    "            # True when agent reaches the end states (colliding or passing the time)\n",
    "            done = False \n",
    "            # True when agent takes more than 100 actions    \n",
    "            truncated = False  \n",
    "            # while(not done and not truncated):\n",
    "            for i in range(2):\n",
    "                # choose best action\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state=state, action=action, next_state=next_state, reward=reward, done=done)\n",
    "                self.experience_replay()\n",
    "                \n",
    "                self.update_target_network()\n",
    "                \n",
    "                state = next_state\n",
    "                # env.render()\n",
    "\n",
    "    #   either the policies are able to get miltuple actions and into the NN or the input of NN should be able to handle all of these\n",
    "    # output (one of): {0: 'LANE_LEFT', 1: 'IDLE', 2: 'LANE_RIGHT', 3: 'FASTER', 4: 'SLOWER'}\n",
    "    def get_action(self, state):\n",
    "        # print(state)\n",
    "        if random.random() <= self.epsilon: # amount of exploration reduces with the epsilon value\n",
    "            return random.randrange(self.num_actions)\n",
    "\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(np.array([state]), dtype=torch.float32)[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "        # pick the action with maximum Q-value in the Q-network \n",
    "            action = self.q_net(state)\n",
    "        # these actions are discrete, return index that has highest Q\n",
    "        # TODO: understand which we should be returning\n",
    "        return torch.argmax(action[0]).item() \n",
    "\n",
    "    def experience_replay(self):\n",
    "        states, actions, next_states, rewards, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        q_pred = self.q_net(states[0])\n",
    "        # q value of the action taken\n",
    "        q_pred = q_pred.gather(1, actions.view(-1, 1)) \n",
    "        \n",
    "        q_target = self.q_target_net(next_states)[0]\n",
    "        q_target = q_target.gather(1, actions.view(-1, 1))\n",
    "        \n",
    "        # setting Q(s',a') to 0 when the current state is a terminal state\n",
    "        q_target[dones] = 0.0 \n",
    "        \n",
    "        y_j = rewards + (self.discount * q_target)\n",
    "        y_j = y_j.view(-1, 1)\n",
    "        \n",
    "        # calculate the loss as the mean-squared error of yj and qpred\n",
    "        self.q_net.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(y_j, q_pred).mean()\n",
    "        loss.backward()\n",
    "        self.q_net.optimizer.step()\n",
    "        \n",
    "    def prefill_memory(self, env, prefill_num):\n",
    "        print(\"prefilling memory\")\n",
    "        for _ in range(prefill_num):\n",
    "            done = False\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            while not done:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                dqn_agent.memory.store(state=state, \n",
    "                                    action=action, \n",
    "                                    next_state=next_state, \n",
    "                                    reward=reward, \n",
    "                                    done=done)\n",
    "        print(\"prefilling memory done\")\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        print(\"gradient descent\")\n",
    "\n",
    "\n",
    "dqn_agent = DQNAgent()\n",
    "env = gym.make('highway-v0', render_mode='rgb_array')\n",
    "\n",
    "# print(env.unwrapped.config)\n",
    "# print(env.unwrapped.action_type.actions)\n",
    "# print(env.unwrapped.action_type.actions_indexes[\"IDLE\"])\n",
    "\n",
    "dqn_agent.learn(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with other model (also their DQN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # ouptut layer w\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "    \n",
    "    def store(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# FrozeLake Deep Q-Learning\n",
    "class FrozenLakeDQL():\n",
    "    # Hyperparameters (adjustable)\n",
    "    learning_rate_a = 0.001         # learning rate (alpha)\n",
    "    discount_factor_g = 0.9         # discount rate (gamma)    \n",
    "    network_sync_rate = 10          # number of steps the agent takes before syncing the policy and target network\n",
    "    replay_memory_size = 1000       # size of replay memory\n",
    "    mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
    "\n",
    "    # Neural Network\n",
    "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "    optimizer = None                # NN Optimizer. Initialize later.\n",
    "\n",
    "    ACTIONS = ['L','D','R','U']     # for printing 0,1,2,3 => L(eft),D(own),R(ight),U(p)\n",
    "\n",
    "    # Train the FrozeLake environment\n",
    "    def train(self, episodes, render=False, is_slippery=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human' if render else None)\n",
    "        num_states = env.observation_space.n\n",
    "        num_actions = env.action_space.n\n",
    "        \n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
    "        target_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
    "\n",
    "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        print('Policy (random, before training):')\n",
    "        self.print_dqn(policy_dqn)\n",
    "\n",
    "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else. \n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
    "        rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count=0\n",
    "            \n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions    \n",
    "\n",
    "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if random.random() < epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
    "                else:\n",
    "                    # select best action            \n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "                # Save experience into memory\n",
    "                memory.append((state, action, new_state, reward, terminated)) \n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "                # Increment step counter\n",
    "                step_count+=1\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            if reward == 1:\n",
    "                rewards_per_episode[i] = 1\n",
    "\n",
    "            # Check if enough experience has been collected and if at least 1 reward has been collected\n",
    "            if len(memory)>self.mini_batch_size and np.sum(rewards_per_episode)>0:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)        \n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1/episodes, 0)\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count=0\n",
    "\n",
    "        # Close environment\n",
    "        env.close()\n",
    "\n",
    "        # Save policy\n",
    "        torch.save(policy_dqn.state_dict(), \"frozen_lake_dql.pt\")\n",
    "\n",
    "        # Create new graph \n",
    "        plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        sum_rewards = np.zeros(episodes)\n",
    "        for x in range(episodes):\n",
    "            sum_rewards[x] = np.sum(rewards_per_episode[max(0, x-100):(x+1)])\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        plt.plot(sum_rewards)\n",
    "        \n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        plt.plot(epsilon_history)\n",
    "        \n",
    "        # Save plots\n",
    "        plt.savefig('frozen_lake_dql.png')\n",
    "\n",
    "    # Optimize policy network\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        # Get number of input nodes\n",
    "        num_states = policy_dqn.fc1.in_features\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "\n",
    "            if terminated: \n",
    "                # Agent either reached goal (reward=1) or fell into hole (reward=0)\n",
    "                # When in a terminated state, target q value should be set to the reward.\n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate target q value \n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state, num_states)).max()\n",
    "                    )\n",
    "\n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state, num_states))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state, num_states)) \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "                \n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    '''\n",
    "    Converts an state (int) to a tensor representation.\n",
    "    For example, the FrozenLake 4x4 map has 4x4=16 states numbered from 0 to 15. \n",
    "\n",
    "    Parameters: state=1, num_states=16\n",
    "    Return: tensor([0., 1., 0. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "    '''\n",
    "    def state_to_dqn_input(self, state:int, num_states:int)->torch.Tensor:\n",
    "        input_tensor = torch.zeros(num_states)\n",
    "        input_tensor[state] = 1\n",
    "        return input_tensor\n",
    "\n",
    "    # Run the FrozeLake environment with the learned policy\n",
    "    def test(self, episodes, is_slippery=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human')\n",
    "        num_states = env.observation_space.n\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions) \n",
    "        policy_dqn.load_state_dict(torch.load(\"frozen_lake_dql.pt\"))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        print('Policy (trained):')\n",
    "        self.print_dqn(policy_dqn)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions            \n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):  \n",
    "                # Select best action   \n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    # Print DQN: state, best action, q values\n",
    "    def print_dqn(self, dqn):\n",
    "        # Get number of input nodes\n",
    "        num_states = dqn.fc1.in_features\n",
    "\n",
    "        # Loop each state and print policy to console\n",
    "        for s in range(num_states):\n",
    "            #  Format q values for printing\n",
    "            q_values = ''\n",
    "            for q in dqn(self.state_to_dqn_input(s, num_states)).tolist():\n",
    "                q_values += \"{:+.2f}\".format(q)+' '  # Concatenate q values, format to 2 decimals\n",
    "            q_values=q_values.rstrip()              # Remove space at the end\n",
    "\n",
    "            # Map the best action to L D R U\n",
    "            best_action = self.ACTIONS[dqn(self.state_to_dqn_input(s, num_states)).argmax()]\n",
    "\n",
    "            # Print policy in the format of: state, action, q values\n",
    "            # The printed layout matches the FrozenLake map.\n",
    "            print(f'{s:02},{best_action},[{q_values}]', end=' ')         \n",
    "            if (s+1)%4==0:\n",
    "                print() # Print a newline every 4 states\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    frozen_lake = FrozenLakeDQL()\n",
    "    is_slippery = False\n",
    "    frozen_lake.train(1000, is_slippery=is_slippery)\n",
    "    frozen_lake.test(10, is_slippery=is_slippery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
