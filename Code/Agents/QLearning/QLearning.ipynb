{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import json\n",
    "import random\n",
    "import highway_env\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from metrics import Metrics\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, params):\n",
    "        self.env = env\n",
    " \n",
    "        self.exploration_rate = params.get(\"exploration_rate\", 0.3)\n",
    "        self.q_table = defaultdict()\n",
    "        self.q_table_path = \"q_table.json\"\n",
    "        self.load_q_table()\n",
    "        self.action_space = env.action_space.n\n",
    "        use_metrics = params.get(\"use_metrics\", False)\n",
    "        self.leaning_rate = params.get(\"learning_rate\", 0.1)\n",
    "\n",
    "        self.discount_factor = params.get(\"gamma\", 0.9) # Discount Factor\n",
    "        self.episode_num = params.get(\"episode_num\", 100)\n",
    "        self.metrics = Metrics(\"value_iteration\", \"training_results\", use_metrics)\n",
    "\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return self.env.action_space.sample()  # Explore: random action\n",
    "        else:\n",
    "            try:\n",
    "                return str(max(self.q_table[state], key = self.q_table[state].get))\n",
    "            except:\n",
    "                return self.env.action_space.sample()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        total_rewards = []\n",
    "        steps_per_episode = []\n",
    "        average_rewards = []\n",
    "        for episode in tqdm(range(self.episode_num), desc=\"Training Model\"):\n",
    "            state = str(self.env.reset()[0])  # Convert state to string for indexing\n",
    "            done = False\n",
    "            truncated = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            while not done and not truncated:\n",
    "                steps += 1\n",
    "                action = str(self.choose_action(state))\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "                next_state = str(next_obs)\n",
    "\n",
    "                if state not in self.q_table:\n",
    "                    self.q_table[state] = {str(i): 0 for i in range(0, self.action_space)}\n",
    "\n",
    "\n",
    "                if next_state not in self.q_table:\n",
    "                    self.q_table[next_state] = {str(i): 0 for i in range(0, self.action_space)}\n",
    "\n",
    "                best_next_action = str(max(self.q_table[next_state], key = self.q_table[next_state].get))\n",
    "                \n",
    "                self.q_table[state][action] = self.q_table[state][action] + self.leaning_rate * (reward + self.discount_factor * self.q_table[next_state][best_next_action] - self.q_table[state][action])\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                total_reward += reward\n",
    "            \n",
    "\n",
    "            total_rewards.append(total_reward)\n",
    "            steps_per_episode.append(steps)\n",
    "            average_rewards.append(np.mean(total_rewards)) \n",
    "\n",
    "            self.exploration_rate = max(0.01, self.exploration_rate * 0.995)\n",
    "\n",
    "\n",
    "            self.metrics.add(\"rollout/rewards\", sum(total_rewards) / len(total_rewards), episode)\n",
    "            self.metrics.add(\"rollout/steps\", sum(steps_per_episode) / len(steps_per_episode), episode)\n",
    "\n",
    "            self.metrics.add(\"rollout/episode-length\", steps, episode)\n",
    "\n",
    "\n",
    "            if (episode + 1) % 50 == 0:\n",
    "                print(f\"Episode {episode + 1}/{self.episode_num}\")\n",
    "\n",
    "\n",
    "        self.save_q_table()\n",
    "        self.metrics.close()\n",
    "\n",
    "    def evaluate(self, episodes = 10):\n",
    "        for episode in tqdm(range(episodes), desc=\"Evaluate Model\"):\n",
    "            state = str(self.env.reset()[0])\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            step = 0\n",
    "            \n",
    "            while not done:\n",
    "                step += 1\n",
    "                \n",
    "                try:\n",
    "                    action = str(max(self.q_table[state], key = self.q_table[state].get))\n",
    "                except:\n",
    "                    action = self.env.action_space.sample()\n",
    "\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "                state = str(next_obs)\n",
    "                total_reward += reward\n",
    "                if total_reward > 50:\n",
    "                    break\n",
    "                self.env.render()\n",
    "\n",
    "            # print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Steps: {step}\")\n",
    "\n",
    "    def load_q_table(self):\n",
    "        if os.path.exists(self.q_table_path):\n",
    "            try:\n",
    "                with open(self.q_table_path, 'r') as file:\n",
    "                # self.q_table = np.load(self.q_table_path, allow_pickle=True).item()\n",
    "                    loaded =  json.load(file)\n",
    "                    self.q_table.update(loaded)\n",
    "                    # print(type(self.q_table))\n",
    "                    print(\"Q-table loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading Q-table: {e}\")\n",
    "\n",
    "    def save_q_table(self):\n",
    "        try:\n",
    "            with open(self.q_table_path, 'w') as file:  \n",
    "                json.dump(self.q_table, file, indent=4)\n",
    "                print(\"Q-table saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving Q-table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"lanes_count\": 3,\n",
    "    \"observation\": {\n",
    "        \"type\": \"TimeToCollision\",\n",
    "        \"horizon\": 5,\n",
    "    }}\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config=config)\n",
    "\n",
    "params = {\n",
    "    \"use_metrics\": True,\n",
    "    \"episode_num\": 1,\n",
    "    \"gamma\": 0.9, # Discount Factor\n",
    "    \"exploration_rate\": 0.3,\n",
    "    \"learning_rate\": 0.1,\n",
    "}\n",
    "\n",
    "agent = QLearningAgent(env, params=params)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate Model:   0%|          | 0/10 [00:00<?, ?it/s]2025-01-05 13:25:13.655 python[10665:1534212] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-05 13:25:13.655 python[10665:1534212] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "Evaluate Model: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"highway-v0\", render_mode=\"rgb_array\",   config=config)\n",
    "agent.evaluate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Traceback (most recent call last):\n",
       "  File \"C:\\Users\\ZhurL\\anaconda3\\envs\\rl2\\Scripts\\tensorboard-script.py\", line 6, in <module>\n",
       "    from tensorboard.main import run_main\n",
       "  File \"C:\\Users\\ZhurL\\anaconda3\\envs\\rl2\\lib\\site-packages\\tensorboard\\main.py\", line 27, in <module>\n",
       "    from tensorboard import default\n",
       "  File \"C:\\Users\\ZhurL\\anaconda3\\envs\\rl2\\lib\\site-packages\\tensorboard\\default.py\", line 39, in <module>\n",
       "    from tensorboard.plugins.hparams import hparams_plugin\n",
       "  File \"C:\\Users\\ZhurL\\anaconda3\\envs\\rl2\\lib\\site-packages\\tensorboard\\plugins\\hparams\\hparams_plugin.py\", line 30, in <module>\n",
       "    from tensorboard.plugins.hparams import backend_context\n",
       "  File \"C:\\Users\\ZhurL\\anaconda3\\envs\\rl2\\lib\\site-packages\\tensorboard\\plugins\\hparams\\backend_context.py\", line 26, in <module>\n",
       "    from tensorboard.plugins.hparams import metadata\n",
       "  File \"C:\\Users\\ZhurL\\anaconda3\\envs\\rl2\\lib\\site-packages\\tensorboard\\plugins\\hparams\\metadata.py\", line 32, in <module>\n",
       "    NULL_TENSOR = tensor_util.make_tensor_proto(\n",
       "  File \"C:\\Users\\ZhurL\\anaconda3\\envs\\rl2\\lib\\site-packages\\tensorboard\\util\\tensor_util.py\", line 405, in make_tensor_proto\n",
       "    numpy_dtype = dtypes.as_dtype(nparray.dtype)\n",
       "  File \"C:\\Users\\ZhurL\\anaconda3\\envs\\rl2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py\", line 677, in as_dtype\n",
       "    if type_value.type == np.string_ or type_value.type == np.unicode_:\n",
       "  File \"C:\\Users\\ZhurL\\anaconda3\\envs\\rl2\\lib\\site-packages\\numpy\\__init__.py\", line 397, in __getattr__\n",
       "    raise AttributeError(\n",
       "AttributeError: `np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir training_results --host localhost --port 6012"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
