{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Install environment and agent\n",
    "# !pip install highway-env\n",
    "# !pip install --upgrade sympy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning using existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the pesudocode that will be followed when creating the DQN\n",
    "\n",
    "Useful: https://www.youtube.com/watch?v=RVMpm86equc&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=2\n",
    "\n",
    "https://github.com/saashanair/rl-series/tree/master/dqn\n",
    "\n",
    "https://github.com/johnnycode8/gym_solutions/blob/main/frozen_lake_dql.py\n",
    "\n",
    "<img src=\"DQN.png\" style=\"width: 900px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "Potential Problems: https://www.reddit.com/r/reinforcementlearning/comments/1555wgi/dqn_loss_increasing_and_rewards_decreasing/\n",
    "\n",
    "\n",
    "For CNN:\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/3l5qu7/rules_of_thumb_for_cnn_architectures/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros( 2 * capacity - 1 )\n",
    "        self.data = np.zeros( capacity, dtype=object )\n",
    "        self.writer = 0\n",
    "\n",
    "    # propagate upwards to update the sum values\n",
    "    def _propagate(self, index, change):\n",
    "        parent = (index - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # get the leaf nodes (Transaction)\n",
    "    def _retrieve(self, index, s):\n",
    "        left = 2 * index + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return index\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        index = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(index, p)\n",
    "\n",
    "        self.write += 1\n",
    "        # circular\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, index, p):\n",
    "        change = p - self.tree[index]\n",
    "\n",
    "        self.tree[index] = p\n",
    "        self._propagate(index, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        index = self._retrieve(0, s)\n",
    "        data_index = index - self.capacity + 1\n",
    "\n",
    "        return (index, self.tree[index], self.data[data_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "\n",
    "# Define model\n",
    "class MLPNetwork(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super(MLPNetwork, self).__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # output layer\n",
    "        self.out2 = nn.Linear(out_actions, 1) # output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = F.relu(self.out(x))         \n",
    "        x = self.out2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CNN, self).__init__()\n",
    "        # greyscale Image is(stack,height,width)\n",
    "        stack, height, width = input_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(stack,32,kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=4,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(32,64,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(64,128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv2d(128,256, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        # This is for finding the size to dense more robust compared to decision manually\n",
    "        with torch.no_grad():\n",
    "                # Torch uses(1,channels,height,width)\n",
    "                test = torch.zeros(1, stack, height, width)\n",
    "                find_conv_size = self.conv(test)\n",
    "                conv_size = find_conv_size.numel()\n",
    "        self.fc = nn.Linear(conv_size,num_actions)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # self.tan = nn.Tanh(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define memory for Experience Replay\n",
    "# TODO: Prioritize, n-steps\n",
    "\n",
    "from collections.__init__ import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, config, transition_type=Transition):\n",
    "        self.capacity = config.get(\"capacity\", 1000)\n",
    "        self.index = 0\n",
    "        self.transition_type = transition_type\n",
    "        self.discount = config.get(\"discount\", 0.99)\n",
    "\n",
    "        self.memory = []\n",
    "        self.device = config.get(\"device\", torch.device(\"cpu\"))\n",
    "        self.n_steps = config.get(\"n_steps\", 2)\n",
    "    \n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.index = len(self.memory) - 1\n",
    "        elif len(self.memory) > self.capacity:\n",
    "            self.memory = self.memory[:self.capacity]\n",
    "        # Faster than append and pop\n",
    "        self.memory[self.index] = self.transition_type(*args)\n",
    "        \n",
    "        self.index = (self.index+1)%self.capacity # for circular memory\n",
    "\n",
    "    def sample(self, batch_size, collapsed=True):\n",
    "        if batch_size >  len(self.memory):\n",
    "            batch_size = len(self.memory)\n",
    "            \n",
    "        if self.n_steps <= 1:\n",
    "            # Directly sample transitions\n",
    "            memories = random.sample(self.memory, batch_size)\n",
    "            return self.unwrap_transition(*memories)\n",
    "        else:\n",
    "            # Sample initial transition indexes\n",
    "            indexes = random.sample(range(len(self.memory)), batch_size)\n",
    "            # Get the batch of n-consecutive-transitions starting from sampled indexes\n",
    "            all_transitions = [self.memory[i:i+self.n_steps] for i in indexes]\n",
    "            \n",
    "            memories = map(self.collapse_n_steps, all_transitions) if collapsed else all_transitions\n",
    "\n",
    "            return self.unwrap_transition(*memories)\n",
    "        \n",
    "    def collapse_n_steps(self, transitions):\n",
    "        state, action, next_state, reward, done = transitions[0]\n",
    "        discount = self.discount\n",
    "        for transition in transitions[1:]:\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                _, _, next_state, reward, done = transition\n",
    "                discount *= self.discount\n",
    "                reward += discount * reward\n",
    "        return state, action, next_state, reward, done\n",
    "    \n",
    "    def unwrap_transition(self, *transition):\n",
    "        state, action, next_state, reward, done = zip(*transition)\n",
    "        \n",
    "        states = torch.from_numpy(np.array(state)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(action)).to(self.device)\n",
    "        next_states = torch.from_numpy(np.array(next_state)).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(reward)).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(done)).to(self.device)\n",
    "\n",
    "        return states, actions, next_states, rewards, dones \n",
    "\n",
    "class PrioritizedReplayMemory(ReplayMemory):\n",
    "    def __init__(self, config, transition_type=Transition):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.capacity = config.get(\"capacity\", 1000)\n",
    "        self.tree = SumTree(self.capacity)\n",
    "        self.index = 0\n",
    "        self.transition_type = transition_type\n",
    "\n",
    "        self.memory = []\n",
    "        self.device = config.get(\"device\", torch.device(\"cpu\"))\n",
    "        \n",
    "        self.alpha = config.get(\"alpha\", 0.6)\n",
    "        self.beta = config.get(\"beta\", 0.2) #  will go to 1\n",
    "        self.max_priority = 1  # priority for new samples, init as eps\n",
    "        \n",
    "    def store(self, *args):\n",
    "        super().store(*args)\n",
    "        self.tree.add(self.max_priority ** self.alpha, )\n",
    "    \n",
    "    def sample(self, batch_size, collapsed=True):\n",
    "        priorities = torch.empty(batch_size, 1, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, policy, result_file_name, use_metrics, time):\n",
    "        self.use_metrics = use_metrics\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        new_num = str(len(os.listdir(\"./\" +result_file_name)) + 1)\n",
    "        file_name = f'{result_file_name}/{policy}_DQN_{new_num}_{time}'\n",
    "        self.writer = SummaryWriter(log_dir=file_name, flush_secs=60)\n",
    "            \n",
    "    def add(self, type, y, x):\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        self.writer.add_scalar(type, y, x)\n",
    "    def close(self):\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, params):\n",
    "        self.q_net = {}\n",
    "        self.q_target_net = {} \n",
    "        self.optimizer= {}        \n",
    "        \n",
    "        self.policy = params.get(\"policy\", \"CnnPolicy\")        \n",
    "        self.episode_num = params.get(\"episode_num\", 10)\n",
    "\n",
    "        self.epsilon = params.get(\"epsilon_max \", 1)\n",
    "        self.epsilon_min = params.get(\"epsilon_min \", 0.1)\n",
    "        self.epsilon_decay = params.get(\"epsilon_decay\", 0.995)\n",
    "        \n",
    "        self.learning_rate = params.get(\"learning_rate\", 3e-4)\n",
    "        self.discount = params.get(\"discount\", 0.2)\n",
    "        self.batch_size = params.get(\"batch_size\", 32 )\n",
    "        self.device = params.get(\"device\", torch.device(\"cpu\"))\n",
    "        \n",
    "        self.memory_capacity = params.get(\"memory_capacity\", 1000)\n",
    "        self.memory = {} # this is the memory buffer -> setting a limit\n",
    "        self.n_steps = params.get(\"n_steps\", 2)\n",
    "        self.prioritize_memory = params.get(\"prioritize_memory\", False)\n",
    "        \n",
    "        self.double = params.get(\"double\", False)\n",
    "        \n",
    "        self.timeout = params.get(\"timeout_minute\", 0) * 60 # in minutes\n",
    "        ct = datetime.datetime.now()\n",
    "        self.time = str(ct).replace(\" \", \"|\")\n",
    "        self.to_save_model = params.get(\"save_model\", False)\n",
    "        \n",
    "        use_metrics = params.get(\"use_metrics\", False)\n",
    "        if use_metrics:\n",
    "            self.create_folder(\"training_results\")\n",
    "            self.save_params(params)\n",
    "            \n",
    "        self.metrics = Metrics(self.policy, \"training_results\", use_metrics, self.time)\n",
    "        \n",
    "    def create_network(self, env):\n",
    "        if self.policy == \"CnnPolicy\":\n",
    "            self.create_CNN(env)\n",
    "        \n",
    "        if self.policy == \"MlpPolicy\":\n",
    "            self.create_MLP_Network(env)\n",
    "            \n",
    "        self.update_target_network()\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)\n",
    "    \n",
    "    def create_CNN(self, env):\n",
    "        self.num_states = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = CNN(self.num_states, self.num_actions).to(self.device)\n",
    "        self.q_target_net = CNN(self.num_states, self.num_actions).to(self.device)\n",
    "        \n",
    "    \n",
    "    def create_MLP_Network(self, env):\n",
    "        # the lanes\n",
    "        self.num_states = env.observation_space.shape[1]\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = MLPNetwork(self.num_states, self.num_states, self.num_actions).to(self.device)\n",
    "        self.q_target_net = MLPNetwork(self.num_states, self.num_states, self.num_actions).to(self.device)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        # make the weights and biases the same\n",
    "        self.q_target_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n",
    "    def learn(self, env):\n",
    "        self.create_network(env)\n",
    "        \n",
    "        # if (self.prioritize_memory):\n",
    "        self.memory = ReplayMemory({\n",
    "            \"capacity\": self.memory_capacity,\n",
    "            \"device\": self.device,\n",
    "            \"n_steps\": self.n_steps,\n",
    "        })\n",
    "\n",
    "        self.prefill_memory(env, self.batch_size)\n",
    "\n",
    "        start_time = time.time()        \n",
    "        \n",
    "        for epoch in tqdm(range(self.episode_num), desc=\"Training Model\"):\n",
    "            state = env.reset()[0]\n",
    "            \n",
    "            # True when agent reaches the end states (colliding or passing the time)\n",
    "            done = False \n",
    "            \n",
    "            # TODO: see how many actions until truncate\n",
    "            # True when agent takes more than some actions \n",
    "            truncated = False\n",
    "            episode_rewards = []\n",
    "            episode_loss = []\n",
    "            episode_len = 0\n",
    "            while(not done and not truncated):\n",
    "                # choose best action\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)\n",
    "                \n",
    "                episode_loss.append(self.experience_replay())\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                episode_len += 1\n",
    "                \n",
    "            self.metrics.add(\"rollout/rewards\", sum(episode_rewards) / len(episode_rewards), epoch)\n",
    "            self.metrics.add(\"rollout/exploration-rate\", self.epsilon, epoch)\n",
    "            self.metrics.add(\"rollout/episode-length\", episode_len, epoch)\n",
    "            self.metrics.add(\"train/loss\", sum(episode_loss) / len(episode_loss), epoch)\n",
    "            \n",
    "            if self.timeout:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                if elapsed_time > self.timeout:\n",
    "                    print(\"Timeout reached. Stopping training.\\n\")\n",
    "                    break\n",
    "            \n",
    "            # if epoch % 1000 == 0:\n",
    "            self.decay_epsilon()\n",
    "            self.update_target_network()\n",
    "            \n",
    "        self.metrics.close()\n",
    "        \n",
    "        if self.to_save_model:\n",
    "            self.save_model()\n",
    "        \n",
    "    # either the policies are able to get miltuple actions and into the NN or the input of NN should be able to handle all of these\n",
    "    # output (one of): {0: 'LANE_LEFT', 1: 'IDLE', 2: 'LANE_RIGHT', 3: 'FASTER', 4: 'SLOWER'}\n",
    "    def get_action(self, state, eval_mode=False):\n",
    "        if random.random() <= self.epsilon and not eval_mode: # amount of exploration reduces with the epsilon value\n",
    "            return random.randrange(self.num_actions)\n",
    "        \n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float32).to(self.device)\n",
    "        actions = self.q_net(state)\n",
    "        return torch.argmax(actions).item()             \n",
    "\n",
    "    def experience_replay(self):\n",
    "        states, actions, next_states, rewards, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        q_pred = self.q_net(states)\n",
    "        # q value of the action taken\n",
    "        q_pred = q_pred.gather(1, actions.view(-1, 1)) \n",
    "        q_pred = q_pred.squeeze(1)\n",
    "\n",
    "        # Double DQN\n",
    "        if self.double:\n",
    "             # pick best actions from policy network\n",
    "            q_best_action = self.q_net(next_states)\n",
    "            _, q_best_action = q_best_action.max(dim=1)\n",
    "            q_best_action = q_best_action.unsqueeze(1)\n",
    "            \n",
    "            # use those actions for the target policy\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.gather(1, q_best_action)\n",
    "            q_target = q_target.squeeze(1)\n",
    "        else:\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.max(dim=1).values\n",
    "        \n",
    "        # setting Q(s',a') to 0 when the current state is a terminal state\n",
    "        q_target[dones] = 0.0\n",
    "        \n",
    "        y_j = rewards + (self.discount * q_target)\n",
    "        \n",
    "        # calculate the loss as the mean-squared error of yj and qpred\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(y_j, q_pred).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "    def prefill_memory(self, env, prefill_num):\n",
    "        for _ in tqdm(range(prefill_num), desc=\"Prefilling Memory \"):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            while not done and not truncated:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)    \n",
    "                \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "    \n",
    "    def evaluate(self, env, episode_num):\n",
    "        # add camera here\n",
    "        for _ in tqdm(range(episode_num), desc=\"Evaluating Model\"):\n",
    "            state = env.reset()[0]  \n",
    "            done = False      \n",
    "            truncated = False \n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not done and not truncated):  \n",
    "                # Select best action   \n",
    "                action = self.get_action(state, eval_mode=True)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                env.render()\n",
    "        \n",
    "    def save_model(self):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "        self.create_folder(folder_name)\n",
    "        new_model_num = str(len(os.listdir(\"./\" +folder_name)) + 1)\n",
    "        file_name = f'{folder_name}/DQN_{new_model_num}_{self.time}.pth'\n",
    "        state = {'state_dict': self.q_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()}\n",
    "        torch.save(state, file_name)\n",
    "        \n",
    "    def load_model(self, env, file_name):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "                \n",
    "        filename = folder_name + \"/\" + file_name + \".pth\"\n",
    "        self.create_network(env)\n",
    "        \n",
    "        models = torch.load(filename, map_location=self.device)\n",
    "        \n",
    "        self.q_net.load_state_dict(models['state_dict'])\n",
    "        self.optimizer.load_state_dict(models['optimizer'])\n",
    "    \n",
    "    def save_params(self, params):\n",
    "        folder_name = \"hyperparameters\"\n",
    "        self.create_folder(folder_name)\n",
    "        \n",
    "        file_name = f'./{folder_name}/{self.policy}_DQN_{self.time}'\n",
    "        with open(file_name + '.txt', 'w') as file:\n",
    "            file.write(json.dumps(str(params)))\n",
    "\n",
    "    def create_folder(self, directory_name):\n",
    "        try:\n",
    "            os.mkdir(directory_name)\n",
    "            print(f\"Directory '{directory_name}' created successfully.\")\n",
    "        except FileExistsError:\n",
    "            return\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "policy = \"CnnPolicy\"\n",
    "# policy = \"MlpPolicy\"\n",
    "\n",
    "if policy == \"CnnPolicy\":\n",
    "    config={\n",
    "        \"lanes_count\" : 3,\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 64),\n",
    "            \"stack_size\": 4,\n",
    "            \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion keep this conversion this is in the highway env page\n",
    "            \"scaling\": 1.75,\n",
    "        },\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"lanes_count\" : 3,\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"vehicles_count\": 5,\n",
    "            \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "            \"features_range\": {\n",
    "                \"x\": [-100, 100],\n",
    "                \"y\": [-100, 100],\n",
    "                \"vx\": [-20, 20],\n",
    "                \"vy\": [-20, 20]\n",
    "            },\n",
    "            \"absolute\": False,\n",
    "            \"order\": \"sorted\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefilling Memory : 100%|██████████| 16/16 [00:03<00:00,  4.19it/s]\n",
      "Training Model:  17%|█▋        | 170/1000 [04:16<20:50,  1.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [146]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m dqn_agent \u001b[38;5;241m=\u001b[39m DQNAgent(params)\n\u001b[1;32m     16\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighway-fast-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighway-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     20\u001b[0m dqn_agent\u001b[38;5;241m.\u001b[39mevaluate(env, \u001b[38;5;241m10\u001b[39m)\n",
      "Input \u001b[0;32mIn [143]\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    109\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstore(state, action, next_state, reward, done)\n\u001b[0;32m--> 112\u001b[0m episode_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience_replay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    114\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    116\u001b[0m episode_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Input \u001b[0;32mIn [143]\u001b[0m, in \u001b[0;36mDQNAgent.experience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'policy' : policy,\n",
    "    'episode_num' : 1000,\n",
    "    'discount' : 0.8,\n",
    "    'batch_size' : 16,\n",
    "    'n_steps': 4,\n",
    "    'double': True,\n",
    "    'device' : torch.device(\"mps\"),\n",
    "    'memory_capacity' : 10000,\n",
    "    'timeout_minute': 1,\n",
    "    'use_metrics' : False,\n",
    "    'save_model': False,\n",
    "}\n",
    "\n",
    "dqn_agent = DQNAgent(params)\n",
    "env = gym.make('highway-fast-v0', render_mode='rgb_array', config=config)\n",
    "dqn_agent.learn(env)\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "dqn_agent.evaluate(env, 10)\n",
    "\n",
    "# if you wanna save a model again\n",
    "# dqn_agent.save_model(\"highway_dqn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/m_r158w50cqfn3ydymrpm7rc0000gn/T/ipykernel_45635/1614493651.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  models = torch.load(filename, map_location=self.device)\n",
      "Evaluating Model:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000, -8770.1182,  2399.5125,     0.0000, -2908.1982]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8549.9873, -8838.2910,     0.0000,  6042.5273,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8784.0957,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.2500, 0.0000, 0.2500, 0.2500, 0.2500]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:   5%|▌         | 1/20 [00:00<00:18,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8558.8809, -8827.7100,  2423.6118,  6067.7080,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8539.6514, -8807.8467,  2445.2800,  6045.2661,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[  -0.0000,   -0.0000, 2403.3269, 6054.5024,   -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  10%|█         | 2/20 [00:01<00:18,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8511.7383,    -0.0000,  2490.5554,  6003.3286, -2905.4438]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8775.1357,  2460.4775,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  15%|█▌        | 3/20 [00:02<00:14,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000,    -0.0000,     0.0000,  6019.4766, -2846.6851]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8558.3945, -8830.0674,     0.0000,     0.0000, -2870.7412]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8569.0361,    -0.0000,  2368.9377,  6060.7788, -2860.5146]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8540.6738,    -0.0000,  2416.6252,  6046.9609, -2870.5032]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  20%|██        | 4/20 [00:04<00:16,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000,    -0.0000,     0.0000,     0.0000, -2873.8994]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8814.1631,  2400.1755,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[  -0.0000,   -0.0000, 2403.5991, 5990.6343,   -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8529.0967, -8809.9434,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.3333, 0.3333, 0.3333]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  25%|██▌       | 5/20 [00:05<00:17,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000, -8811.5947,  2485.3889,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8496.9766, -8776.6846,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.3333, 0.3333, 0.3333]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000,    -0.0000,  2476.5017,     0.0000, -2910.2288]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8518.9639, -8803.5947,     0.0000,  6007.2695, -2926.7568]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  30%|███       | 6/20 [00:06<00:17,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000, -8778.9209,  2371.9949,     0.0000, -2840.9897]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8505.6318, -8775.7158,  2419.5999,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8775.2764,  2353.8770,  6026.2705, -2843.2256]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8780.2676,  2392.9507,     0.0000, -2904.1072]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  35%|███▌      | 7/20 [00:08<00:16,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  -0.0000,   -0.0000, 2492.3000,    0.0000,   -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8450.4160,    -0.0000,  2436.1694,  5978.3745,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000,    -0.0000,  2510.4150,     0.0000, -2916.6406]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  40%|████      | 8/20 [00:09<00:14,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000,    -0.0000,     0.0000,  6010.4424, -2675.3989]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8474.2412,    -0.0000,  2270.3926,     0.0000, -2704.6902]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8473.0010, -8782.4189,  2294.4070,  5994.4912, -2695.7891]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8818.8232,     0.0000,     0.0000, -2718.8093]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.3333, 0.0000, 0.3333, 0.3333, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[  -0.0000,   -0.0000, 2325.7273, 6036.5835,   -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8774.1660,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.2500, 0.0000, 0.2500, 0.2500, 0.2500]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  45%|████▌     | 9/20 [00:11<00:15,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000,    -0.0000,  2387.0266,  6034.4795, -2877.1606]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8491.6895, -8759.8135,     0.0000,  6000.8525, -2878.9768]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8514.6562, -8784.7881,     0.0000,  6026.4263,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000,    -0.0000,  2410.4460,     0.0000, -2886.4802]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8567.8467,    -0.0000,  2402.7493,  6065.1484, -2887.6973]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8564.1367, -8828.9893,     0.0000,  6055.5605, -2887.4973]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8820.6211,     0.0000,  6071.2798, -2883.3557]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8515.9258,    -0.0000,  2426.7839,     0.0000, -2876.6274]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  50%|█████     | 10/20 [00:13<00:18,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000, -8779.3145,  2310.2092,  5965.7939, -2686.5967]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8495.6133,    -0.0000,     0.0000,     0.0000, -2688.3474]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.3333, 0.3333, 0.3333, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8789.0635,     0.0000,  6007.0234,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8541.6416,    -0.0000,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.2500, 0.2500, 0.2500, 0.2500]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  55%|█████▌    | 11/20 [00:15<00:15,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000, -8798.2207,     0.0000,     0.0000, -2914.0476]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.3333, 0.0000, 0.3333, 0.3333, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8488.6592, -8780.3584,     0.0000,     0.0000, -2896.3677]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8475.3623,    -0.0000,  2441.4973,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  60%|██████    | 12/20 [00:16<00:11,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8516.2451, -8803.7314,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.3333, 0.3333, 0.3333]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8503.3359,    -0.0000,     0.0000,  5988.9668, -2663.3184]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8478.2559, -8750.9346,  2279.4270,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8479.4346, -8770.0029,     0.0000,     0.0000, -2689.3374]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8560.3076, -8838.0322,     0.0000,     0.0000, -2708.6907]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8847.2158,  2307.7300,  6035.3584, -2651.7207]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  65%|██████▌   | 13/20 [00:18<00:11,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000, -8812.1826,     0.0000,     0.0000, -2918.2358]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.3333, 0.0000, 0.3333, 0.3333, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8521.2686, -8789.2041,  2485.9221,  6015.7314,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8526.3008, -8811.1191,  2458.2271,     0.0000, -2912.3242]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8527.2744, -8795.9287,  2487.1677,  6011.9150,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  70%|███████   | 14/20 [00:19<00:09,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000, -8801.4375,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.2500, 0.0000, 0.2500, 0.2500, 0.2500]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8519.5303,    -0.0000,  2317.5598,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8471.7197, -8758.6143,  2305.1560,  6003.0669,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8537.7930,    -0.0000,     0.0000,     0.0000, -2693.9395]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.3333, 0.3333, 0.3333, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8576.4170, -8865.0352,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.3333, 0.3333, 0.3333]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8776.1904,  2262.5625,  5992.0400, -2680.5952]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8800.1865,  2295.0591,     0.0000, -2684.5112]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  75%|███████▌  | 15/20 [00:21<00:08,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000,    -0.0000,  2519.2739,  5961.4316, -2896.3013]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8459.0225, -8745.1729,     0.0000,  5950.1387, -2888.1309]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8724.2295,     0.0000,  5949.1621,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8518.5264, -8805.0127,  2471.4836,     0.0000, -2909.2949]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  80%|████████  | 16/20 [00:23<00:06,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   -0.0000, -8884.3721,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.2500, 0.0000, 0.2500, 0.2500, 0.2500]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8507.1475, -8794.0400,     0.0000,  6004.5166, -2867.3611]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000,    -0.0000,  2455.0603,     0.0000, -2935.5593]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8524.8057, -8819.5381,     0.0000,  6000.7158, -2906.3994]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  85%|████████▌ | 17/20 [00:24<00:04,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8535.6611,    -0.0000,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.2500, 0.2500, 0.2500, 0.2500]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[  -0.0000,   -0.0000, 2433.1858, 6051.3398,   -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000,    -0.0000,     0.0000,     0.0000, -2881.3979]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8826.0654,     0.0000,     0.0000, -2902.2871]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.3333, 0.0000, 0.3333, 0.3333, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  90%|█████████ | 18/20 [00:25<00:02,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8510.5029, -8784.0303,  2341.1194,     0.0000, -2721.8931]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8498.0225, -8778.8027,     0.0000,     0.0000, -2662.5183]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8480.6729, -8783.1045,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.3333, 0.3333, 0.3333]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8570.4639,    -0.0000,  2324.5725,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8521.5811, -8809.7334,  2292.7952,     0.0000, -2709.0364]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8493.2607,    -0.0000,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.2500, 0.2500, 0.2500, 0.2500]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000,    -0.0000,     0.0000,     0.0000, -2690.4319]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 0.0000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-0., -0., 0., 0., -0.]], device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8561.3770, -8848.9268,     0.0000,  6030.3506, -2679.0981]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8504.9717,    -0.0000,  2288.8845,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000,    -0.0000,  2295.7410,     0.0000, -2690.5459]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8495.4941,    -0.0000,  2252.3677,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8730.0625,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.2500, 0.0000, 0.2500, 0.2500, 0.2500]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  95%|█████████▌| 19/20 [00:30<00:02,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8476.7178, -8762.5361,     0.0000,  5957.5884, -2876.7578]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000,    -0.0000,  2497.9519,  6035.9854, -2942.3179]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-8478.6875, -8741.1748,     0.0000,     0.0000,    -0.0000]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.3333, 0.3333, 0.3333]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[   -0.0000, -8814.0127,  2506.6042,     0.0000, -2932.4558]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 1., 0., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:  95%|█████████▌| 19/20 [00:31<00:01,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8501.7441, -8783.5869,  2472.9182,  5987.3418, -2908.2495]],\n",
      "       device='mps:0', grad_fn=<MulBackward0>)\n",
      "tensor([[0., 0., 0., 1., 0.]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m dqn_agent_test \u001b[38;5;241m=\u001b[39m DQNAgent(params)\n\u001b[1;32m      4\u001b[0m dqn_agent_test\u001b[38;5;241m.\u001b[39mload_model(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDQN_3_2024-12-24|10:46:24.193447\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdqn_agent_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36mDQNAgent.evaluate\u001b[0;34m(self, env, episode_num)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated):  \n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# Select best action   \u001b[39;00m\n\u001b[1;32m    211\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action(state, eval_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 212\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:271\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    260\u001b[0m     action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    268\u001b[0m ):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/road.py:464\u001b[0m, in \u001b[0;36mRoad.act\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[0;32m--> 464\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/vehicle/behavior.py:115\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    110\u001b[0m action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\n\u001b[1;32m    111\u001b[0m     action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_STEERING_ANGLE\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m front_vehicle, rear_vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane_index\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macceleration(\n\u001b[1;32m    119\u001b[0m     ego_vehicle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, front_vehicle\u001b[38;5;241m=\u001b[39mfront_vehicle, rear_vehicle\u001b[38;5;241m=\u001b[39mrear_vehicle\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/road.py:504\u001b[0m, in \u001b[0;36mRoad.neighbour_vehicles\u001b[0;34m(self, vehicle, lane_index)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjects:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    501\u001b[0m         v, Landmark\n\u001b[1;32m    502\u001b[0m     ):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m         s_v, lat_v \u001b[38;5;241m=\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mon_lane(v\u001b[38;5;241m.\u001b[39mposition, s_v, lat_v, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    506\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/lane.py:213\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[0;34m(self, position)\u001b[0m\n\u001b[1;32m    211\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m    212\u001b[0m lateral \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection_lateral)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlongitudinal\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "\n",
    "dqn_agent_test = DQNAgent(params)\n",
    "dqn_agent_test.load_model(env, \"DQN_3_2024-12-24|10:46:24.193447\")\n",
    "\n",
    "dqn_agent_test.evaluate(env, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b95d3aa0fdeff6c3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b95d3aa0fdeff6c3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6010;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir training_results --host localhost --port 6010"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
