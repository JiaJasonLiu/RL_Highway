{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Install environment and agent\n",
    "# !pip install highway-env\n",
    "# !pip install --upgrade sympy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning using existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the pesudocode that will be followed when creating the DQN\n",
    "\n",
    "Useful: https://www.youtube.com/watch?v=RVMpm86equc&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=2\n",
    "\n",
    "https://github.com/saashanair/rl-series/tree/master/dqn\n",
    "\n",
    "https://github.com/johnnycode8/gym_solutions/blob/main/frozen_lake_dql.py\n",
    "\n",
    "<img src=\"DQN.png\" style=\"width: 900px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "Potential Problems: https://www.reddit.com/r/reinforcementlearning/comments/1555wgi/dqn_loss_increasing_and_rewards_decreasing/\n",
    "\n",
    "\n",
    "For CNN:\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/3l5qu7/rules_of_thumb_for_cnn_architectures/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros( 2 * capacity - 1 )\n",
    "        self.data = np.zeros( capacity, dtype=object )\n",
    "        self.writer = 0\n",
    "\n",
    "    # propagate upwards to update the sum values\n",
    "    def _propagate(self, index, change):\n",
    "        parent = (index - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # get the leaf nodes (Transaction)\n",
    "    def _retrieve(self, index, s):\n",
    "        left = 2 * index + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return index\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        index = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(index, p)\n",
    "\n",
    "        self.write += 1\n",
    "        # circular\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, index, p):\n",
    "        change = p - self.tree[index]\n",
    "\n",
    "        self.tree[index] = p\n",
    "        self._propagate(index, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        index = self._retrieve(0, s)\n",
    "        data_index = index - self.capacity + 1\n",
    "\n",
    "        return (index, self.tree[index], self.data[data_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import torch.nn.init as init\n",
    "\n",
    "# the paper: https://arxiv.org/pdf/1706.10295\n",
    "class NoisyLayer(nn.Module):\n",
    "    #sigma is Ïƒi,j for all param where 3.2 INITIALISATION OF NOISY NETWORKS in the paper 0.017(for indimendent gaussain distri)\n",
    "    def __init__(self, in_features, out_features, sigmaparam=0.4):\n",
    "        super(NoisyLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigmaparam = sigmaparam\n",
    "\n",
    "        # learnable parameters (sigma and mu)\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.sigma_weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.sigma_bias = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        # noise param using a distribution\n",
    "        # register buffer so it persists through gradient descent updates, as epsilon doesn't change \n",
    "        self.register_buffer(\"epsilon_weight_middle\", torch.empty(out_features, in_features))\n",
    "        self.register_buffer(\"epsilon_bias\", torch.empty(out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.factorized_noise()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      # xavier uniform due to DQN sigma activation function for actions\n",
    "        mu_range = 1 / (self.in_features ** 0.5)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "\n",
    "        self.sigma_weight.data.fill_(self.sigmaparam)\n",
    "        self.sigma_bias.data.fill_(self.sigmaparam)\n",
    "      \n",
    "    def noise(self,size):\n",
    "        \"\"\"generate noise (factorized gaussian noise): f(x) = sign(x) * sqrt(abs(x))\"\"\"\n",
    "        factor_noise = torch.randn(size, device=self.weight_mu.device)\n",
    "        return factor_noise.sign().mul_(factor_noise.abs().sqrt_()) \n",
    "\n",
    "    def factorized_noise(self):\n",
    "        \"\"\"Create a new noise\"\"\"\n",
    "        epsilon_in = self.noise(self.in_features)\n",
    "        epsilon_out = self.noise(self.out_features)\n",
    "        self.epsilon_weight = self.epsilon_weight_middle.copy_(epsilon_out.outer(epsilon_in))\n",
    "        self.epsilon_bias.copy_(epsilon_out)\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Jason change this please, I am not sure how you defined trianing = true or false\n",
    "        this return currently present is for training = true where there are presence of noise ie. sigma*epsilon\n",
    "\n",
    "        For evaluation you will need to only return F.linear(input, self.weight_mu, self.bias_mu)\n",
    "        see below\n",
    "        '''\n",
    "        noisy_weight = self.weight_mu + (self.sigma_weight * self.epsilon_weight)\n",
    "        noisy_bias = self.bias_mu + (self.sigma_bias * self.epsilon_bias)\n",
    "        return F.linear(input, noisy_weight, noisy_bias)\n",
    "\n",
    "# Define model\n",
    "class MLPNetwork(nn.Module):\n",
    "    def __init__(self, in_states, out_actions):\n",
    "        super(MLPNetwork, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_states, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, out_actions),\n",
    "            nn.ReLU(), \n",
    "        )\n",
    "        self.out = nn.Linear(out_actions ** 2, out_actions)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, noisy=False):\n",
    "        super(CNN, self).__init__()\n",
    "        # greyscale Image is(stack,height,width)\n",
    "        stack, height, width = input_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(stack,16,kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16,32,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32,64, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # This is for finding the size to dense more robust compared to decision manually\n",
    "        with torch.no_grad():\n",
    "                # Torch uses(1,channels,height,width)\n",
    "                test = torch.zeros(1, stack, height, width)\n",
    "                find_conv_size = self.conv(test)\n",
    "                conv_size = find_conv_size.numel()\n",
    "        self.out1 = nn.Linear(conv_size,num_actions)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.out1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define memory for Experience Replay\n",
    "# TODO: Prioritize, n-steps\n",
    "\n",
    "from collections.__init__ import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, config, transition_type=Transition):\n",
    "        self.capacity = config.get(\"capacity\", 1000)\n",
    "        self.index = 0\n",
    "        self.transition_type = transition_type\n",
    "        self.discount = config.get(\"discount\", 0.99)\n",
    "\n",
    "        self.memory = []\n",
    "        self.device = config.get(\"device\", torch.device(\"cpu\"))\n",
    "        self.n_steps = config.get(\"n_steps\", 2)\n",
    "    \n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.index = len(self.memory) - 1\n",
    "        elif len(self.memory) > self.capacity:\n",
    "            self.memory = self.memory[:self.capacity]\n",
    "        # Faster than append and pop\n",
    "        self.memory[self.index] = self.transition_type(*args)\n",
    "        \n",
    "        self.index = (self.index+1)%self.capacity # for circular memory\n",
    "\n",
    "    def sample(self, batch_size, collapsed=True):\n",
    "        if batch_size >  len(self.memory):\n",
    "            batch_size = len(self.memory)\n",
    "            \n",
    "        if self.n_steps <= 1:\n",
    "            # Directly sample transitions\n",
    "            memories = random.sample(self.memory, batch_size)\n",
    "            return self.unwrap_transition(*memories)\n",
    "        else:\n",
    "            # Sample initial transition indexes\n",
    "            indexes = random.sample(range(len(self.memory)), batch_size)\n",
    "            # Get the batch of n-consecutive-transitions starting from sampled indexes\n",
    "            all_transitions = [self.memory[i:i+self.n_steps] for i in indexes]\n",
    "            \n",
    "            memories = map(self.collapse_n_steps, all_transitions) if collapsed else all_transitions\n",
    "\n",
    "            return self.unwrap_transition(*memories)\n",
    "        \n",
    "    def collapse_n_steps(self, transitions):\n",
    "        state, action, next_state, reward, done = transitions[0]\n",
    "        discount = self.discount\n",
    "        for transition in transitions[1:]:\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                _, _, next_state, reward, done = transition\n",
    "                discount *= self.discount\n",
    "                reward += discount * reward\n",
    "        return state, action, next_state, reward, done\n",
    "    \n",
    "    def unwrap_transition(self, *transition):\n",
    "        state, action, next_state, reward, done = zip(*transition)\n",
    "        \n",
    "        states = torch.from_numpy(np.array(state)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(action)).to(self.device)\n",
    "        next_states = torch.from_numpy(np.array(next_state)).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(reward)).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(done)).to(self.device)\n",
    "\n",
    "        return states, actions, next_states, rewards, dones \n",
    "\n",
    "class PrioritizedReplayMemory(ReplayMemory):\n",
    "    def __init__(self, config, transition_type=Transition):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.capacity = config.get(\"capacity\", 1000)\n",
    "        self.tree = SumTree(self.capacity)\n",
    "        self.index = 0\n",
    "        self.transition_type = transition_type\n",
    "\n",
    "        self.memory = []\n",
    "        self.device = config.get(\"device\", torch.device(\"cpu\"))\n",
    "        \n",
    "        self.alpha = config.get(\"alpha\", 0.6)\n",
    "        self.beta = config.get(\"beta\", 0.2) #  will go to 1\n",
    "        self.max_priority = 1  # priority for new samples, init as eps\n",
    "        \n",
    "    def store(self, *args):\n",
    "        super().store(*args)\n",
    "        self.tree.add(self.max_priority ** self.alpha, )\n",
    "    \n",
    "    def sample(self, batch_size, collapsed=True):\n",
    "        priorities = torch.empty(batch_size, 1, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from metrics import Metrics\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, params):\n",
    "        self.q_net = {}\n",
    "        self.q_target_net = {} \n",
    "        self.optimizer= {}        \n",
    "        \n",
    "        self.policy = params.get(\"policy\", \"CnnPolicy\")        \n",
    "        self.episode_num = params.get(\"episode_num\", 10)\n",
    "\n",
    "        self.epsilon = params.get(\"epsilon_max \", 1)\n",
    "        self.epsilon_min = params.get(\"epsilon_min \", 0.1)\n",
    "        self.epsilon_decay = params.get(\"epsilon_decay\", 0.995)\n",
    "        \n",
    "        self.learning_rate = params.get(\"learning_rate\", 5e-4)\n",
    "        self.discount = params.get(\"discount\", 0.2)\n",
    "        self.batch_size = params.get(\"batch_size\", 32)\n",
    "        self.device = params.get(\"device\", torch.device(\"cpu\"))\n",
    "        \n",
    "        self.memory_capacity = params.get(\"memory_capacity\", 1000)\n",
    "        self.memory = {} # this is the memory buffer -> setting a limit\n",
    "        self.n_steps = params.get(\"n_steps\", 2)\n",
    "        self.prioritize_memory = params.get(\"prioritize_memory\", False)\n",
    "        \n",
    "        self.double = params.get(\"double\", False)\n",
    "        \n",
    "        self.timeout = params.get(\"timeout_minute\", 0) * 60 # in minutes\n",
    "        self.time = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.to_save_model = params.get(\"save_model\", False)\n",
    "        \n",
    "        use_metrics = params.get(\"use_metrics\", False)\n",
    "        \n",
    "        # if use_metrics:\n",
    "            # self.save_params(params)\n",
    "            \n",
    "        self.metrics = Metrics(self.policy, \"training_results\", use_metrics)\n",
    "        \n",
    "    def initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # Xavier initialization for Conv2d weights\n",
    "            init.xavier_uniform_(m.weight)\n",
    "            init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            # Xavier initialization for Linear weights\n",
    "            init.xavier_uniform_(m.weight)\n",
    "            init.zeros_(m.bias)\n",
    "\n",
    "    def create_network(self, env):\n",
    "        if self.policy == \"CnnPolicy\":\n",
    "            self.create_CNN(env)\n",
    "        \n",
    "        if self.policy == \"MlpPolicy\":\n",
    "            self.create_MLP_Network(env)\n",
    "        \n",
    "        self.q_net.apply(self.initialize_weights)    \n",
    "        self.update_target_network()\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "    \n",
    "    def create_CNN(self, env):\n",
    "        self.num_states = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = CNN(self.num_states, self.num_actions).to(self.device)\n",
    "        self.q_target_net = CNN(self.num_states, self.num_actions).to(self.device)\n",
    "        \n",
    "    \n",
    "    def create_MLP_Network(self, env):\n",
    "        # the lanes\n",
    "        self.num_states = env.observation_space.shape[1]\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = MLPNetwork(self.num_states, self.num_actions).to(self.device)\n",
    "        self.q_target_net = MLPNetwork(self.num_states, self.num_actions).to(self.device)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.q_target_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n",
    "    def learn(self, env):\n",
    "        self.create_network(env)\n",
    "        \n",
    "        # if (self.prioritize_memory):\n",
    "        self.memory = ReplayMemory({\n",
    "            \"capacity\": self.memory_capacity,\n",
    "            \"device\": self.device,\n",
    "            \"n_steps\": self.n_steps,\n",
    "        })\n",
    "\n",
    "        self.prefill_memory(env, self.batch_size)\n",
    "\n",
    "        start_time = time.time()        \n",
    "        \n",
    "        for epoch in tqdm(range(self.episode_num), desc=\"Training Model\"):\n",
    "            state = env.reset()[0]\n",
    "            \n",
    "            # True when agent reaches the end states (colliding or passing the time)\n",
    "            done = False \n",
    "            \n",
    "            # TODO: see how many actions until truncate\n",
    "            # True when agent takes more than some actions \n",
    "            truncated = False\n",
    "            episode_rewards = []\n",
    "            episode_loss = []\n",
    "            episode_len = 0\n",
    "            while(not done and not truncated):\n",
    "                # choose best action\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)\n",
    "                \n",
    "                episode_loss.append(self.experience_replay())\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                episode_len += 1\n",
    "                \n",
    "            self.metrics.add(\"rollout/rewards\", sum(episode_rewards) / len(episode_rewards), epoch)\n",
    "            self.metrics.add(\"rollout/exploration-rate\", self.epsilon, epoch)\n",
    "            self.metrics.add(\"rollout/episode-length\", episode_len, epoch)\n",
    "            self.metrics.add(\"train/loss\", sum(episode_loss) / len(episode_loss), epoch)\n",
    "            \n",
    "            if self.timeout:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                if elapsed_time > self.timeout:\n",
    "                    print(\"Timeout reached. Stopping training.\\n\")\n",
    "                    break\n",
    "            \n",
    "            # if epoch % 10 == 0:\n",
    "            self.decay_epsilon()\n",
    "            self.update_target_network()\n",
    "            \n",
    "        self.metrics.close()\n",
    "        \n",
    "        if self.to_save_model:\n",
    "            self.save_model()\n",
    "        \n",
    "    # either the policies are able to get miltuple actions and into the NN or the input of NN should be able to handle all of these\n",
    "    # output (one of): {0: 'LANE_LEFT', 1: 'IDLE', 2: 'LANE_RIGHT', 3: 'FASTER', 4: 'SLOWER'}\n",
    "    def get_action(self, state, eval_mode=False):\n",
    "        if random.random() <= self.epsilon and not eval_mode: # amount of exploration reduces with the epsilon value\n",
    "            return random.randrange(self.num_actions)\n",
    "        \n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float32).to(self.device)\n",
    "        actions = self.q_net(state)\n",
    "        return torch.argmax(actions).item()             \n",
    "\n",
    "    def experience_replay(self):\n",
    "        states, actions, next_states, rewards, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        q_pred = self.q_net(states)\n",
    "        # q value of the action taken\n",
    "        q_pred = q_pred.gather(1, actions.view(-1, 1)) \n",
    "        q_pred = q_pred.squeeze(1)\n",
    "\n",
    "        # Double DQN\n",
    "        if self.double:\n",
    "             # pick best actions from policy network\n",
    "            q_best_action = self.q_net(next_states)\n",
    "            _, q_best_action = q_best_action.max(dim=1)\n",
    "            q_best_action = q_best_action.unsqueeze(1)\n",
    "            \n",
    "            # use those actions for the target policy\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.gather(1, q_best_action)\n",
    "            q_target = q_target.squeeze(1)\n",
    "        else:\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.max(dim=1).values\n",
    "        \n",
    "        # setting Q(s',a') to 0 when the current state is a terminal state\n",
    "        q_target[dones] = 0.0\n",
    "        \n",
    "        y_j = rewards + (self.discount * q_target)\n",
    "        \n",
    "        # calculate the loss as the mean-squared error of yj and qpred\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(y_j, q_pred).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "    def prefill_memory(self, env, prefill_num):\n",
    "        for _ in tqdm(range(prefill_num), desc=\"Prefilling Memory \"):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            while not done and not truncated:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)    \n",
    "                \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "    \n",
    "    def evaluate(self, env, episode_num):\n",
    "        # add camera here\n",
    "        for _ in tqdm(range(episode_num), desc=\"Evaluating Model\"):\n",
    "            state = env.reset()[0]  \n",
    "            done = False      \n",
    "            truncated = False \n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not done and not truncated):  \n",
    "                # Select best action   \n",
    "                action = self.get_action(state, eval_mode=True)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                state = next_state\n",
    "                env.render()\n",
    "        \n",
    "    def save_model(self):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "        self.metrics.create_folder(folder_name)\n",
    "        new_model_num = str(len(os.listdir(\"./\" +folder_name)) + 1)\n",
    "        file_name = f'{folder_name}/DQN_{new_model_num}_{self.time}.pth'\n",
    "        state = {'state_dict': self.q_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()}\n",
    "        torch.save(state, file_name)\n",
    "        \n",
    "    def load_model(self, env, file_name):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "                \n",
    "        filename = folder_name + \"/\" + file_name + \".pth\"\n",
    "        self.create_network(env)\n",
    "        \n",
    "        models = torch.load(filename, map_location=self.device)\n",
    "        \n",
    "        self.q_net.load_state_dict(models['state_dict'])\n",
    "        self.optimizer.load_state_dict(models['optimizer'])\n",
    "    \n",
    "    def save_params(self, params):\n",
    "        folder_name = \"hyperparameters\"\n",
    "        self.metrics.create_folder(folder_name)\n",
    "        \n",
    "        file_name = f'./{folder_name}/{self.policy}_DQN_{self.time}'\n",
    "        with open(file_name + '.txt', 'w') as file:\n",
    "            file.write(json.dumps(str(params)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "# policy = \"CnnPolicy\"\n",
    "policy = \"MlpPolicy\"\n",
    "\n",
    "if policy == \"CnnPolicy\":\n",
    "    config={\n",
    "        \"lanes_count\" : 3,\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 64),\n",
    "            \"stack_size\": 4,\n",
    "            \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion keep this conversion this is in the highway env page\n",
    "            \"scaling\": 1.75,\n",
    "        },\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"lanes_count\" : 3,\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"vehicles_count\": 10,\n",
    "            \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "            \"features_range\": {\n",
    "                \"x\": [-100, 100],\n",
    "                \"y\": [-100, 100],\n",
    "                \"vx\": [-20, 20],\n",
    "                \"vy\": [-20, 20]\n",
    "            },\n",
    "            \"absolute\": False,\n",
    "            \"order\": \"sorted\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefilling Memory : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.61it/s]\n",
      "Training Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:38<00:00,  2.61it/s]\n",
      "Evaluating Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:12<00:00,  2.48s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "params = {\n",
    "    'policy' : policy,\n",
    "    'episode_num' : 100,\n",
    "    'discount' : 0.7,\n",
    "    'batch_size' : 3,\n",
    "    'learning_rate': 4e-5,\n",
    "    'n_steps': 4,\n",
    "    'double': True,\n",
    "    'device' : torch.device(\"mps\"),\n",
    "    'memory_capacity' : 10000,\n",
    "    'timeout_minute': 15,\n",
    "    'use_metrics' : False,\n",
    "    'save_model': False,\n",
    "}\n",
    "\n",
    "seed = 72 # Our group number\n",
    "for i in range(1):\n",
    "    seed += i\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # so that the GPU seed is also random\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = False # not optimize for when the network doesn't get the same input size (because of batch sizes and get best action)\n",
    "        torch.backends.cudnn.deterministic = True # slows down training\n",
    "    \n",
    "    dqn_agent = DQNAgent(params)\n",
    "    env = gym.make('highway-fast-v0', render_mode='rgb_array', config=config)\n",
    "    dqn_agent.learn(env)\n",
    "    \n",
    "    # env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "    # dqn_agent.evaluate(env, 5)\n",
    "\n",
    "# # if you wanna save a model again\n",
    "# # dqn_agent.save_model(\"highway_dqn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:08<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "\n",
    "dqn_agent_test = DQNAgent(params)\n",
    "dqn_agent_test.load_model(env, \"DQN_1_20241230164948\")\n",
    "\n",
    "dqn_agent_test.evaluate(env, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-773cba20f143bb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-773cba20f143bb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6010;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir training_results --host localhost --port 6010"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
