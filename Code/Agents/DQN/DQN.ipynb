{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Install environment and agent\n",
    "# !pip install highway-env\n",
    "# !pip install --upgrade sympy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning using existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the pesudocode that will be followed when creating the DQN\n",
    "\n",
    "Useful: https://www.youtube.com/watch?v=RVMpm86equc&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=2\n",
    "\n",
    "https://github.com/saashanair/rl-series/tree/master/dqn\n",
    "\n",
    "https://github.com/johnnycode8/gym_solutions/blob/main/frozen_lake_dql.py\n",
    "\n",
    "<img src=\"DQN.png\" style=\"width: 900px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "Potential Problems: https://www.reddit.com/r/reinforcementlearning/comments/1555wgi/dqn_loss_increasing_and_rewards_decreasing/\n",
    "\n",
    "\n",
    "For CNN:\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/3l5qu7/rules_of_thumb_for_cnn_architectures/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import torch.nn.init as init\n",
    "\n",
    "# the paper: https://arxiv.org/pdf/1706.10295\n",
    "class NoisyLayer(nn.Module):\n",
    "    #sigma is σi,j for all param where 3.2 INITIALISATION OF NOISY NETWORKS in the paper 0.017(for indimendent gaussain distri)\n",
    "    def __init__(self, in_features, out_features, sigmaparam=0.4):\n",
    "        super(NoisyLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.sigmaparam = sigmaparam\n",
    "\n",
    "        # learnable parameters (sigma and mu)\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.sigma_weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.sigma_bias = nn.Parameter(torch.empty(out_features))\n",
    "\n",
    "        # noise param using a distribution\n",
    "        # register buffer so it persists through gradient descent updates, as epsilon doesn't change \n",
    "        self.register_buffer(\"epsilon_weight\", torch.empty(out_features, in_features))\n",
    "        self.register_buffer(\"epsilon_bias\", torch.empty(out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.factorized_noise()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      # xavier uniform due to DQN sigma activation function for actions\n",
    "        mu_range = 1 / (self.in_features ** 0.5)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "\n",
    "        self.sigma_weight.data.fill_(self.sigmaparam)\n",
    "        self.sigma_bias.data.fill_(self.sigmaparam)\n",
    "      \n",
    "    def noise(self,size):\n",
    "        \"\"\"generate noise (factorized gaussian noise): f(x) = sign(x) * sqrt(abs(x))\"\"\"\n",
    "        factor_noise = torch.randn(size)\n",
    "        return factor_noise.sign().mul_(factor_noise.abs().sqrt_()) \n",
    "\n",
    "    def factorized_noise(self):\n",
    "        \"\"\"Create a new noise\"\"\"\n",
    "        epsilon_in = self.noise(self.in_features)\n",
    "        epsilon_out = self.noise(self.out_features)\n",
    "        self.epsilon_weight.copy_(epsilon_out.outer(epsilon_in))\n",
    "        self.epsilon_bias.copy_(epsilon_out)\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Comparison between training and evaluation different differ, so it won't be handled differently here\n",
    "        '''\n",
    "        noisy_weight = self.weight_mu + (self.sigma_weight * self.epsilon_weight)\n",
    "        noisy_bias = self.bias_mu + (self.sigma_bias * self.epsilon_bias)\n",
    "        return F.linear(x, noisy_weight, noisy_bias)\n",
    "\n",
    "# Define model\n",
    "class MLPNetwork(nn.Module):\n",
    "    def __init__(self, in_states, out_actions, vehicle_count, noisy_net=False):\n",
    "        super(MLPNetwork, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_states, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, out_actions),\n",
    "            nn.ReLU(), \n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(out_actions * vehicle_count, out_actions)\n",
    "        \n",
    "        self.noisy_net = noisy_net\n",
    "        if self.noisy_net:\n",
    "            self.noisy_layer = NoisyLayer(out_actions * vehicle_count, out_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        if self.noisy_net:\n",
    "            x = self.noisy_layer(x)\n",
    "        else:\n",
    "            x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        self.noisy_layer.factorized_noise()\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, noisy_net=False):\n",
    "        super(CNN, self).__init__()\n",
    "        # greyscale Image is(stack,height,width)\n",
    "        stack, height, width = input_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(stack,16,kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16,32,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32,64, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # This is for finding the size to dense more robust compared to decision manually\n",
    "        with torch.no_grad():\n",
    "                # Torch uses(1,channels,height,width)\n",
    "                test = torch.zeros(1, stack, height, width)\n",
    "                find_conv_size = self.conv(test)\n",
    "                conv_size = find_conv_size.numel()\n",
    "        self.out1 = nn.Linear(conv_size,num_actions)\n",
    "        \n",
    "        self.noisy_net = noisy_net\n",
    "        if self.noisy_net:\n",
    "            self.noisy_layer1 = NoisyLayer(conv_size, 64)\n",
    "            self.noisy_layer2 = NoisyLayer(64, num_actions)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        if self.noisy_net:\n",
    "            x = F.relu(self.noisy_layer1(x))\n",
    "            x = self.noisy_layer2(x)\n",
    "        else:\n",
    "            x = self.out1(x)\n",
    "        return x\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset noisy layers.\"\"\"\n",
    "        self.noisy_layer1.factorized_noise()\n",
    "        self.noisy_layer2.factorized_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.__init__ import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, config, transition_type=Transition):\n",
    "        self.capacity = config.get(\"capacity\", 1000)\n",
    "        self.index = 0\n",
    "        self.transition_type = transition_type\n",
    "        self.discount = config.get(\"discount\", 0.99)\n",
    "\n",
    "        self.memory = []\n",
    "        self.device = config.get(\"device\", torch.device(\"cpu\"))\n",
    "        self.n_steps = config.get(\"n_steps\", 2)\n",
    "    \n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.index = len(self.memory) - 1\n",
    "        elif len(self.memory) > self.capacity:\n",
    "            self.memory = self.memory[:self.capacity]\n",
    "        # Faster than append and pop\n",
    "        self.memory[self.index] = self.transition_type(*args)\n",
    "        \n",
    "        self.index = (self.index+1)%self.capacity # for circular memory\n",
    "\n",
    "    def sample(self, batch_size, collapsed=True):\n",
    "        if batch_size > len(self.memory):\n",
    "            batch_size = len(self.memory)\n",
    "            \n",
    "        if self.n_steps <= 1:\n",
    "            # Directly sample transitions\n",
    "            memories = random.sample(self.memory, batch_size)\n",
    "            return self.unwrap_transition(*memories)\n",
    "        else:\n",
    "            # Sample initial transition indexes\n",
    "            indexes = random.sample(range(len(self.memory)), batch_size)\n",
    "            # Get the batch of n-consecutive-transitions starting from sampled indexes\n",
    "            all_transitions = [self.memory[i:i+self.n_steps] for i in indexes]\n",
    "            \n",
    "            memories = map(self.collapse_n_steps, all_transitions) if collapsed else all_transitions\n",
    "\n",
    "            return self.unwrap_transition(*memories)\n",
    "        \n",
    "    def collapse_n_steps(self, transitions):\n",
    "        state, action, next_state, reward, done = transitions[0]\n",
    "        discount = self.discount\n",
    "        for transition in transitions[1:]:\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                _, _, next_state, reward, done = transition\n",
    "                discount *= self.discount\n",
    "                reward += discount * reward\n",
    "        return state, action, next_state, reward, done\n",
    "    \n",
    "    def unwrap_transition(self, *transition):\n",
    "        state, action, next_state, reward, done = zip(*transition)\n",
    "        \n",
    "        states = torch.from_numpy(np.array(state)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(action)).to(self.device)\n",
    "        next_states = torch.from_numpy(np.array(next_state)).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(reward)).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(done)).to(self.device)\n",
    "\n",
    "        return states, actions, next_states, rewards, dones \n",
    "\n",
    "from segment_tree import MinSegmentTree, SumSegmentTree\n",
    "\n",
    "class PrioritizedReplayMemory(ReplayMemory):\n",
    "    def __init__(self, config, transition_type=Transition):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.capacity = config.get(\"capacity\", 1000)\n",
    "\n",
    "        tree_size = 1\n",
    "        while tree_size < self.capacity:\n",
    "            tree_size *= 2\n",
    "        \n",
    "        # size needs to be power of 2\n",
    "        self.sum_tree = SumSegmentTree(tree_size)\n",
    "        self.min_tree = MinSegmentTree(tree_size)        \n",
    "        \n",
    "        self.index = 0\n",
    "        self.transition_type = transition_type\n",
    "\n",
    "        self.memory = []\n",
    "        self.memory_indexes = []\n",
    "        self.device = config.get(\"device\", torch.device(\"cpu\"))\n",
    "        \n",
    "        # emphasis on the priority\n",
    "        self.alpha = config.get(\"alpha\", 0.6)\n",
    "        # control degree of correction B=1 fully corrects bias B=0 no correction\n",
    "        self.beta = config.get(\"beta\", 0.2) #  will go to 1\n",
    "        self.max_priority = 1  # priority for new samples, init as eps\n",
    "    \n",
    "    def store(self, *args):\n",
    "        # store the transition in memory\n",
    "        super().store(*args)\n",
    "        # add the index to the tree and the loss value\n",
    "        self.sum_tree[self.index] = self.max_priority ** self.alpha\n",
    "        self.min_tree[self.index] = self.max_priority ** self.alpha\n",
    "        # circular\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        \n",
    "    def get_sample_weights(self):\n",
    "        return np.array([self.calculate_weight(i) for i in self.memory_indexes])\n",
    "    \n",
    "    def sample(self, batch_size, collapsed=True):\n",
    "        self.memory_indexes = self.sample_proportional_indexes(batch_size)\n",
    "        memories = [self.memory[i] for i in self.memory_indexes]\n",
    "        \n",
    "        return super().unwrap_transition(*memories)\n",
    "    \n",
    "    def sample_proportional_indexes(self, batch_size):\n",
    "        \"\"\"\n",
    "        proportional to the loss for this particular example in the Bellman update\n",
    "        By adjusting the priorities, we introduce bias to data distribution\n",
    "        To get the value of the weights for each sample using beta\n",
    "        \"\"\"\n",
    "        indexes = []\n",
    "        p_total = self.sum_tree.sum(0, len(self.memory) - 1)\n",
    "        segment = p_total / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indexes.append(idx)\n",
    "        return indexes\n",
    "    \n",
    "    def calculate_weight(self, index):\n",
    "        \"\"\"Calculate the weight of the experience at index using the paper.\"\"\"\n",
    "        sum_weights = self.sum_tree.sum()\n",
    "        \n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / sum_weights\n",
    "        max_weight = (p_min * len(self.memory)) ** (-self.beta)\n",
    "        \n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[index] / sum_weights\n",
    "        weight = (p_sample * len(self.memory)) ** (-self.beta)\n",
    "        \n",
    "        # normalize the weight\n",
    "        return weight / max_weight\n",
    "    \n",
    "    def update_priorities(self, priorities):\n",
    "        for index, priority in zip(self.memory_indexes, priorities):\n",
    "            if priority < 0 or (index < 0 and index > len(self.memory)):\n",
    "                return\n",
    "            \n",
    "            self.sum_tree[index] = priority ** self.alpha\n",
    "            self.min_tree[index] = priority ** self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from metrics import Metrics\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, params):\n",
    "        self.q_net = {}\n",
    "        self.q_target_net = {} \n",
    "        self.optimizer= {}        \n",
    "        \n",
    "        self.policy = params.get(\"policy\", \"CnnPolicy\")        \n",
    "        self.episode_num = params.get(\"episode_num\", 10)\n",
    "\n",
    "        self.epsilon = params.get(\"epsilon_max \", 1)\n",
    "        self.epsilon_min = params.get(\"epsilon_min \", 0.1)\n",
    "        self.epsilon_decay = params.get(\"epsilon_decay\", 0.995)\n",
    "        \n",
    "        self.learning_rate = params.get(\"learning_rate\", 5e-4)\n",
    "        self.discount = params.get(\"discount\", 0.2)\n",
    "        self.batch_size = params.get(\"batch_size\", 32)\n",
    "        self.device = params.get(\"device\", torch.device(\"cpu\"))\n",
    "        \n",
    "        self.memory_capacity = params.get(\"memory_capacity\", 1000)\n",
    "        self.memory = {} # this is the memory buffer -> setting a limit\n",
    "        self.n_steps = params.get(\"n_steps\", 2)\n",
    "        self.prioritize_memory = params.get(\"prioritize_memory\", False)\n",
    "        \n",
    "        self.double = params.get(\"double\", False)\n",
    "        self.noisy_net = params.get(\"noisy_net\", False)\n",
    "        self.prioritized_memory = params.get(\"prioritized_memory\", False)\n",
    "        \n",
    "        self.timeout = params.get(\"timeout_minute\", 0) * 60 # in minutes\n",
    "        self.time = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.to_save_model = params.get(\"save_model\", False)\n",
    "        \n",
    "        use_metrics = params.get(\"use_metrics\", False)\n",
    "        save_params = params.get(\"save_params\", False)\n",
    "        \n",
    "        if save_params and use_metrics:\n",
    "            self.save_params(params)\n",
    "            \n",
    "        self.metrics = Metrics(self.policy, \"training_results\", use_metrics)\n",
    "        \n",
    "    def initialize_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # Xavier initialization for Conv2d weights\n",
    "            init.xavier_uniform_(m.weight)\n",
    "            init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            # Xavier initialization for Linear weights\n",
    "            init.xavier_uniform_(m.weight)\n",
    "            init.zeros_(m.bias)\n",
    "\n",
    "    def create_network(self, env):\n",
    "        if self.policy == \"CnnPolicy\":\n",
    "            self.create_CNN(env)\n",
    "        \n",
    "        if self.policy == \"MlpPolicy\":\n",
    "            self.create_MLP_Network(env)\n",
    "        \n",
    "        self.q_net.apply(self.initialize_weights)    \n",
    "        self.update_target_network()\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def create_CNN(self, env):\n",
    "        self.num_states = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = CNN(self.num_states, self.num_actions, self.noisy_net).to(self.device)\n",
    "        self.q_target_net = CNN(self.num_states, self.num_actions, self.noisy_net).to(self.device)\n",
    "        \n",
    "    \n",
    "    def create_MLP_Network(self, env):\n",
    "        # the lanes\n",
    "        self.vehicle_count = env.observation_space.shape[0]\n",
    "        self.features = env.observation_space.shape[1]\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = MLPNetwork(self.features, self.num_actions, self.vehicle_count,  self.noisy_net).to(self.device)\n",
    "        self.q_target_net = MLPNetwork(self.features, self.num_actions, self.vehicle_count, self.noisy_net).to(self.device)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.q_target_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n",
    "    def learn(self, env):\n",
    "        self.create_network(env)\n",
    "        \n",
    "        self.memory = ReplayMemory({\n",
    "            \"capacity\": self.memory_capacity,\n",
    "            \"device\": self.device,\n",
    "            \"n_steps\": self.n_steps,\n",
    "        }) if not self.prioritize_memory else PrioritizedReplayMemory({\n",
    "            \"capacity\": self.memory_capacity,\n",
    "            \"device\": self.device,\n",
    "            \"alpha\": 0.6,\n",
    "            \"beta\": 0.2,\n",
    "        })\n",
    "\n",
    "        self.prefill_memory(env, self.batch_size)\n",
    "\n",
    "        start_time = time.time()        \n",
    "        \n",
    "        for epoch in tqdm(range(self.episode_num), desc=\"Training Model\"):\n",
    "            state = env.reset()[0]\n",
    "            \n",
    "            # True when agent reaches the end states (colliding or passing the time)\n",
    "            done = False \n",
    "            \n",
    "            truncated = False\n",
    "            episode_rewards = []\n",
    "            episode_loss = []\n",
    "            episode_len = 0\n",
    "            while(not done and not truncated):\n",
    "                # choose best action\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)\n",
    "                \n",
    "                episode_loss.append(self.experience_replay())\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                episode_len += 1\n",
    "                \n",
    "            self.metrics.add(\"rollout/rewards\", sum(episode_rewards) / len(episode_rewards), epoch)\n",
    "            self.metrics.add(\"rollout/exploration-rate\", self.epsilon, epoch)\n",
    "            self.metrics.add(\"rollout/episode-length\", episode_len, epoch)\n",
    "            self.metrics.add(\"train/loss\", sum(episode_loss) / len(episode_loss), epoch)\n",
    "            \n",
    "            if self.timeout:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                if elapsed_time > self.timeout:\n",
    "                    print(\"Timeout reached. Stopping training.\\n\")\n",
    "                    break\n",
    "            \n",
    "            # if epoch % 10 == 0:\n",
    "            self.decay_epsilon()\n",
    "            self.update_target_network()\n",
    "            \n",
    "        self.metrics.close()\n",
    "        \n",
    "        if self.to_save_model:\n",
    "            self.save_model()\n",
    "        \n",
    "    # either the policies are able to get miltuple actions and into the NN or the input of NN should be able to handle all of these\n",
    "    # output (one of): {0: 'LANE_LEFT', 1: 'IDLE', 2: 'LANE_RIGHT', 3: 'FASTER', 4: 'SLOWER'}\n",
    "    def get_action(self, state, eval_mode=False):\n",
    "        # amount of exploration reduces with the epsilon value\n",
    "        if random.random() <= self.epsilon and not eval_mode and self.epsilon_decay != 0: \n",
    "            return random.randrange(self.num_actions)\n",
    "        \n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float32).to(self.device)\n",
    "        actions = self.q_net(state)\n",
    "        return torch.argmax(actions).item()             \n",
    "\n",
    "\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        # I need the weights here for the loss\n",
    "        states, actions, next_states, rewards, dones = self.memory.sample(self.batch_size)\n",
    "        if self.prioritize_memory:\n",
    "            memory_weights = self.memory.get_sample_weights()\n",
    "            memory_weights = torch.tensor(memory_weights).float().to(self.device)\n",
    "        \n",
    "        q_pred = self.q_net(states)\n",
    "        # q value of the action taken\n",
    "        q_pred = q_pred.gather(1, actions.view(-1, 1)) \n",
    "        q_pred = q_pred.squeeze(1)\n",
    "\n",
    "        # Double DQN\n",
    "        if self.double:\n",
    "             # pick best actions from policy network\n",
    "            q_best_action = self.q_net(next_states)\n",
    "            _, q_best_action = q_best_action.max(dim=1)\n",
    "            q_best_action = q_best_action.unsqueeze(1)\n",
    "            \n",
    "            # use those actions for the target policy\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.gather(1, q_best_action)\n",
    "            q_target = q_target.squeeze(1)\n",
    "        else:\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.max(dim=1).values\n",
    "        \n",
    "        # setting Q(s',a') to 0 when the current state is a terminal state\n",
    "        q_target[dones] = 0.0\n",
    "        \n",
    "        y_j = rewards + (self.discount * q_target)\n",
    "        \n",
    "        \n",
    "        # calculate the loss as the mean-squared error of yj and qpred\n",
    "        element_loss = F.mse_loss(y_j, q_pred, reduction=\"none\")\n",
    "        if self.prioritize_memory:\n",
    "            element_loss = element_loss * memory_weights\n",
    "        loss = element_loss.mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.noisy_net:\n",
    "            self.q_net.reset_noise()\n",
    "            self.q_target_net.reset_noise()\n",
    "        \n",
    "        if self.prioritize_memory:\n",
    "            loss_for_prior = np.array(element_loss.detach().cpu())\n",
    "            # so that each transition is sampled\n",
    "            new_priorities = loss_for_prior + 1e-5\n",
    "            self.memory.update_priorities(new_priorities)\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "    def prefill_memory(self, env, prefill_num):\n",
    "        for _ in tqdm(range(prefill_num), desc=\"Prefilling Memory \"):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            while not done and not truncated:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)    \n",
    "                \n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon_decay != 0:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "    \n",
    "    def evaluate(self, env, episode_num):\n",
    "        # add camera here\n",
    "        for _ in tqdm(range(episode_num), desc=\"Evaluating Model\"):\n",
    "            state = env.reset()[0]  \n",
    "            done = False      \n",
    "            truncated = False \n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not done and not truncated):  \n",
    "                # Select best action   \n",
    "                action = self.get_action(state, eval_mode=True)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                state = next_state\n",
    "                env.render()\n",
    "        \n",
    "    def save_model(self):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "        self.metrics.create_folder(folder_name)\n",
    "        new_model_num = str(len(os.listdir(\"./\" +folder_name)) + 1)\n",
    "        file_name = f'{folder_name}/DQN_{new_model_num}_{self.time}.pth'\n",
    "        state = {'state_dict': self.q_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()}\n",
    "        torch.save(state, file_name)\n",
    "        \n",
    "    def load_model(self, env, file_name):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "                \n",
    "        filename = folder_name + \"/\" + file_name + \".pth\"\n",
    "        self.create_network(env)\n",
    "        \n",
    "        models = torch.load(filename, map_location=self.device)\n",
    "        \n",
    "        self.q_net.load_state_dict(models['state_dict'])\n",
    "        self.optimizer.load_state_dict(models['optimizer'])\n",
    "    \n",
    "    def save_params(self, params):\n",
    "        folder_name = \"hyperparameters\"\n",
    "        self.metrics.create_folder(folder_name)\n",
    "        \n",
    "        file_name = f'./{folder_name}/{self.policy}_DQN_{self.time}'\n",
    "        with open(file_name + '.txt', 'w') as file:\n",
    "            file.write(json.dumps(str(params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "policy = \"CnnPolicy\"\n",
    "# policy = \"MlpPolicy\"\n",
    "\n",
    "if policy == \"CnnPolicy\":\n",
    "    config={\n",
    "        \"lanes_count\" : 3,\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 64),\n",
    "            \"stack_size\": 4,\n",
    "            \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion keep this conversion this is in the highway env page\n",
    "            \"scaling\": 1.75,\n",
    "        },\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"lanes_count\" : 3,\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"vehicles_count\": 5,\n",
    "            \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "            \"features_range\": {\n",
    "                \"x\": [-100, 100],\n",
    "                \"y\": [-100, 100],\n",
    "                \"vx\": [-20, 20],\n",
    "                \"vy\": [-20, 20]\n",
    "            },\n",
    "            \"absolute\": False,\n",
    "            \"order\": \"sorted\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefilling Memory : 100%|██████████| 3/3 [00:00<00:00,  5.20it/s]\n",
      "Training Model: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Evaluating Model:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "params = {\n",
    "    'policy' : policy,\n",
    "    'episode_num' : 10,\n",
    "    'discount' : 0.7,\n",
    "    'batch_size' : 3,\n",
    "    'learning_rate': 4e-5,\n",
    "    'n_steps': 3,\n",
    "    'double': True,\n",
    "    # 'epsilon_decay': 0, # To deactivate epsilon decay \n",
    "    'noisy_net': False,\n",
    "    'prioritize_memory': False,\n",
    "    'device' : torch.device(\"mps\"),\n",
    "    'memory_capacity' : 10000,\n",
    "    'timeout_minute': 15,\n",
    "    'use_metrics' : False,\n",
    "    'save_model': False,\n",
    "    'save_params': True, # metrics also need to be on\n",
    "}\n",
    "\n",
    "seed = 72 # Our group number\n",
    "for i in range(1):\n",
    "    seed += i\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # so that the GPU seed is also random\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # not optimize for when the network doesn't get the same input size \n",
    "        # (because of batch sizes and get best action input different sizes)\n",
    "        torch.backends.cudnn.benchmark = False \n",
    "        torch.backends.cudnn.deterministic = True # slows down training\n",
    "    \n",
    "    dqn_agent = DQNAgent(params)\n",
    "    env = gym.make('highway-fast-v0', render_mode='rgb_array', config=config)\n",
    "    dqn_agent.learn(env)\n",
    "    \n",
    "    env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "    dqn_agent.evaluate(env, 5)\n",
    "    \n",
    "\n",
    "# # if you wanna save a model again\n",
    "# # dqn_agent.save_model(\"highway_dqn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/m_r158w50cqfn3ydymrpm7rc0000gn/T/ipykernel_17432/120296199.py:270: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  models = torch.load(filename, map_location=self.device)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CnnPolicy_save_models/DQN_1_20241230164948.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [218]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighway-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m      3\u001b[0m dqn_agent_test \u001b[38;5;241m=\u001b[39m DQNAgent(params)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdqn_agent_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDQN_1_20241230164948\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m dqn_agent_test\u001b[38;5;241m.\u001b[39mevaluate(env, \u001b[38;5;241m20\u001b[39m)\n",
      "Input \u001b[0;32mIn [215]\u001b[0m, in \u001b[0;36mDQNAgent.load_model\u001b[0;34m(self, env, file_name)\u001b[0m\n\u001b[1;32m    267\u001b[0m filename \u001b[38;5;241m=\u001b[39m folder_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_network(env)\n\u001b[0;32m--> 270\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net\u001b[38;5;241m.\u001b[39mload_state_dict(models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mload_state_dict(models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CnnPolicy_save_models/DQN_1_20241230164948.pth'"
     ]
    }
   ],
   "source": [
    "env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "\n",
    "dqn_agent_test = DQNAgent(params)\n",
    "dqn_agent_test.load_model(env, \"DQN_1_20241230164948\")\n",
    "\n",
    "dqn_agent_test.evaluate(env, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-773cba20f143bb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-773cba20f143bb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6010;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir training_results --host localhost --port 6010"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
