{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Install environment and agent\n",
    "# !pip install highway-env\n",
    "# !pip install --upgrade sympy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning using existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the pesudocode that will be followed when creating the DQN\n",
    "\n",
    "Useful: https://www.youtube.com/watch?v=RVMpm86equc&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=2\n",
    "\n",
    "https://github.com/saashanair/rl-series/tree/master/dqn\n",
    "\n",
    "https://github.com/johnnycode8/gym_solutions/blob/main/frozen_lake_dql.py\n",
    "\n",
    "<img src=\"DQN.png\" style=\"width: 900px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "Potential Problems: https://www.reddit.com/r/reinforcementlearning/comments/1555wgi/dqn_loss_increasing_and_rewards_decreasing/\n",
    "\n",
    "\n",
    "For CNN:\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/3l5qu7/rules_of_thumb_for_cnn_architectures/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "\n",
    "# Define model\n",
    "class MLPNetwork(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super(MLPNetwork, self).__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # output layer\n",
    "        self.out2 = nn.Linear(out_actions, 1) # output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = F.relu(self.out(x))         \n",
    "        x = self.out2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CNN, self).__init__()\n",
    "        # greyscale Image is(stack,height,width)\n",
    "        stack, height, width = input_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(stack,32,kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(32,64,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(64,128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv2d(128,256, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        # This is for finding the size to dense more robust compared to decision manually\n",
    "        with torch.no_grad():\n",
    "                # Torch uses(1,channels,height,width)\n",
    "                test = torch.zeros(1, stack, height, width)\n",
    "                find_conv_size = self.conv(test)\n",
    "                conv_size = find_conv_size.numel()\n",
    "        self.fc = nn.Linear(conv_size,num_actions)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.unsqueeze(2)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "# Define memory for Experience Replay\n",
    "# TODO: Prioritize, n-steps\n",
    "\n",
    "from collections.__init__ import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, config, transition_type=Transition):\n",
    "        self.capacity = config.get(\"capacity\", 1000)\n",
    "        self.index = 0\n",
    "        self.transition_type = transition_type\n",
    "        self.discount = config.get(\"discount\", 0.99)\n",
    "\n",
    "        self.memory = []\n",
    "        self.device = config.get(\"device\", torch.device(\"cpu\"))\n",
    "        self.n_steps = config.get(\"n_steps\", 2)\n",
    "    \n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.index = len(self.memory) - 1\n",
    "        elif len(self.memory) > self.capacity:\n",
    "            self.memory = self.memory[:self.capacity]\n",
    "        # Faster than append and pop\n",
    "        self.memory[self.index] = self.transition_type(*args)\n",
    "        \n",
    "        self.index = (self.index+1)%self.capacity # for circular memory\n",
    "\n",
    "    def sample(self, batch_size, collapsed=True):\n",
    "        if batch_size >  len(self.memory):\n",
    "            batch_size = len(self.memory)\n",
    "            \n",
    "        if self.n_steps <= 1:\n",
    "            # Directly sample transitions\n",
    "            memories = random.sample(self.memory, batch_size)\n",
    "            return self.unwrap_transition(*memories)\n",
    "        else:\n",
    "            # Sample initial transition indexes\n",
    "            indexes = random.sample(range(len(self.memory)), batch_size)\n",
    "            # Get the batch of n-consecutive-transitions starting from sampled indexes\n",
    "            all_transitions = [self.memory[i:i+self.n_steps] for i in indexes]\n",
    "            \n",
    "            memories = map(self.collapse_n_steps, all_transitions) if collapsed else all_transitions\n",
    "\n",
    "            return self.unwrap_transition(*memories)\n",
    "        \n",
    "        # self.collapse_n_steps(states, actions, next_states, rewards, done)\n",
    "    def collapse_n_steps(self, transitions):\n",
    "        state, action, next_state, reward, done = transitions[0]\n",
    "        discount = self.discount\n",
    "        for transition in transitions[1:]:\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                _, _, next_state, reward, done = transition\n",
    "                discount *= self.discount\n",
    "                reward += discount * reward\n",
    "        return state, action, next_state, reward, done\n",
    "    \n",
    "    def unwrap_transition(self, *transition):\n",
    "        state, action, next_state, reward, done = zip(*transition)\n",
    "        \n",
    "        states = torch.from_numpy(np.array(state)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(action)).to(self.device)\n",
    "        next_states = torch.from_numpy(np.array(next_state)).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(reward)).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(done)).to(self.device)\n",
    "\n",
    "        return states, actions, next_states, rewards, dones \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, policy, result_file_name, use_metrics, time):\n",
    "        self.use_metrics = use_metrics\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        new_num = str(len(os.listdir(\"./\" +result_file_name)) + 1)\n",
    "        file_name = f'{result_file_name}/{policy}_DQN_{new_num}_{time}'\n",
    "        self.writer = SummaryWriter(log_dir=file_name, flush_secs=60)\n",
    "            \n",
    "    def add(self, type, y, x):\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        self.writer.add_scalar(type, y, x)\n",
    "    def close(self):\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, params):\n",
    "        self.q_net = {}\n",
    "        self.q_target_net = {} \n",
    "        self.optimizer= {}        \n",
    "        \n",
    "        self.policy = params.get(\"policy\", \"CnnPolicy\")        \n",
    "        self.episode_num = params.get(\"episode_num\", 10)\n",
    "\n",
    "        self.epsilon = params.get(\"epsilon_max \", 1)\n",
    "        self.epsilon_min = params.get(\"epsilon_min \", 0.1)\n",
    "        self.epsilon_decay = params.get(\"epsilon_decay\", 0.995)\n",
    "        \n",
    "        self.learning_rate = params.get(\"learning_rate\", 3e-4)\n",
    "        self.discount = params.get(\"discount\", 0.2)\n",
    "        self.batch_size = params.get(\"batch_size\", 32 )\n",
    "        self.device = params.get(\"device\", torch.device(\"cpu\"))\n",
    "        \n",
    "        self.memory_capacity = params.get(\"memory_capacity\", 1000)\n",
    "        self.memory = {} # this is the memory buffer -> setting a limit\n",
    "        self.n_steps = params.get(\"n_steps\", 2)\n",
    "        \n",
    "        self.double = True\n",
    "        \n",
    "        self.timeout = params.get(\"timeout_minute\", 60) * 60 # in minutes\n",
    "        ct = datetime.datetime.now()\n",
    "        self.time = str(ct).replace(\" \", \"|\")\n",
    "        self.to_save_model = params.get(\"save_model\", False)\n",
    "        \n",
    "        use_metrics = params.get(\"use_metrics\", False)\n",
    "        if use_metrics:\n",
    "            self.create_folder(\"training_results\")\n",
    "            self.save_params(params)\n",
    "            \n",
    "        self.metrics = Metrics(self.policy, \"training_results\", use_metrics, self.time)\n",
    "        \n",
    "    def create_network(self, env):\n",
    "        if self.policy == \"CnnPolicy\":\n",
    "            self.create_CNN(env)\n",
    "        \n",
    "        if self.policy == \"MlpPolicy\":\n",
    "            self.create_MLP_Network(env)\n",
    "            \n",
    "        self.update_target_network()\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)\n",
    "    \n",
    "    def create_CNN(self, env):\n",
    "        self.num_states = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = CNN(self.num_states, self.num_actions).to(self.device)\n",
    "        self.q_target_net = CNN(self.num_states, self.num_actions).to(self.device)\n",
    "        \n",
    "    \n",
    "    def create_MLP_Network(self, env):\n",
    "        # the lanes\n",
    "        self.num_states = env.observation_space.shape[1]\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = MLPNetwork(self.num_states, self.num_states, self.num_actions).to(self.device)\n",
    "        self.q_target_net = MLPNetwork(self.num_states, self.num_states, self.num_actions).to(self.device)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        # make the weights and biases the same\n",
    "        self.q_target_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n",
    "    def learn(self, env):\n",
    "        self.create_network(env)\n",
    "        \n",
    "        self.memory = ReplayMemory({\n",
    "            \"capacity\": self.memory_capacity,\n",
    "            \"device\": self.device,\n",
    "            \"n_steps\": self.n_steps,\n",
    "        })\n",
    "\n",
    "        self.prefill_memory(env, self.batch_size)\n",
    "\n",
    "        start_time = time.time()        \n",
    "        \n",
    "        for epoch in tqdm(range(self.episode_num), desc=\"Training Model\"):\n",
    "            state = env.reset()[0]\n",
    "            \n",
    "            # True when agent reaches the end states (colliding or passing the time)\n",
    "            done = False \n",
    "            \n",
    "            # TODO: see how many actions until truncate\n",
    "            # True when agent takes more than some actions \n",
    "            truncated = False\n",
    "            episode_rewards = []\n",
    "            episode_loss = []\n",
    "            episode_len = 0\n",
    "            while(not done and not truncated):\n",
    "                # choose best action\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)\n",
    "                \n",
    "                episode_loss.append(self.experience_replay())\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                episode_len += 1\n",
    "                \n",
    "            self.metrics.add(\"rollout/rewards\", sum(episode_rewards) / len(episode_rewards), epoch)\n",
    "            self.metrics.add(\"rollout/exploration-rate\", self.epsilon, epoch)\n",
    "            self.metrics.add(\"rollout/episode-length\", episode_len, epoch)\n",
    "            self.metrics.add(\"train/loss\", sum(episode_loss) / len(episode_loss), epoch)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time > self.timeout:\n",
    "                print(\"Timeout reached. Stopping training.\\n\")\n",
    "                break\n",
    "            \n",
    "            if epoch % 1000 == 0:\n",
    "                self.decay_epsilon()\n",
    "                self.update_target_network()\n",
    "            \n",
    "        self.metrics.close()\n",
    "        \n",
    "        if self.to_save_model:\n",
    "            self.save_model()\n",
    "        \n",
    "    # either the policies are able to get miltuple actions and into the NN or the input of NN should be able to handle all of these\n",
    "    # output (one of): {0: 'LANE_LEFT', 1: 'IDLE', 2: 'LANE_RIGHT', 3: 'FASTER', 4: 'SLOWER'}\n",
    "    def get_action(self, state):\n",
    "        if random.random() <= self.epsilon: # amount of exploration reduces with the epsilon value\n",
    "            return random.randrange(self.num_actions)\n",
    "        \n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float32).to(self.device)\n",
    "        actions = self.q_net(state)\n",
    "        return torch.argmax(actions).item()             \n",
    "\n",
    "    def experience_replay(self):\n",
    "        states, actions, next_states, rewards, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        q_pred = self.q_net(states)\n",
    "        # q value of the action taken\n",
    "        q_pred = q_pred.gather(1, actions.view(-1, 1, 1)) \n",
    "        q_pred = q_pred.squeeze(1)\n",
    "\n",
    "        \n",
    "        # Double DQN\n",
    "        if self.double:\n",
    "             # pick best actions from policy network\n",
    "            q_best_action = self.q_net(next_states)\n",
    "            _, q_best_action = q_best_action.max(dim=1)\n",
    "            q_best_action = q_best_action.unsqueeze(1)\n",
    "            \n",
    "            # use those actions for the target policy\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.gather(1, q_best_action)\n",
    "            q_target = q_target.squeeze(1)\n",
    "        else:\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.max(dim=1).values\n",
    "        \n",
    "        # setting Q(s',a') to 0 when the current state is a terminal state\n",
    "        q_target[dones] = 0.0\n",
    "        \n",
    "        y_j = rewards + (self.discount * q_target)\n",
    "        y_j = y_j.mean(dim=0, keepdim=True)\n",
    "        y_j = y_j.view(-1, 1)\n",
    "        \n",
    "        # calculate the loss as the mean-squared error of yj and qpred\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(y_j, q_pred).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "    def prefill_memory(self, env, prefill_num):\n",
    "        for _ in tqdm(range(prefill_num), desc=\"Prefilling Memory \"):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            while not done and not truncated:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)    \n",
    "                \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "\n",
    "    def save_model(self):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "        self.create_folder(folder_name)\n",
    "        new_model_num = str(len(os.listdir(\"./\" +folder_name)) + 1)\n",
    "        file_name = f'{folder_name}/DQN_{new_model_num}_{self.time}.pth'\n",
    "        state = {'state_dict': self.q_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()}\n",
    "        torch.save(state, file_name)\n",
    "        \n",
    "    def load_model(self, env, file_name):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "                \n",
    "        filename = folder_name + \"/\" + file_name + \".pth\"\n",
    "        self.create_network(env)\n",
    "        \n",
    "        models = torch.load(filename, map_location=self.device)\n",
    "        \n",
    "        self.q_net.load_state_dict(models['state_dict'])\n",
    "        self.q_net.load_state_dict(models['state_dict'])\n",
    "        self.optimizer.load_state_dict(models['optimizer'])\n",
    "    \n",
    "    def save_params(self, params):\n",
    "        folder_name = \"hyperparameters\"\n",
    "        self.create_folder(folder_name)\n",
    "        \n",
    "        file_name = f'./{folder_name}/{self.policy}_DQN_{self.time}'\n",
    "        with open(file_name + '.txt', 'w') as file:\n",
    "            file.write(json.dumps(str(params)))\n",
    "\n",
    "    def create_folder(self, directory_name):\n",
    "        try:\n",
    "            os.mkdir(directory_name)\n",
    "            print(f\"Directory '{directory_name}' created successfully.\")\n",
    "        except FileExistsError:\n",
    "            return\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "policy = \"CnnPolicy\"\n",
    "# policy = \"MlpPolicy\"\n",
    "\n",
    "if policy == \"CnnPolicy\":\n",
    "    config={\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 64),\n",
    "            \"stack_size\": 4,\n",
    "            \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion keep this conversion this is in the highway env page\n",
    "            \"scaling\": 1.75,\n",
    "        },\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"vehicles_count\": 5,\n",
    "            \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "            \"features_range\": {\n",
    "                \"x\": [-100, 100],\n",
    "                \"y\": [-100, 100],\n",
    "                \"vx\": [-20, 20],\n",
    "                \"vy\": [-20, 20]\n",
    "            },\n",
    "            \"absolute\": False,\n",
    "            \"order\": \"sorted\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefilling Memory : 100%|██████████| 2/2 [00:00<00:00,  2.92it/s]\n",
      "Training Model:   2%|▎         | 25/1000 [00:08<05:26,  2.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m dqn_agent \u001b[38;5;241m=\u001b[39m DQNAgent(params)\n\u001b[1;32m     15\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighway-fast-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [90]\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    107\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstore(state, action, next_state, reward, done)\n\u001b[0;32m--> 110\u001b[0m episode_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience_replay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    112\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    114\u001b[0m episode_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Input \u001b[0;32mIn [90]\u001b[0m, in \u001b[0;36mDQNAgent.experience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    179\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(y_j, q_pred)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 180\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'policy' : policy,\n",
    "    'episode_num' : 1000,\n",
    "    'discount' : 0.2,\n",
    "    'batch_size' : 2,\n",
    "    'n_steps': 1,\n",
    "    'device' : torch.device(\"mps\"),\n",
    "    'memory_capacity' : 10000,\n",
    "    'timeout_minute': 1,\n",
    "    'use_metrics' : False,\n",
    "    'save_model': False,\n",
    "}\n",
    "\n",
    "dqn_agent = DQNAgent(params)\n",
    "env = gym.make('highway-fast-v0', render_mode='rgb_array', config=config)\n",
    "\n",
    "dqn_agent.learn(env)\n",
    "\n",
    "# if you wanna save a model again\n",
    "# dqn_agent.save_model(\"highway_dqn_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b95d3aa0fdeff6c3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b95d3aa0fdeff6c3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6010;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir training_results --host localhost --port 6010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "# env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "# dqn_agent_test = DQNAgent(policy=policy, episode_num=0,epsilon=0.0, discount=1e-2, batch_size=10, device=torch.device(\"mps\"), memory_capacity=5000)\n",
    "# dqn_agent_test.load_model(env, \"DQN_2_2024-12-20|14:29:48.867232\")\n",
    "\n",
    "for i in range(10):\n",
    "    state = env.reset()[0]  \n",
    "    done = False      \n",
    "    truncated = False \n",
    "\n",
    "    # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "    while(not done and not truncated):  \n",
    "        # Select best action   \n",
    "        action = dqn_agent.get_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
