{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Install environment and agent\n",
    "# !pip install highway-env\n",
    "# !pip install --upgrade sympy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning using existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the pesudocode that will be followed when creating the DQN\n",
    "\n",
    "Useful: https://www.youtube.com/watch?v=RVMpm86equc&list=PL58zEckBH8fCMIVzQCRSZVPUp3ZAVagWi&index=2\n",
    "\n",
    "https://github.com/saashanair/rl-series/tree/master/dqn\n",
    "\n",
    "https://github.com/johnnycode8/gym_solutions/blob/main/frozen_lake_dql.py\n",
    "\n",
    "<img src=\"DQN.png\" style=\"width: 900px;\" align=\"left\"/>\n",
    "\n",
    "\n",
    "Potential Problems: https://www.reddit.com/r/reinforcementlearning/comments/1555wgi/dqn_loss_increasing_and_rewards_decreasing/\n",
    "\n",
    "\n",
    "For CNN:\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/3l5qu7/rules_of_thumb_for_cnn_architectures/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros( 2 * capacity - 1 )\n",
    "        self.data = np.zeros( capacity, dtype=object )\n",
    "        self.writer = 0\n",
    "\n",
    "    # propagate upwards to update the sum values\n",
    "    def _propagate(self, index, change):\n",
    "        parent = (index - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # get the leaf nodes (Transaction)\n",
    "    def _retrieve(self, index, s):\n",
    "        left = 2 * index + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return index\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        index = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(index, p)\n",
    "\n",
    "        self.write += 1\n",
    "        # circular\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, index, p):\n",
    "        change = p - self.tree[index]\n",
    "\n",
    "        self.tree[index] = p\n",
    "        self._propagate(index, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        index = self._retrieve(0, s)\n",
    "        data_index = index - self.capacity + 1\n",
    "\n",
    "        return (index, self.tree[index], self.data[data_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "\n",
    "# Define model\n",
    "class MLPNetwork(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super(MLPNetwork, self).__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # output layer\n",
    "        self.out2 = nn.Linear(out_actions, 1) # output layer\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = F.relu(self.out(x))         \n",
    "        x = self.out2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CNN, self).__init__()\n",
    "        # greyscale Image is(stack,height,width)\n",
    "        stack, height, width = input_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(stack,32,kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(32,64,kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(64,128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Conv2d(128,256, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1,stride=2),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        # This is for finding the size to dense more robust compared to decision manually\n",
    "        with torch.no_grad():\n",
    "                # Torch uses(1,channels,height,width)\n",
    "                test = torch.zeros(1, stack, height, width)\n",
    "                find_conv_size = self.conv(test)\n",
    "                conv_size = find_conv_size.numel()\n",
    "        self.fc = nn.Linear(conv_size,num_actions)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.unsqueeze(2)\n",
    "        print(x)\n",
    "        x = self.softmax(x)\n",
    "        print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define memory for Experience Replay\n",
    "# TODO: Prioritize, n-steps\n",
    "\n",
    "from collections.__init__ import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, config, transition_type=Transition):\n",
    "        self.capacity = config.get(\"capacity\", 1000)\n",
    "        self.index = 0\n",
    "        self.transition_type = transition_type\n",
    "        self.discount = config.get(\"discount\", 0.99)\n",
    "\n",
    "        self.memory = []\n",
    "        self.device = config.get(\"device\", torch.device(\"cpu\"))\n",
    "        self.n_steps = config.get(\"n_steps\", 2)\n",
    "    \n",
    "    def store(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.index = len(self.memory) - 1\n",
    "        elif len(self.memory) > self.capacity:\n",
    "            self.memory = self.memory[:self.capacity]\n",
    "        # Faster than append and pop\n",
    "        self.memory[self.index] = self.transition_type(*args)\n",
    "        \n",
    "        self.index = (self.index+1)%self.capacity # for circular memory\n",
    "\n",
    "    def sample(self, batch_size, collapsed=True):\n",
    "        if batch_size >  len(self.memory):\n",
    "            batch_size = len(self.memory)\n",
    "            \n",
    "        if self.n_steps <= 1:\n",
    "            # Directly sample transitions\n",
    "            memories = random.sample(self.memory, batch_size)\n",
    "            return self.unwrap_transition(*memories)\n",
    "        else:\n",
    "            # Sample initial transition indexes\n",
    "            indexes = random.sample(range(len(self.memory)), batch_size)\n",
    "            # Get the batch of n-consecutive-transitions starting from sampled indexes\n",
    "            all_transitions = [self.memory[i:i+self.n_steps] for i in indexes]\n",
    "            \n",
    "            memories = map(self.collapse_n_steps, all_transitions) if collapsed else all_transitions\n",
    "\n",
    "            return self.unwrap_transition(*memories)\n",
    "        \n",
    "        # self.collapse_n_steps(states, actions, next_states, rewards, done)\n",
    "    def collapse_n_steps(self, transitions):\n",
    "        state, action, next_state, reward, done = transitions[0]\n",
    "        discount = self.discount\n",
    "        for transition in transitions[1:]:\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                _, _, next_state, reward, done = transition\n",
    "                discount *= self.discount\n",
    "                reward += discount * reward\n",
    "        return state, action, next_state, reward, done\n",
    "    \n",
    "    def unwrap_transition(self, *transition):\n",
    "        state, action, next_state, reward, done = zip(*transition)\n",
    "        \n",
    "        states = torch.from_numpy(np.array(state)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.array(action)).to(self.device)\n",
    "        next_states = torch.from_numpy(np.array(next_state)).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.array(reward)).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.array(done)).to(self.device)\n",
    "\n",
    "        return states, actions, next_states, rewards, dones \n",
    "\n",
    "class PrioritizedReplayMemory(ReplayMemory):\n",
    "    def __init__(self, config, transition_type=Transition):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.capacity = config.get(\"capacity\", 1000)\n",
    "        self.tree = SumTree(self.capacity)\n",
    "        self.index = 0\n",
    "        self.transition_type = transition_type\n",
    "\n",
    "        self.memory = []\n",
    "        self.device = config.get(\"device\", torch.device(\"cpu\"))\n",
    "        \n",
    "        self.alpha = config.get(\"alpha\", 0.6)\n",
    "        self.beta = config.get(\"beta\", 0.2) #  will go to 1\n",
    "        self.max_priority = 1  # priority for new samples, init as eps\n",
    "        \n",
    "    def store(self, *args):\n",
    "        super().store(*args)\n",
    "        self.tree.add(self.max_priority ** self.alpha, )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, policy, result_file_name, use_metrics, time):\n",
    "        self.use_metrics = use_metrics\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        new_num = str(len(os.listdir(\"./\" +result_file_name)) + 1)\n",
    "        file_name = f'{result_file_name}/{policy}_DQN_{new_num}_{time}'\n",
    "        self.writer = SummaryWriter(log_dir=file_name, flush_secs=60)\n",
    "            \n",
    "    def add(self, type, y, x):\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        self.writer.add_scalar(type, y, x)\n",
    "    def close(self):\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, params):\n",
    "        self.q_net = {}\n",
    "        self.q_target_net = {} \n",
    "        self.optimizer= {}        \n",
    "        \n",
    "        self.policy = params.get(\"policy\", \"CnnPolicy\")        \n",
    "        self.episode_num = params.get(\"episode_num\", 10)\n",
    "\n",
    "        self.epsilon = params.get(\"epsilon_max \", 1)\n",
    "        self.epsilon_min = params.get(\"epsilon_min \", 0.1)\n",
    "        self.epsilon_decay = params.get(\"epsilon_decay\", 0.995)\n",
    "        \n",
    "        self.learning_rate = params.get(\"learning_rate\", 3e-4)\n",
    "        self.discount = params.get(\"discount\", 0.2)\n",
    "        self.batch_size = params.get(\"batch_size\", 32 )\n",
    "        self.device = params.get(\"device\", torch.device(\"cpu\"))\n",
    "        \n",
    "        self.memory_capacity = params.get(\"memory_capacity\", 1000)\n",
    "        self.memory = {} # this is the memory buffer -> setting a limit\n",
    "        self.n_steps = params.get(\"n_steps\", 2)\n",
    "        self.prioritize_memory = params.get(\"prioritize_memory\", False)\n",
    "        \n",
    "        self.double = True\n",
    "        \n",
    "        self.timeout = params.get(\"timeout_minute\", 0) * 60 # in minutes\n",
    "        ct = datetime.datetime.now()\n",
    "        self.time = str(ct).replace(\" \", \"|\")\n",
    "        self.to_save_model = params.get(\"save_model\", False)\n",
    "        \n",
    "        use_metrics = params.get(\"use_metrics\", False)\n",
    "        if use_metrics:\n",
    "            self.create_folder(\"training_results\")\n",
    "            self.save_params(params)\n",
    "            \n",
    "        self.metrics = Metrics(self.policy, \"training_results\", use_metrics, self.time)\n",
    "        \n",
    "    def create_network(self, env):\n",
    "        if self.policy == \"CnnPolicy\":\n",
    "            self.create_CNN(env)\n",
    "        \n",
    "        if self.policy == \"MlpPolicy\":\n",
    "            self.create_MLP_Network(env)\n",
    "            \n",
    "        self.update_target_network()\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)\n",
    "    \n",
    "    def create_CNN(self, env):\n",
    "        self.num_states = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = CNN(self.num_states, self.num_actions).to(self.device)\n",
    "        self.q_target_net = CNN(self.num_states, self.num_actions).to(self.device)\n",
    "        \n",
    "    \n",
    "    def create_MLP_Network(self, env):\n",
    "        # the lanes\n",
    "        self.num_states = env.observation_space.shape[1]\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.q_net = MLPNetwork(self.num_states, self.num_states, self.num_actions).to(self.device)\n",
    "        self.q_target_net = MLPNetwork(self.num_states, self.num_states, self.num_actions).to(self.device)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        # make the weights and biases the same\n",
    "        self.q_target_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n",
    "    def learn(self, env):\n",
    "        self.create_network(env)\n",
    "        \n",
    "        # if (self.prioritize_memory):\n",
    "        self.memory = ReplayMemory({\n",
    "            \"capacity\": self.memory_capacity,\n",
    "            \"device\": self.device,\n",
    "            \"n_steps\": self.n_steps,\n",
    "        })\n",
    "\n",
    "        self.prefill_memory(env, self.batch_size)\n",
    "\n",
    "        start_time = time.time()        \n",
    "        \n",
    "        for epoch in tqdm(range(self.episode_num), desc=\"Training Model\"):\n",
    "            state = env.reset()[0]\n",
    "            \n",
    "            # True when agent reaches the end states (colliding or passing the time)\n",
    "            done = False \n",
    "            \n",
    "            # TODO: see how many actions until truncate\n",
    "            # True when agent takes more than some actions \n",
    "            truncated = False\n",
    "            episode_rewards = []\n",
    "            episode_loss = []\n",
    "            episode_len = 0\n",
    "            while(not done and not truncated):\n",
    "                # choose best action\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)\n",
    "                \n",
    "                episode_loss.append(self.experience_replay())\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                episode_len += 1\n",
    "                \n",
    "            self.metrics.add(\"rollout/rewards\", sum(episode_rewards) / len(episode_rewards), epoch)\n",
    "            self.metrics.add(\"rollout/exploration-rate\", self.epsilon, epoch)\n",
    "            self.metrics.add(\"rollout/episode-length\", episode_len, epoch)\n",
    "            self.metrics.add(\"train/loss\", sum(episode_loss) / len(episode_loss), epoch)\n",
    "            \n",
    "            if self.timeout:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                if elapsed_time > self.timeout:\n",
    "                    print(\"Timeout reached. Stopping training.\\n\")\n",
    "                    break\n",
    "            \n",
    "            if epoch % 1000 == 0:\n",
    "                self.decay_epsilon()\n",
    "                self.update_target_network()\n",
    "            \n",
    "        self.metrics.close()\n",
    "        \n",
    "        if self.to_save_model:\n",
    "            self.save_model()\n",
    "        \n",
    "    # either the policies are able to get miltuple actions and into the NN or the input of NN should be able to handle all of these\n",
    "    # output (one of): {0: 'LANE_LEFT', 1: 'IDLE', 2: 'LANE_RIGHT', 3: 'FASTER', 4: 'SLOWER'}\n",
    "    def get_action(self, state, eval_mode=False):\n",
    "        if random.random() <= self.epsilon and not eval_mode: # amount of exploration reduces with the epsilon value\n",
    "            return random.randrange(self.num_actions)\n",
    "        \n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float32).to(self.device)\n",
    "        actions = self.q_net(state)\n",
    "        return torch.argmax(actions).item()             \n",
    "\n",
    "    def experience_replay(self):\n",
    "        states, actions, next_states, rewards, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        q_pred = self.q_net(states)\n",
    "        # q value of the action taken\n",
    "        q_pred = q_pred.gather(1, actions.view(-1, 1, 1)) \n",
    "        q_pred = q_pred.squeeze(1)\n",
    "\n",
    "        # Double DQN\n",
    "        if self.double:\n",
    "             # pick best actions from policy network\n",
    "            q_best_action = self.q_net(next_states)\n",
    "            _, q_best_action = q_best_action.max(dim=1)\n",
    "            q_best_action = q_best_action.unsqueeze(1)\n",
    "            \n",
    "            # use those actions for the target policy\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.gather(1, q_best_action)\n",
    "            q_target = q_target.squeeze(1)\n",
    "        else:\n",
    "            q_target = self.q_target_net(next_states)\n",
    "            q_target = q_target.max(dim=1).values\n",
    "        \n",
    "        # setting Q(s',a') to 0 when the current state is a terminal state\n",
    "        q_target[dones] = 0.0\n",
    "        \n",
    "        y_j = rewards + (self.discount * q_target)\n",
    "        y_j = y_j.mean(dim=0, keepdim=True)\n",
    "        y_j = y_j.view(-1, 1)\n",
    "        \n",
    "        # calculate the loss as the mean-squared error of yj and qpred\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(y_j, q_pred).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "    def prefill_memory(self, env, prefill_num):\n",
    "        for _ in tqdm(range(prefill_num), desc=\"Prefilling Memory \"):\n",
    "            done = False\n",
    "            truncated = False\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            while not done and not truncated:\n",
    "                action = env.action_space.sample()\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                self.memory.store(state, action, next_state, reward, done)    \n",
    "                \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "    \n",
    "    def evaluate(self, env, episode_num):\n",
    "        # add camera here\n",
    "        for _ in tqdm(range(episode_num), desc=\"Evaluating Model\"):\n",
    "            state = env.reset()[0]  \n",
    "            done = False      \n",
    "            truncated = False \n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not done and not truncated):  \n",
    "                # Select best action   \n",
    "                action = self.get_action(state, eval_mode=True)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                env.render()\n",
    "        \n",
    "    def save_model(self):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "        self.create_folder(folder_name)\n",
    "        new_model_num = str(len(os.listdir(\"./\" +folder_name)) + 1)\n",
    "        file_name = f'{folder_name}/DQN_{new_model_num}_{self.time}.pth'\n",
    "        state = {'state_dict': self.q_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict()}\n",
    "        torch.save(state, file_name)\n",
    "        \n",
    "    def load_model(self, env, file_name):\n",
    "        folder_name = self.policy + \"_save_models\"\n",
    "                \n",
    "        filename = folder_name + \"/\" + file_name + \".pth\"\n",
    "        self.create_network(env)\n",
    "        \n",
    "        models = torch.load(filename, map_location=self.device)\n",
    "        \n",
    "        self.q_net.load_state_dict(models['state_dict'])\n",
    "        self.optimizer.load_state_dict(models['optimizer'])\n",
    "    \n",
    "    def save_params(self, params):\n",
    "        folder_name = \"hyperparameters\"\n",
    "        self.create_folder(folder_name)\n",
    "        \n",
    "        file_name = f'./{folder_name}/{self.policy}_DQN_{self.time}'\n",
    "        with open(file_name + '.txt', 'w') as file:\n",
    "            file.write(json.dumps(str(params)))\n",
    "\n",
    "    def create_folder(self, directory_name):\n",
    "        try:\n",
    "            os.mkdir(directory_name)\n",
    "            print(f\"Directory '{directory_name}' created successfully.\")\n",
    "        except FileExistsError:\n",
    "            return\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: Unable to create '{directory_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "policy = \"CnnPolicy\"\n",
    "# policy = \"MlpPolicy\"\n",
    "\n",
    "if policy == \"CnnPolicy\":\n",
    "    config={\n",
    "        \"lanes_count\" : 3,\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 64),\n",
    "            \"stack_size\": 4,\n",
    "            \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion keep this conversion this is in the highway env page\n",
    "            \"scaling\": 1.75,\n",
    "        },\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"vehicles_count\": 5,\n",
    "            \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "            \"features_range\": {\n",
    "                \"x\": [-100, 100],\n",
    "                \"y\": [-100, 100],\n",
    "                \"vx\": [-20, 20],\n",
    "                \"vy\": [-20, 20]\n",
    "            },\n",
    "            \"absolute\": False,\n",
    "            \"order\": \"sorted\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefilling Memory : 100%|██████████| 1/1 [00:00<00:00,  6.23it/s]\n",
      "Training Model:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2473],\n",
      "         [0.2473],\n",
      "         [0.2473],\n",
      "         [0.0108],\n",
      "         [0.2473]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[3.7530e-11],\n",
      "         [8.7122e-01],\n",
      "         [4.4790e-09],\n",
      "         [4.5609e-06],\n",
      "         [1.2878e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[3.8895e-09],\n",
      "         [1.6337e-04],\n",
      "         [9.9984e-01],\n",
      "         [5.5553e-07],\n",
      "         [3.7967e-07]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[8.4658e-06],\n",
      "         [1.7700e-03],\n",
      "         [6.1924e-06],\n",
      "         [4.3407e-29],\n",
      "         [9.9822e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[1.2875e-15],\n",
      "         [1.2875e-15],\n",
      "         [1.2875e-15],\n",
      "         [0.0000e+00],\n",
      "         [1.0000e+00]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[3.2171e-10],\n",
      "         [9.8818e-01],\n",
      "         [6.1157e-03],\n",
      "         [2.8499e-03],\n",
      "         [2.8499e-03]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[1.5263e-06],\n",
      "         [6.0837e-12],\n",
      "         [8.9160e-04],\n",
      "         [0.0000e+00],\n",
      "         [9.9911e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[1.4973e-22],\n",
      "         [3.3333e-01],\n",
      "         [3.3333e-01],\n",
      "         [0.0000e+00],\n",
      "         [3.3333e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[4.5802e-10],\n",
      "         [4.5802e-10],\n",
      "         [1.1464e-12],\n",
      "         [7.2435e-08],\n",
      "         [1.0000e+00]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[2.5000e-01],\n",
      "         [2.5000e-01],\n",
      "         [4.4152e-14],\n",
      "         [2.5000e-01],\n",
      "         [2.5000e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[2.9106e-15],\n",
      "         [2.9106e-15],\n",
      "         [2.9106e-15],\n",
      "         [8.4252e-17],\n",
      "         [1.0000e+00]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[2.4994e-01],\n",
      "         [2.4994e-01],\n",
      "         [2.3778e-04],\n",
      "         [2.4994e-01],\n",
      "         [2.4994e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[4.4594e-03],\n",
      "         [4.4594e-03],\n",
      "         [9.8654e-01],\n",
      "         [8.3726e-05],\n",
      "         [4.4594e-03]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000e+00],\n",
      "         [3.3333e-01],\n",
      "         [8.8189e-37],\n",
      "         [3.3333e-01],\n",
      "         [3.3333e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[5.2921e-08],\n",
      "         [1.1759e-04],\n",
      "         [6.7370e-09],\n",
      "         [4.7751e-07],\n",
      "         [9.9988e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.2000],\n",
      "         [0.2000],\n",
      "         [0.2000],\n",
      "         [0.2000],\n",
      "         [0.2000]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.0000],\n",
      "         [0.3333],\n",
      "         [0.3333],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[4.6430e-13],\n",
      "         [4.3630e-03],\n",
      "         [3.5304e-12],\n",
      "         [1.5422e-08],\n",
      "         [9.9564e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.0000],\n",
      "         [0.2500]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[1.3205e-09],\n",
      "         [1.8750e-09],\n",
      "         [1.8750e-09],\n",
      "         [1.8750e-09],\n",
      "         [1.0000e+00]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.2000],\n",
      "         [0.2000],\n",
      "         [0.2000],\n",
      "         [0.2000],\n",
      "         [0.2000]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.0000],\n",
      "         [0.2500]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[1.8384e-07],\n",
      "         [9.6927e-01],\n",
      "         [1.0245e-02],\n",
      "         [1.0245e-02],\n",
      "         [1.0245e-02]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[5.9667e-16],\n",
      "         [4.1894e-13],\n",
      "         [4.1894e-13],\n",
      "         [3.5710e-09],\n",
      "         [1.0000e+00]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.0000],\n",
      "         [0.5000],\n",
      "         [0.0000],\n",
      "         [0.5000]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[6.1612e-11],\n",
      "         [1.9971e-05],\n",
      "         [5.5535e-07],\n",
      "         [1.7920e-07],\n",
      "         [9.9998e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.2000],\n",
      "         [0.2000],\n",
      "         [0.2000],\n",
      "         [0.2000],\n",
      "         [0.2000]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.0000],\n",
      "         [0.3333],\n",
      "         [0.3333],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[3.2728e-01],\n",
      "         [3.2728e-01],\n",
      "         [2.1851e-04],\n",
      "         [1.7937e-02],\n",
      "         [3.2728e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[6.2369e-12],\n",
      "         [9.9993e-01],\n",
      "         [6.6684e-05],\n",
      "         [4.8010e-07],\n",
      "         [4.8010e-07]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.3333],\n",
      "         [0.0000],\n",
      "         [0.3333],\n",
      "         [0.0000],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[4.0495e-13],\n",
      "         [4.0495e-13],\n",
      "         [4.0495e-13],\n",
      "         [4.0495e-13],\n",
      "         [1.0000e+00]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.3333],\n",
      "         [0.3333],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[3.9225e-09],\n",
      "         [1.5549e-05],\n",
      "         [7.1776e-05],\n",
      "         [1.5549e-05],\n",
      "         [9.9990e-01]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.5000],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.5000]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[4.1970e-04],\n",
      "         [9.5833e-01],\n",
      "         [3.8539e-02],\n",
      "         [7.7240e-04],\n",
      "         [1.9412e-03]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[4.6463e-04],\n",
      "         [4.6463e-04],\n",
      "         [9.9814e-01],\n",
      "         [4.6463e-04],\n",
      "         [4.6463e-04]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[1.9506e-03],\n",
      "         [9.9415e-01],\n",
      "         [9.4626e-09],\n",
      "         [1.9506e-03],\n",
      "         [1.9506e-03]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.3333],\n",
      "         [0.0000],\n",
      "         [0.3333],\n",
      "         [0.0000],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.1621e-17],\n",
      "         [9.0144e-09],\n",
      "         [9.4278e-12],\n",
      "         [8.5165e-13],\n",
      "         [1.0000e+00]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.0000],\n",
      "         [0.3333],\n",
      "         [0.3333],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.3333],\n",
      "         [0.3333],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.3333],\n",
      "         [0.0000],\n",
      "         [0.3333],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.0000],\n",
      "         [0.5000],\n",
      "         [0.0000],\n",
      "         [0.5000]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.3333],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.3333],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.0000],\n",
      "         [0.2500]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.0000],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.3333],\n",
      "         [0.0000],\n",
      "         [0.0000],\n",
      "         [0.3333],\n",
      "         [0.3333]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.2500],\n",
      "         [0.2500],\n",
      "         [0.2500],\n",
      "         [0.0000],\n",
      "         [0.2500]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[0.2500],\n",
      "         [0.2500],\n",
      "         [0.0000],\n",
      "         [0.2500],\n",
      "         [0.2500]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model:   0%|          | 0/10 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [381]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m dqn_agent\u001b[38;5;241m.\u001b[39mlearn(env)\n\u001b[1;32m     18\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighway-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [376]\u001b[0m, in \u001b[0;36mDQNAgent.evaluate\u001b[0;34m(self, env, episode_num)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated):  \n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# Select best action   \u001b[39;00m\n\u001b[1;32m    211\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action(state, eval_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 212\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:280\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    278\u001b[0m         frame \u001b[38;5;241m<\u001b[39m frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    279\u001b[0m     ):  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:340\u001b[0m, in \u001b[0;36mAbstractEnv._automatic_rendering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:303\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m EnvViewer(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39moffscreen:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/graphics.py:124\u001b[0m, in \u001b[0;36mEnvViewer.display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_surface\u001b[38;5;241m.\u001b[39mmove_display_window_to(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_position())\n\u001b[0;32m--> 124\u001b[0m \u001b[43mRoadGraphics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim_surface\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicle_trajectory:\n\u001b[1;32m    127\u001b[0m     VehicleGraphics\u001b[38;5;241m.\u001b[39mdisplay_trajectory(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicle_trajectory, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_surface, offscreen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffscreen\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/graphics.py:307\u001b[0m, in \u001b[0;36mRoadGraphics.display\u001b[0;34m(road, surface)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _to \u001b[38;5;129;01min\u001b[39;00m road\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgraph[_from]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m road\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgraph[_from][_to]:\n\u001b[0;32m--> 307\u001b[0m         \u001b[43mLaneGraphics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/graphics.py:145\u001b[0m, in \u001b[0;36mLaneGraphics.display\u001b[0;34m(cls, lane, surface)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m side \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mline_types[side] \u001b[38;5;241m==\u001b[39m LineType\u001b[38;5;241m.\u001b[39mSTRIPED:\n\u001b[0;32m--> 145\u001b[0m         \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstriped_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstripes_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mline_types[side] \u001b[38;5;241m==\u001b[39m LineType\u001b[38;5;241m.\u001b[39mCONTINUOUS:\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_curve(lane, surface, stripes_count, s0, side)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/graphics.py:176\u001b[0m, in \u001b[0;36mLaneGraphics.striped_line\u001b[0;34m(cls, lane, surface, stripes_count, longitudinal, side)\u001b[0m\n\u001b[1;32m    170\u001b[0m ends \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    171\u001b[0m     longitudinal\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39marange(stripes_count) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mSTRIPE_SPACING\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mSTRIPE_LENGTH\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    175\u001b[0m lats \u001b[38;5;241m=\u001b[39m [(side \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m*\u001b[39m lane\u001b[38;5;241m.\u001b[39mwidth_at(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m starts]\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_stripes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlane\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mends\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/graphics.py:253\u001b[0m, in \u001b[0;36mLaneGraphics.draw_stripes\u001b[0;34m(cls, lane, surface, starts, ends, lats)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(starts):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(starts[k] \u001b[38;5;241m-\u001b[39m ends[k]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mSTRIPE_LENGTH:\n\u001b[1;32m    250\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mline(\n\u001b[1;32m    251\u001b[0m             surface,\n\u001b[1;32m    252\u001b[0m             surface\u001b[38;5;241m.\u001b[39mWHITE,\n\u001b[0;32m--> 253\u001b[0m             (surface\u001b[38;5;241m.\u001b[39mvec2pix(\u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlats\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)),\n\u001b[1;32m    254\u001b[0m             (surface\u001b[38;5;241m.\u001b[39mvec2pix(lane\u001b[38;5;241m.\u001b[39mposition(ends[k], lats[k]))),\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;28mmax\u001b[39m(surface\u001b[38;5;241m.\u001b[39mpix(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mSTRIPE_WIDTH), \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    256\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/road/lane.py:199\u001b[0m, in \u001b[0;36mStraightLane.position\u001b[0;34m(self, longitudinal, lateral)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mposition\u001b[39m(\u001b[38;5;28mself\u001b[39m, longitudinal: \u001b[38;5;28mfloat\u001b[39m, lateral: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m--> 199\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[43mlongitudinal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection\u001b[49m\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;241m+\u001b[39m lateral \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection_lateral\n\u001b[1;32m    201\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'policy' : policy,\n",
    "    'episode_num' : 1,\n",
    "    'discount' : 0.2,\n",
    "    'batch_size' : 1,\n",
    "    'n_steps': 4,\n",
    "    'device' : torch.device(\"mps\"),\n",
    "    'memory_capacity' : 10000,\n",
    "    'timeout_minute': 15,\n",
    "    'use_metrics' : True,\n",
    "    'save_model': True,\n",
    "}\n",
    "\n",
    "dqn_agent = DQNAgent(params)\n",
    "env = gym.make('highway-fast-v0', render_mode='rgb_array', config=config)\n",
    "dqn_agent.learn(env)\n",
    "\n",
    "env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "dqn_agent.evaluate(env, 10)\n",
    "\n",
    "# if you wanna save a model again\n",
    "# dqn_agent.save_model(\"highway_dqn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/m_r158w50cqfn3ydymrpm7rc0000gn/T/ipykernel_78170/3158821895.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  models = torch.load(filename, map_location=self.device)\n",
      "Evaluating Model: 100%|██████████| 20/20 [00:51<00:00,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('highway-v0', render_mode='rgb_array', config=config)\n",
    "\n",
    "dqn_agent_test = DQNAgent(params)\n",
    "dqn_agent_test.load_model(env, \"DQN_2_2024-12-23|20:32:52.148162\")\n",
    "\n",
    "dqn_agent_test.evaluate(env, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b95d3aa0fdeff6c3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b95d3aa0fdeff6c3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6010;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir training_results --host localhost --port 6010"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
