{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import highway_env\n",
    "# import finite-mdp\n",
    "# from finite_mdp.envs import finite_mdp_env\n",
    "\n",
    "%load_ext tensorboard\n",
    "import sys\n",
    "from tqdm.notebook import trange\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# doesn't work cause not linux\n",
    "# !apt-get install -y xvfb ffmpeg\n",
    "# !git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "# !git clone https://github.com/eleurent/finite-mdp.git 2> /dev/null\n",
    "sys.path.insert(0, '/content/HighwayEnv/scripts/')\n",
    "# from utils import record_videos, show_videos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import highway_env\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from metrics import Metrics\n",
    "\n",
    "class ValueIteration():\n",
    "    def __init__(self, env, params):\n",
    "        self.finite_mdp = self.is_finite_mdp(env)\n",
    "        self.states = env.observation_space\n",
    "        if self.finite_mdp:\n",
    "            self.mpd = env.mdp\n",
    "        elif not self.finite_mdp:\n",
    "            try:\n",
    "                self.mdp = env.unwrapped.to_finite_mdp()\n",
    "            except AttributeError:\n",
    "                raise TypeError(\"not finite mdp\")\n",
    "            \n",
    "        self.env = env\n",
    "        self.obs, self.info = env.reset() \n",
    "        self.state_action_value = env.action_space\n",
    "        self.load_dictionary()\n",
    "        \n",
    "        use_metrics = params.get(\"use_metrics\", False)\n",
    "\n",
    "        self.gamma = params.get(\"gamma\", 0.9) # Discount Factor\n",
    "        self.episode_num = params.get(\"episode_num\", 100)\n",
    "        self.metrics = Metrics(\"value_iteration\", \"training_results\", use_metrics)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.episode_num), desc=\"Training Model\"):\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            episode_count = 0\n",
    "            episode_rewards = []\n",
    "\n",
    "            # Initialize the environment and get the initial state\n",
    "            self.obs = self.env.reset()  # Assuming `reset` initializes the environment\n",
    "            state = str(self.obs)\n",
    "\n",
    "            while not done:\n",
    "                # Select a random action\n",
    "                action = random.randrange(0, self.state_action_value.n)\n",
    "\n",
    "                # Take the action in the environment\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "                # Accumulate discounted rewards\n",
    "                total_reward += reward * (self.gamma ** episode_count)\n",
    "                episode_rewards.append(total_reward)\n",
    "                \n",
    "                episode_count += 1\n",
    "\n",
    "                # Update policy if the current action is better\n",
    "            if state not in self.policy or self.policy[state]['reward'] < total_reward:\n",
    "                self.policy[state] = {'action': action, 'reward': total_reward}            \n",
    "\n",
    "            self.save_policy()\n",
    "            \n",
    "            self.metrics.add(\"rollout/rewards\", sum(episode_rewards) / len(episode_rewards), epoch)\n",
    "            self.metrics.add(\"rollout/episode-length\", episode_count, epoch)\n",
    "            \n",
    "        self.metrics.close()\n",
    "        \n",
    "\n",
    "    def load_dictionary(self):\n",
    "        file_path = \"policies.json\"\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:  # Open in text mode for JSON\n",
    "                    self.policy = json.load(file)\n",
    "                    print(\"Dictionary loaded successfully.\")\n",
    "                    return True\n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                print(f\"Error loading dictionary: {e}\")\n",
    "                self.policy = {}  # Use a regular dictionary as fallback\n",
    "                return False\n",
    "        else:\n",
    "            self.policy = {}  # Use a regular dictionary if file doesn't exist\n",
    "            print(\"No existing policy found. Starting with an empty dictionary.\")\n",
    "            return False\n",
    "\n",
    "    def save_policy(self):\n",
    "        file_path = \"policies.json\"\n",
    "        try:\n",
    "            # Ensure self.policy is serializable\n",
    "            if isinstance(self.policy, defaultdict):\n",
    "                self.policy = dict(self.policy)  # Convert defaultdict to dict\n",
    "\n",
    "            with open(file_path, 'w') as file:  # Open in write mode\n",
    "                json.dump(self.policy, file, indent=4)  # Save with pretty printing\n",
    "                # print(\"Dictionary saved successfully.\")\n",
    "                return True\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving dictionary: {e}\")\n",
    "            return False\n",
    "                \n",
    "    def evaluate(self, env, episode_num):\n",
    "        for _ in range(episode_num):\n",
    "            state = env.reset()[0]  \n",
    "            done = False      \n",
    "            truncated = False \n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            print(self.policy)\n",
    "            while(not done and not truncated):  \n",
    "                try:   \n",
    "                    # Select best action\n",
    "                    action = self.policy[str(state)]\n",
    "                    # print(\"choosing best action\", )\n",
    "                except KeyError:\n",
    "                    action = random.randrange(0, env.action_space.n)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                state = next_state\n",
    "                env.render()\n",
    "\n",
    "    def is_finite_mdp(self,env):\n",
    "        try:\n",
    "            finite_mdp = __import__(\"finite_mdp.envs.finite_mdp_env\")\n",
    "            if isinstance(env.unwrapped, finite_mdp.envs.finite_mdp_env.FiniteMDPEnv):\n",
    "                return True\n",
    "        except (ModuleNotFoundError, TypeError):\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 10/10 [00:02<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"lanes_count\": 3,\n",
    "    \"observation\": {\n",
    "        \"type\": \"TimeToCollision\",\n",
    "        \"horizon\": 5,\n",
    "    }}\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\", config=config)\n",
    "\n",
    "params = {\n",
    "    \"use_metrics\": True,\n",
    "    \"episode_num\": 10,\n",
    "    \"gamma\": 0.9, # Discount Factor\n",
    "}\n",
    "\n",
    "finite_mdp = ValueIteration(env, params=params)\n",
    "finite_mdp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"highway-v0\", render_mode=\"rgb_array\",   config=config)\n",
    "finite_mdp.evaluate(env, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f5595f0209e8713c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f5595f0209e8713c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6012;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir training_results --host localhost --port 6012"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
