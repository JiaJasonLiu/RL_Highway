{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import highway_env\n",
    "# import finite-mdp\n",
    "# from finite_mdp.envs import finite_mdp_env\n",
    "\n",
    "%load_ext tensorboard\n",
    "import sys\n",
    "from tqdm.notebook import trange\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# doesn't work cause not linux\n",
    "# !apt-get install -y xvfb ffmpeg\n",
    "# !git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
    "# !git clone https://github.com/eleurent/finite-mdp.git 2> /dev/null\n",
    "sys.path.insert(0, '/content/HighwayEnv/scripts/')\n",
    "# from utils import record_videos, show_videos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, policy, result_file_name, use_metrics):\n",
    "        self.use_metrics = use_metrics\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        time = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        new_num = str(len(os.listdir(\"./\" +result_file_name)) + 1)\n",
    "        file_name = f'{result_file_name}/{policy}_{new_num}_{time}'\n",
    "        self.writer = SummaryWriter(log_dir=file_name, flush_secs=60)\n",
    "            \n",
    "    def add(self, type, y, x):\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        self.writer.add_scalar(type, y, x)\n",
    "    def close(self):\n",
    "        if not self.use_metrics:\n",
    "            return\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing policy found. Starting with an empty dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 13:22:51.195 python[23201:4136766] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-30 13:22:51.195 python[23201:4136766] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    130\u001b[0m finite_mdp \u001b[38;5;241m=\u001b[39m ValueIteration(env)\n\u001b[0;32m--> 131\u001b[0m \u001b[43mfinite_mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mValueIteration.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandrange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_action_value\u001b[38;5;241m.\u001b[39mn)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Take the action in the environment\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m next_obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Accumulate discounted rewards\u001b[39;00m\n\u001b[1;32m     56\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m*\u001b[39m (gamma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m episode_count)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:240\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:280\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# Automatically render intermediate simulation steps if a viewer has been launched\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Ignored if the rendering is done offscreen\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    278\u001b[0m         frame \u001b[38;5;241m<\u001b[39m frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    279\u001b[0m     ):  \u001b[38;5;66;03m# Last frame will be rendered through env.render() as usual\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_automatic_rendering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:340\u001b[0m, in \u001b[0;36mAbstractEnv._automatic_rendering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_video_wrapper\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/abstract.py:303\u001b[0m, in \u001b[0;36mAbstractEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m EnvViewer(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_auto_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39moffscreen:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39mhandle_events()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/highway_env/envs/common/graphics.py:160\u001b[0m, in \u001b[0;36mEnvViewer.display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal_time_rendering\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSAVE_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory:\n\u001b[1;32m    163\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_surface,\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirectory \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighway-env_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import highway_env\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\",  \n",
    "               config={\n",
    "                \"lanes_count\": 3,\n",
    "                \"observation\": {\n",
    "                    \"type\": \"TimeToCollision\",\n",
    "                    \"horizon\": 5,\n",
    "                }})\n",
    "\n",
    "class ValueIteration():\n",
    "    def __init__(self,env):\n",
    "        self.finite_mdp = self.is_finite_mdp(env)\n",
    "        self.states = env.observation_space\n",
    "        if self.finite_mdp:\n",
    "            self.mpd = env.mdp\n",
    "        elif not self.finite_mdp:\n",
    "            try:\n",
    "                self.mdp = env.unwrapped.to_finite_mdp()\n",
    "            except AttributeError:\n",
    "                raise TypeError(\"not finite mdp\")\n",
    "            \n",
    "        self.env = env\n",
    "        self.obs, self.info = env.reset() \n",
    "        self.state_action_value = env.action_space\n",
    "        self.load_dictionary()\n",
    "\n",
    "    def train(self):\n",
    "        gamma = 0.9  # Discount factor\n",
    "        episodes = 1000\n",
    "\n",
    "        for _ in range(episodes):  # Loop through each episode\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            episode_count = 0\n",
    "\n",
    "            # Initialize the environment and get the initial state\n",
    "            self.obs = self.env.reset()  # Assuming `reset` initializes the environment\n",
    "            state = str(self.obs)\n",
    "\n",
    "            while not done:\n",
    "                # Select a random action\n",
    "                action = random.randrange(0, self.state_action_value.n)\n",
    "\n",
    "                # Take the action in the environment\n",
    "                next_obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "                # Accumulate discounted rewards\n",
    "                total_reward += reward * (gamma ** episode_count)\n",
    "                episode_count += 1\n",
    "\n",
    "                # Update policy if the current action is better\n",
    "            if state not in self.policy or self.policy[state]['reward'] < total_reward:\n",
    "                self.policy[state] = {'action': action, 'reward': total_reward}            \n",
    "\n",
    "            self.save_policy()\n",
    "            \n",
    "            # metrics = getting the reward\n",
    "            # getting the episode count\n",
    "        \n",
    "\n",
    "    def load_dictionary(self):\n",
    "        file_path = \"policies.json\"\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:  # Open in text mode for JSON\n",
    "                    self.policy = json.load(file)\n",
    "                    print(\"Dictionary loaded successfully.\")\n",
    "                    return True\n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                print(f\"Error loading dictionary: {e}\")\n",
    "                self.policy = {}  # Use a regular dictionary as fallback\n",
    "                return False\n",
    "        else:\n",
    "            self.policy = {}  # Use a regular dictionary if file doesn't exist\n",
    "            print(\"No existing policy found. Starting with an empty dictionary.\")\n",
    "            return False\n",
    "\n",
    "    def save_policy(self):\n",
    "        file_path = \"policies.json\"\n",
    "        try:\n",
    "            # Ensure self.policy is serializable\n",
    "            if isinstance(self.policy, defaultdict):\n",
    "                self.policy = dict(self.policy)  # Convert defaultdict to dict\n",
    "\n",
    "            with open(file_path, 'w') as file:  # Open in write mode\n",
    "                json.dump(self.policy, file, indent=4)  # Save with pretty printing\n",
    "                # print(\"Dictionary saved successfully.\")\n",
    "                return True\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving dictionary: {e}\")\n",
    "            return False\n",
    "                \n",
    "    def evaluate(self, env, episode_num):\n",
    "        for _ in range(episode_num):\n",
    "            state = env.reset()[0]  \n",
    "            done = False      \n",
    "            truncated = False \n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            print(self.policy)\n",
    "            while(not done and not truncated):  \n",
    "                try:   \n",
    "                    # Select best action\n",
    "                    action = self.policy[str(state)]\n",
    "                    # print(\"choosing best action\", )\n",
    "                except KeyError:\n",
    "                    action = random.randrange(0, env.action_space.n)\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                state = next_state\n",
    "                env.render()\n",
    "\n",
    "    def is_finite_mdp(self,env):\n",
    "        try:\n",
    "            finite_mdp = __import__(\"finite_mdp.envs.finite_mdp_env\")\n",
    "            if isinstance(env.unwrapped, finite_mdp.envs.finite_mdp_env.FiniteMDPEnv):\n",
    "                return True\n",
    "        except (ModuleNotFoundError, TypeError):\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "finite_mdp = ValueIteration(env)\n",
    "finite_mdp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"highway-v0\", render_mode=\"rgb_array\",   config ={\n",
    "    \"lanes_count\": 3,\n",
    "                \"observation\": {\n",
    "    \"type\": \"TimeToCollision\",\n",
    "    \"horizon\": 5,\n",
    "}})\n",
    "finite_mdp.evaluate(env, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
