{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install environment and agent\n",
    "!pip install highway-env\n",
    "!pip install --upgrade sympy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers:\n",
    "[Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)\n",
    "\n",
    "[Towards Delivering a Coherent Self-Contained Explanation of Proximal Policy Optimization](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)\n",
    "\n",
    "### Tutorial\n",
    "[Hugging Face Deep RL Course: PROXIMAL POLICY OPTIMIZATION (PPO)](https://huggingface.co/learn/deep-rl-course/unit8/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def init_layer(layer, gaiN = np.sqrt(2)):\n",
    "  nn.init.orthogonal_(layer.weight, gain)\n",
    "  nn.init.constant_(layer.bias, 0)\n",
    "  return layer\n",
    "\n",
    "\n",
    "# Policy network (MLP)\n",
    "class MLPPolicyNetwork(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super(MLPPolicyNetwork, self).__init__()\n",
    "\n",
    "        # Actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            init_layer(nn.Linear(in_states, h1_nodes)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(h1_nodes, h1_nodes)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(h1_nodes, out_actions), std = 0.01)\n",
    "        )\n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            init_layer(nn.Linear(in_states, h1_nodes)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(h1_nodes, h1_nodes)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(h1_nodes, 1), std = 1.)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class CNNPolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CNNPolicyNetwork, self).__init__()\n",
    "        stack, height, width = input_shape\n",
    "\n",
    "        self.shared_conv = nn.Sequential(\n",
    "            nn.Conv2d(stack, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test = torch.zeros(1, stack, height, width)\n",
    "            find_conv_size = self.shared_conv(test)\n",
    "            conv_size = find_conv_size.numel()\n",
    "\n",
    "        # self.actor_fc = nn.Linear(conv_size, num_actions)\n",
    "        # self.critic_fc = nn.Linear(conv_size, 1)\n",
    "\n",
    "        self.actor_fc = init_layer(nn.Linear(conv_size, num_actions), gain = 0.01)\n",
    "        self.critic_fc = init_layer(nn.Linear(conv_size, 1), gain = 1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.shared_conv(x)\n",
    "        feats = torch.flatten(feats, start_dim=1)\n",
    "\n",
    "        logits = self.actor_fc(feats)\n",
    "        value = self.critic_fc(feats)\n",
    "\n",
    "        return logits, value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
