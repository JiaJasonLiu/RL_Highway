\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading rl_project.

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{rl_project}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{rl_project}
% \usepackage[numbers]{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Mastering the Highway using Rainbow DQN and PPO}


% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Jia Sheng Liu \\
  Department of Computer Science \\
  University of Bath \\
  Bath, BA2 7AY \\
  \texttt{jsl86@bath.ac.uk} \\
  \And
  Le Lyu \\
  Department of Computer Science \\
  University of Bath \\
  Bath, BA2 7AY \\
  \texttt{ll2300@bath.ac.uk} \\
  \And
  Zhe Yang \\
  Department of Computer Science \\
  University of Bath \\
  Bath, BA2 7AY \\
  \texttt{zy756@bath.ac.uk} \\
  \And
  Leonid Zhuravlev \\
  Department of Computer Science \\
  University of Bath \\
  Bath, BA2 7AY \\
  \texttt{lz2091@bath.ac.uk} \\
}

\begin{document}

\maketitle

% should be no longer than seven pages including figures but excluding references and appendices.

\section{Problem Definition}
% A clear, precise and concise description of your chosen problem, including the states, actions, transition dynamics, and the reward function. You will lose marks for an unclear, incorrect, or incomplete problem definition. You should also discuss the difficulty of your chosen problem and justify why it cannot be solved effectively using tabular reinforcement learning methods.


The Problem Definition we are evaluating is of a highway, where are car is driving and has the goal of driving as fast as possible for a set amount of time (10 seconds for now) without crashing to other cars. This is a continuous problem because the car positions on the highway will be random and will follow some rules of the highway, such as giving way. So, it will require non-tabular reinforcement learning methods to efficiently learn from the most optimal method to traverse this highway environment.

\begin{description}
\item[States]The states will be different combinations of the 4 lanes of the highway and the random number of cars per lane. Not including the agent car, the amount of cars per lane should be at most 2.

\item[Actions]The actions for any car on the highway environment is to stay in the same lane (idle), move left or right. Furthermore, continuous actions can be applied using kinematics where the speed of going left and right and the distance will also be taken into consideration when training the agent using non-tabular methods.

\item[Transition dynamics]The transition dynamics of the agent will be the probabilities of going to each of the lanes next to the agent, as well as staying in the same lane. Furthermore, their is also the probability of the agent to crash for each lane the agent is on.

\item[Reward function]As the goal of the agent is to be as fast as possible, the reward function will be negative rewards when the car crashes. However, the negative rewards need to be balanced so the agent isn't scared of going too fast because it is afraid of crashing. Moreover, the agent will be intrinsically motivated to get ahead as possible in the time it is given.
\end{description}

% \subsection{States}
% The states will be different combinations of the 4 lanes of the highway and the random number of cars per lane. Not including the agent car, the amount of cars per lane should be at most 2.

% \subsection{Actions}
% The actions for any car on the highway environment is to stay in the same lane (idle), move left or right. Furthermore, continuous actions can be applied using kinematics where the speed of going left and right and the distance will also be taken into consideration when training the agent using non-tabular methods.

% \subsection{Transition dynamics}
% The transition dynamics of the agent will be the probabilities of going to each of the lanes next to the agent, as well as staying in the same lane. Furthermore, their is also the probability of the agent to crash for each lane the agent is on.

% \subsection{Reward function}
% As the goal of the agent is to be as fast as possible, the reward function will be negative rewards when the car crashes. However, the negative rewards need to be balanced so the agent isn't scared of going too fast because it is afraid of crashing. Moreover, the agent will be intrinsically motivated to get ahead as possible in the time it is given.

\subsection{Difficulty and challenges}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}
% A discussion of reinforcement learning methods that may be effective at solving your chosen problem, their strengths and weaknesses for your chosen problem, and any existing results in the scientific literature (or publicly available online) on your chosen problem or similar problems.

\subsection{Existing Literature and Results}
Several reinforcement learning (RL) solutions have been implemented for highway driving tasks. In general, they can be divided into two main classifications: policy-based methods, which optimize the reward via gradient ascent \citep{sutton_policy_1999}, and value-based methods, which select the next action based on the cumulative benefit of each action. %might need one more citation here

\subsubsection{Policy-Based Methods}
\label{sec: related-work}
Policy-based methods, particularly those utilizing Proximal Policy Optimization (PPO), have emerged as a dominant solution due to their effectiveness in handling a wide range of challenging tasks and their simplicity in implementation and tuning \citep{schulman_proximal_2017}. 

A key advantage of PPO-based methods is their ability to integrate high-level decision making. For example, \citep{forneris_implementing_2023} used a customized reward function to penalize risky actions and generate different driving styles, including behaviors such as “keep lane,” “overtake,” and “move to the rightmost lane.” 

To simplify decision-making and make solutions more interpretable, these solutions often adopt a hierarchical architecture that separates the Decision Maker (DM) and the Behavior Executor (BE) \citep{pighetti_high-level_2022}, \citep{forneris_implementing_2023}, \citep{capello_investigating_2023}.
The Decision Maker focuses on selecting high-level strategic actions, such as “keep lane” or “overtake,” while the Behavior Executor translates these decisions into low-level actions controls. Hence, simplifies decision making and allows modularity.

However, while effective, these PPO-based solutions can be unnecessarily complex for simpler highway environments with discrete action spaces, such as the one in our chosen problem. Their performance often heavily relies on the definition of policies, which may limit adaptability. For example, the stabilizing decisions in \citep{pighetti_high-level_2022} may restrict the agent's ability to respond dynamically to changing traffic conditions. Similarly, fixed driving styles (e.g., “aggressive” or “comfort”) implemented in \citep{forneris_implementing_2023}, while realistic, may not align with our goal of developing an adaptive agent.

% Therefore we started from Monte Carlo and Value Iteration to find how agent perform with relatively simpler RL methods (?

% Could add one more paragraph mentioning SAC, same as DDPG, they are both actor-critic deep RL. Different from PPO which may lack of adaptive, actor-critic deep RL encourage the agent to take diverse actions. (then some how we say therefore we try DDPG with continuous actions space to see if an adaptive agent perform better...?

% Building upon these value-based RL methods, we also implemented a policy-based method to explore policy-based methods and broaden our solution’s scope. Different from value-based method, a policy-based method optimizes expected reward through gradient ascent, which provide .... Particularly, Proximal Policy Optimization (PPO) is selected to be implemented.

% PPO's recent successful applications in similar highway driving tasks (Section \ref{sec: related-work}) suggest that it is a reasonable candidate for our implementation. By introducing PPO, we sought to leverage its capacity for stable policy updates and explore its potential for achieving more adaptive behavior in dynamic traffic scenarios.

% Extension to solution
% Policy-based method: Optimize expected reward through gradient ascent
% Is implemented as an extension to value based solution, to explore policy-based methods and broaden our solution’s scope

%--------------------------------------------------------------------------------------------------------------
\subsubsection{Value-based methods}

Value-based methods, specifically Deep Q-Networks (DQN), are the primary solution for highway driving tasks. DQN estimates the action-value function (Q-values) to determine the optimal action for the agent at each state, making it particularly well-suited for environments with discrete action spaces.

A significant strength of DQN lies in its ability to handle complex decision-making through the use of attention mechanisms. For instance, \citep{leurent_social_2019} and \citep{bellotti_designing_2023} introduced techniques that prioritize vehicles most likely to influence the agent’s decisions.  

At the same time, DQN maintains a high degree of interpretability, which is crucial for understanding and analyzing the agent's behavior. \citep{bellotti_explaining_2023} employed SHapley Additive exPlanations (SHAP) to quantify the importance of environmental factors (such as vehicle speed, position, and distance) that influence specific decisions. Furthermore, \citep{bellotti_designing_2023} provided multiple visualization views for SHAP values. These tools enable a detailed analysis of the agent’s decision-making process across varying traffic conditions and timeframes.

While existing DQN provides a solid foundation, there remains significant room for improvement, particularly in enhancing learning efficiency and performance stability. 

% In "method" make connection to this part, explaining why DQN and rainbow DQN are implemented.


% To address these challenges, we aim to implement Rainbow DQN, which integrates several advanced techniques, such as prioritized replay, dueling networks, and distributional RL, to build upon the strengths of DQN. This will enable the agent to achieve better performance and faster convergence, particularly in scenarios with dynamic and dense highway environments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method (2 Page)}
% A description of the method(s) used to solve your chosen problem, an explanation of how these methods work (in your own words), and an explanation of why you chose these specific methods.

% Students should demonstrate a good understanding of their chosen method(s) and give reasonable justifications for their algorithmic choices. This should include demonstrating a reasonable appreciation of the strengths and weaknesses of alternative methods.(60\%)

% Students may show evidence of creativity in their algorithmic approach and analysis. Students should demonstrate a deep understanding of their chosen method(s), as well as possible alternative approaches. (70\%)
Talk about saving and loading of the model

The metrics are stored using tensorboard to keep track of the average rewards, exploration rate (epsilon), episode length and the average loss of the episode.
Moreover, due to the limitation of our personal computers, Google Colab was used to train the model.
As Google Colab can't render the environment for displaying purposes, saving and loading the parameters and buffers of the neural networks.
As the rendering of the model is done locally, loading of the model was done locally and the videos were recorded.


\subsection{Q-Learning}

Q -Learning is a type of reinforcement learning algorithm that guarantees to find an optimal policy in an environment if there is a finite number of states and actions. Incorporating action exploration allows us to investigate actions outside the policy to reach all possible states. 
=======
% Students should demonstrate a good understanding of their chosen method(s) and give reasonable justifications for their algorithmic choices. This should include demonstrating a reasonable appreciation of the strengths and weaknesses of alternative methods.(60\%)

% Students may show evidence of creativity in their algorithmic approach and analysis. Students should demonstrate a deep understanding of their chosen method(s), as well as possible alternative approaches. (70\%)


\subsection{Deep Q-Learning Network (DQN)}

% DQN used to solve your problem and how they work, and why I chose you it.

% good understanding of their chosen method(s)
% give reasonable justifications for their algorithmic choices. 
% reasonable appreciation of the strengths and weaknesses of alternative methods.(60\%)

% show evidence of creativity in their algorithmic approach and analysis.
% Demonstrate deep understanding of DQN and possible alternative approaches. 


% What is DQN and why is it used to solve the problem
DQN is a value-based deep reinforcement learning algorithm that combines Q-Learning with deep neural networks to solve complex environment, as the state space contains too many features that make it hard to handle using a tabular method.

% how it works

The use of a neural network takes the states and outputs the estimated Q-values of all possible actions given the state to providing the best action.

% methods
% the states in the environment


For the chosen problem, kinematics and gray-scale images were the two different observation spaces utilized~\href{https://highway-env.farama.org/observations/}{link}.

Kinematics observations are a 2D array of nearby vehicles and configurable features that describe the \(x, y\) position of the agent vehicle and other vehicles offset to the agent vehicle, \(x_{velocity}, y_{velocity}\) of the nearby vehicles.

As the observational states are of fixed size, a Multilayer Perceptron (MLP) neural network will process it to get the Q-values of the available actions to get the best action.
MLP's learn the nonlinear relationships between data as it uses multiple layers of neurons with non-linear activation functions (RelU) and is simple to implemented, making it ideal for the initial creation of DQN.
However, it performed worse than CNN (link to the appendix).
This might be due to the nearby vesicles being order dependent, making it less generalizable towards vehicles in different orders.

Therefore, Convolutional Neural Networks (CNN) was implemented more realistic and accurate representations of the state using grayscale images as the state in the environment. (link to appendix where we have the gray-scaled image of the thing)
Moreover, as the CNN normally takes channels/stacks of the image for \(r,g,b\), they were substituted with grayscale images of the 3 previous frames for more broader and accurate representation of the states.
Nevertheless, this increases computation time as it takes more dimensions and data compared to a 2D array, but is worth it for better accuracies.

There are two of the same network, training and target network with the same randomly initialized weights using the he initialization because of RelU.
For network inputs, DQN utilizes a circular memory replay buffer, which stores and learns from past experiences of the agent traversing through the environment.
This experience replay provides a equal distribution of all known states and reselect the best q-value actions for stable learning.
In order to obtain diverse states for the experience replay, epsilon greedy policy is implemented using epsilon-decay to decrease  \(\epsilon = 1 \) to \(0.1\) for the probability of selecting a random action or one from the training q network.
For epsilon decay, which is known to linearly decrease regardless of the total amount of episodes defined.
Linear annealing schedule was used for the decay as it calculates when the epsilon should reach its minimum using the total time steps.
For instance, for decay at .80, the agent will reach 0.1 at 80\% of training.

Furthermore, the agent learns through experience replay when it gets a batch of transitions to compare q values of the actions taken in the states using the training network with the best actions in the state using the target network.
The bellman equation then takes the The target network's q values to get the value of state based on the rewards of the action and the expected value of future states.
The mean square error loss (Huber Loss) then computes how far action taken is from the best action to update the q training network's parameters using gradient descent and backpropagation to update it weights depending on the loss.
Two networks are used because it is necessary for the training network to slowly improve its predictions on a fixed target network compared to chasing its own tail.
Therefore, the target network is updated to be the same as the training network only a few times during training
Furthermore, gradients were clipped to \([-1,1]\)~\cite{mnih_human-level_2015} for more stable training to prevent exploding gradients.


\subsection{Rainbow DQN}

Rainbow DQN refers to various extensions to overcome the shortcomings of DQN as it improves the accuracy, speed, and efficiency of the agent reaching the most optimal policy~\cite{hessel_rainbow_2017}.
The extensions N-Steps, Double, Noisy Network, Prioritized Experience Replay (PER) and Dueling Network were implemented.

N-steps is an extension to the Bellman update from the Replay Memory~\cite{garcia_understanding_2019}.
Depending on the n, we retrieve n states after the chosen one to capture the future information of the states and then aggregated those Q values into the selected state.
This increase the accuracy of the representation of the memory by reducing the variance and learns more about the states with less episodes.

Moreover, the max operator of the actions values tends to prefer overestimated (higher) values instead of the lower ones.
Therefore, it would continuously select the highest action value instead of getting the values of the best action, learning to a suboptimal policy.
Double DQN then takes the best actions from the trained network and uses it to get the probability (Q values) of that action in the target network loss to learn the best action in the state~\cite{hasselt_deep_2015}.
This will stop the target value when overestimating the action values by separating the action selection and evaluation.

The Noisy Network is a Network used for explorations instead of epsilon-greedy policy which is suitable for simpler environments. 
This paper~\cite{fortunato_noisy_2019}, adds noise to the weights (sigma and mu) to the weights of a network for the network to slowly adapt to the noisy over a period of time resulting in more adaptive exploration.
The intensity of the noise is the epsilon which is initialized in the beginning of the network and sampled using the Factorized Gaussian Distribution.
Put the equation here:

Noise was implemented using the two weights (sigma and mu) as two vectors being of the size of the input and output respectively.
Factorized is used to decrease the sample amount and also ensuring non-negative values to make it more scalable


PER is a new Memory Replay for storing and sampling memories based on there importance, which how much loss they generate.~\cite{schaul_prioritized_2016}.
Instead of randomly sampling experiences, PER is used to sample a proportion of the states based on how much it influences the agent.
The storage uses a Sum Segment Tree to store the indexes of the memories and their corresponding priorities to be retrieved.
Then the average sum of the priorities in the tree is used to get the index with the highest priority.
These indexes retrieve the transitions and its equivalent weights to how much should the loss be punished to be updated.
This utilizes importance sampling \((1/N * 1/P(i))^{\beta}\) to compensate for when beta is 1 for non-uniform probabilities.


Dueling Network consists of 2 additional layers that separate the estimation of the state value and the action advantage function~\cite{wang_dueling_2016}.
The action advantage function is the difference between the Q-values for a state.
The state value (V) and the advantage value (A) are then aggregated layer to learn the Q value of the state without considering its actions, and weighting how important it truly is.
I take the mean of the advantage layer to normalize the output so V(s) doesn't contain information about the action values.
The benefits during training is that because it uses the TD error, it separates updating the state value function and action advantage function to reordering the best actions.
Furthermore, only one output neural (the action chosen) is being updated at a time, so the separation leads to more stable learning and generalizability across different states and actions.

%################################################################################
\section{Proximal Policy Optimization (PPO)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Method (2 Page)
% A description of the method(s) used to solve your chosen problem, an explanation of how these methods work (in your own words), and an explanation of why you chose these specific methods.

% Students should demonstrate a good understanding of their chosen method(s) and give reasonable justifications for their algorithmic choices. This should include demonstrating a reasonable appreciation of the strengths and weaknesses of alternative methods.(60\%)

% Students may show evidence of creativity in their algorithmic approach and analysis. Students should demonstrate a deep understanding of their chosen method(s), as well as possible alternative approaches. (70\%)

\subsection{Policy Network}
Actor-Critic RL approaches are implemented in PPO \cite{konda_actor-critic_1999}. Two networks are trained concurrently: \texttt{Critic} learns to approximate the state value function; \texttt{Actor} accordingly maps from states to actions. The structures of two networks are different for different observations ( CNN policy and MLP policy). For MLP, both \texttt{critic} and \texttt{actor} are constructed with 3 fully connected layers. For CNN, the observation is first pass by a shared convolutional neural network to extract the feature of the image, and then passed to \texttt{critic} or \texttt{actor} where each is structured as a 1 full-connected layer. The structure of the shared convolutional neural network is the same as it was in DQN, as it has been adjust to have the best performance in understanding the CNN input. 

Hyperbolic Tangent (Tanh) was selected as the activation function after all layers in both policies. It normalize the outputs between $-1$ and $1$, which allows the network to have a more stable learning process and faster convergence. Also, layers are initiated  with orthogonal matrices for a stable gradient flow. Where the gain of that in \texttt{actor} is $0.01$ for ..., and $1$ in \texttt{critic} for ...

% PPO pseudocode
\subsection{Training}
Overall, the training process for PPO follows an iterative approach\cite{schulman_proximal_2017}. In each iteration, the current policy interacts with the environment to collect data over a fixed number of steps. Using these data, generalized advantage estimates are computed and used to optimize a surrogate loss function over multiple epochs. Minibatches was implemented for stable and efficient updates. After each update, the new policy replaces the old one and the process repeats for subsequent iterations. 

To collect data, a class \texttt{train\_data} is used to record all necessary data for each step, including state, the chosen action, value of the state, reward, log of probability, and whether the episode end.

\subsubsection{Generalized Advantage Estimation (GAE)}
Similar top Dueling Network in DQN, the advantage function is used to measure how much better or worse an action is compared to value of state, and it is generalized and normalized to balance bias and variance. Since policy are updated with minibatch, there exist situations that epoch ends with a non-terminal state of an episode. In this case, the value of the last state is estimated using the value network.

\subsubsection{Update Policy}
After that, GAE is then used to compute loss function to update policy network with ADAM optimizer. Clipped surrogate $L^{\text{CLIP}}(\theta)$ was selected to implement as it outperforms other policy gradient methods\cite{schulman_proximal_2017}. The $L^{\text{CLIP}}(\theta)$ aims at maximize the improvement, but limit the change to avoid bug change in one iteration:
\begin{equation}
    L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \, \text{clip} \left( r_t(\theta), 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \right]
\end{equation}

Where the ratio $r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_\text{old}}(a_t \mid s_t)}$ is the probability ratio between current and old policies for taking action $a_t$ in state $s_t$. Therefore, $r_t(\theta) \hat{A}_t$ reflects ... and hence maximize improvement. However, to avoid big policy change in one iteration, the ratio of GAE is clipped between $1 - \epsilon$ and $1 + \epsilon$, where $\epsilon$ is a hyperparameter.

\section{Results (1 Page)}
To evaluate the quantitative results, the tensorboard's results display how well the agent trains for the environment.

Display the different Extension of the Rainbow as well as MLP and CNN.

A presentation of your results, showing how quickly and how well your agent(s) learn (i.e. improve their policies).

nclude informative baselines for comparison 
best agent
human agent
random agent

% TODO: Have a table that shows the average rewards after certain amount of iterations:


\section{Discussion (< 2 Page)}
An evaluation of how well you solved your chosen problem.

evaluate the performance of their agent(s),and include comparisons with key baselines and some alternative approaches.(60\%)



perform an in-depth evaluation of their chosen method(s) and include comparisons to and evaluations of additional baselines and alternative methods.(70\%)

Monte Carlo

DQN
The implementation of Rainbow DQN showed that MLP wasn't the best policy since it didn't have as much information as with CNN.
Therefore, as most of the features were implemented using the CNN policy, it would result in a better outcome.

TODO: talk about the difference between the different stuff

PPO

Overall, it still does seem that this environment is still more suitable for humans to control as the human agent performed the best and got the best results.
So, it can be said that self-driving cars would require either as many senses as humans and be trained for tremendous amounts of times to reach a level that humans are at.




\section{Future Work (0.5 Page)}
A discussion of potential future work you would complete if you had more time.

Potential future work that could be completed with more time is to finish PPO and make it better than DQN as shown in some other results.



\section{Personal Experience (0.5 Page)}
A discussion of your personal experience with the project, such as difficulties or pleasant surprises you encountered while completing it.

There were various difficulties with the implementation of Rainbow DQN.
Firstly, the neural networks used (MLP and CNN) were initially used softmax when getting the probabilities, however this proved that large values for the CNN at the start of training proved that it would converge to a policy that would produce the same results as softmax makes the probabilities the same and would always choose the first one during argmax.
Therefore, to debug the neural network, the CNN was simplified and removed the softmax to fix issues regarding the actions chosen from the policy.

\newpage
\bibliographystyle{unsrt} 
\bibliography{references} 

% \normalsize
% \newpage
% \section*{Appendices}
% If you have additional content that you would like to include in the appendices, please do so here.
% There is no limit to the length of your appendices, but we are not obliged to read them in their entirety while marking. The main body of your report should contain all essential information, and the content in the appendices should be clearly referenced where it's needed elsewhere.
% \subsection*{Appendix A: Example Appendix 1}
% \subsection*{Appendix B: Example Appendix 2}

\end{document}
