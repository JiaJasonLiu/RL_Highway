\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading rl_project.

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{rl_project}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{rl_project}
% \usepackage[numbers]{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Mastering the Highway using Rainbow DQN and PPO}


% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Jia Sheng Liu \\
  Department of Computer Science \\
  University of Bath \\
  Bath, BA2 7AY \\
  \texttt{jsl86@bath.ac.uk} \\
  \And
  Le Lyu \\
  Department of Computer Science \\
  University of Bath \\
  Bath, BA2 7AY \\
  \texttt{ll2300@bath.ac.uk} \\
  \And
  Zhe Yang \\
  Department of Computer Science \\
  University of Bath \\
  Bath, BA2 7AY \\
  \texttt{zy756@bath.ac.uk} \\
  \And
  Leonid Zhuravlev \\
  Department of Computer Science \\
  University of Bath \\
  Bath, BA2 7AY \\
  \texttt{lz2091@bath.ac.uk} \\
}

\begin{document}

\maketitle

% should be no longer than seven pages including figures but excluding references and appendices.

\section{Problem Definition}
% A clear, precise and concise description of your chosen problem, including the states, actions, transition dynamics, and the reward function. You will lose marks for an unclear, incorrect, or incomplete problem definition. You should also discuss the difficulty of your chosen problem and justify why it cannot be solved effectively using tabular reinforcement learning methods.


The Problem Definition we are evaluating is of a highway, where are car is driving and has the goal of driving as fast as possible for a set amount of time (10 seconds for now) without crashing to other cars. This is a continuous problem because the car positions on the highway will be random and will follow some rules of the highway, such as giving way. So, it will require non-tabular reinforcement learning methods to efficiently learn from the most optimal method to traverse this highway environment.

\begin{description}
\item[States]The states will be different combinations of the 4 lanes of the highway and the random number of cars per lane. Not including the agent car, the amount of cars per lane should be at most 2.

\item[Actions]The actions for any car on the highway environment is to stay in the same lane (idle), move left or right. Furthermore, continuous actions can be applied using kinematics where the speed of going left and right and the distance will also be taken into consideration when training the agent using non-tabular methods.

\item[Transition dynamics]The transition dynamics of the agent will be the probabilities of going to each of the lanes next to the agent, as well as staying in the same lane. Furthermore, their is also the probability of the agent to crash for each lane the agent is on.

\item[Reward function]As the goal of the agent is to be as fast as possible, the reward function will be negative rewards when the car crashes. However, the negative rewards need to be balanced so the agent isn't scared of going too fast because it is afraid of crashing. Moreover, the agent will be intrinsically motivated to get ahead as possible in the time it is given.
\end{description}

% \subsection{States}
% The states will be different combinations of the 4 lanes of the highway and the random number of cars per lane. Not including the agent car, the amount of cars per lane should be at most 2.

% \subsection{Actions}
% The actions for any car on the highway environment is to stay in the same lane (idle), move left or right. Furthermore, continuous actions can be applied using kinematics where the speed of going left and right and the distance will also be taken into consideration when training the agent using non-tabular methods.

% \subsection{Transition dynamics}
% The transition dynamics of the agent will be the probabilities of going to each of the lanes next to the agent, as well as staying in the same lane. Furthermore, their is also the probability of the agent to crash for each lane the agent is on.

% \subsection{Reward function}
% As the goal of the agent is to be as fast as possible, the reward function will be negative rewards when the car crashes. However, the negative rewards need to be balanced so the agent isn't scared of going too fast because it is afraid of crashing. Moreover, the agent will be intrinsically motivated to get ahead as possible in the time it is given.

\subsection{Difficulty and challenges}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}
% A discussion of reinforcement learning methods that may be effective at solving your chosen problem, their strengths and weaknesses for your chosen problem, and any existing results in the scientific literature (or publicly available online) on your chosen problem or similar problems.

\subsection{Existing Literature and Results}
Several reinforcement learning (RL) solutions have been implemented for highway driving tasks. In general, they can be divided into two main classifications: policy-based methods, which optimize the reward via gradient ascent \citep{sutton_policy_1999}, and value-based methods, which select the next action based on the cumulative benefit of each action. %might need one more citation here

\subsubsection{Policy-Based Methods}
\label{sec: related-work}
Policy-based methods, particularly those utilizing Proximal Policy Optimization (PPO), have emerged as a dominant solution due to their effectiveness in handling a wide range of challenging tasks and their simplicity in implementation and tuning \citep{schulman_proximal_2017}. 

A key advantage of PPO-based methods is their ability to integrate high-level decision making. For example, \citep{forneris_implementing_2023} used a customized reward function to penalize risky actions and generate different driving styles, including behaviors such as “keep lane,” “overtake,” and “move to the rightmost lane.” 

To simplify decision-making and make solutions more interpretable, these solutions often adopt a hierarchical architecture that separates the Decision Maker (DM) and the Behavior Executor (BE) \citep{pighetti_high-level_2022}, \citep{forneris_implementing_2023}, \citep{capello_investigating_2023}.
The Decision Maker focuses on selecting high-level strategic actions, such as “keep lane” or “overtake,” while the Behavior Executor translates these decisions into low-level actions controls. Hence, simplifies decision making and allows modularity.

However, while effective, these PPO-based solutions can be unnecessarily complex for simpler highway environments with discrete action spaces, such as the one in our chosen problem. Their performance often heavily relies on the definition of policies, which may limit adaptability. For example, the stabilizing decisions in \citep{pighetti_high-level_2022} may restrict the agent's ability to respond dynamically to changing traffic conditions. Similarly, fixed driving styles (e.g., “aggressive” or “comfort”) implemented in \citep{forneris_implementing_2023}, while realistic, may not align with our goal of developing an adaptive agent.

% Therefore we started from Monte Carlo and Value Iteration to find how agent perform with relatively simpler RL methods (?

% Could add one more paragraph mentioning SAC, same as DDPG, they are both actor-critic deep RL. Different from PPO which may lack of adaptive, actor-critic deep RL encourage the agent to take diverse actions. (then some how we say therefore we try DDPG with continuous actions space to see if an adaptive agent perform better...?

% Building upon these value-based RL methods, we also implemented a policy-based method to explore policy-based methods and broaden our solution’s scope. Different from value-based method, a policy-based method optimizes expected reward through gradient ascent, which provide .... Particularly, Proximal Policy Optimization (PPO) is selected to be implemented.

% PPO's recent successful applications in similar highway driving tasks (Section \ref{sec: related-work}) suggest that it is a reasonable candidate for our implementation. By introducing PPO, we sought to leverage its capacity for stable policy updates and explore its potential for achieving more adaptive behavior in dynamic traffic scenarios.

% Extension to solution
% Policy-based method: Optimize expected reward through gradient ascent
% Is implemented as an extension to value based solution, to explore policy-based methods and broaden our solution’s scope

%--------------------------------------------------------------------------------------------------------------
\subsubsection{Value-based methods}

Value-based methods, specifically Deep Q-Networks (DQN), are the primary solution for highway driving tasks. DQN estimates the action-value function (Q-values) to determine the optimal action for the agent at each state, making it particularly well-suited for environments with discrete action spaces.

A significant strength of DQN lies in its ability to handle complex decision-making through the use of attention mechanisms. For instance, \citep{leurent_social_2019} and \citep{bellotti_designing_2023} introduced techniques that prioritize vehicles most likely to influence the agent’s decisions.  

At the same time, DQN maintains a high degree of interpretability, which is crucial for understanding and analyzing the agent's behavior. \citep{bellotti_explaining_2023} employed SHapley Additive exPlanations (SHAP) to quantify the importance of environmental factors (such as vehicle speed, position, and distance) that influence specific decisions. Furthermore, \citep{bellotti_designing_2023} provided multiple visualization views for SHAP values. These tools enable a detailed analysis of the agent’s decision-making process across varying traffic conditions and timeframes.

While existing DQN provides a solid foundation, there remains significant room for improvement, particularly in enhancing learning efficiency and performance stability. 

% In "method" make connection to this part, explaining why DQN and rainbow DQN are implemented.


% To address these challenges, we aim to implement Rainbow DQN, which integrates several advanced techniques, such as prioritized replay, dueling networks, and distributional RL, to build upon the strengths of DQN. This will enable the agent to achieve better performance and faster convergence, particularly in scenarios with dynamic and dense highway environments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Method (2 Page)
% A description of the method(s) used to solve your chosen problem, an explanation of how these methods work (in your own words), and an explanation of why you chose these specific methods.

% Students should demonstrate a good understanding of their chosen method(s) and give reasonable justifications for their algorithmic choices. This should include demonstrating a reasonable appreciation of the strengths and weaknesses of alternative methods.(60\%)

% Students may show evidence of creativity in their algorithmic approach and analysis. Students should demonstrate a deep understanding of their chosen method(s), as well as possible alternative approaches. (70\%)

\section{Deep Q-Learning Network (DQN)}

DQN used to solve your problem and how they work, and why I chose you it.

Students should demonstrate a good understanding of their chosen method(s) and give reasonable justifications for their algorithmic choices. 
This should include demonstrating a reasonable appreciation of the strengths and weaknesses of alternative methods.(60\%)

show evidence of creativity in their algorithmic approach and analysis.
Demonstrate deep understanding of DQN and possible alternative approaches. 

DQN is a value based deep reinforcement learning algorithm that combines Q-Learning with deep neural networks to solve our environment as the state space contains too many features that make it hard to handle using a tabular method.
Therefore a neural network generates an output of the estimated Q-values of all possible actions given the state.
For the chosen problem, there are two types of states that exist are Kinematics and Grey Scale image.

Kinematics refers to an array of nearby vehicles and their features. Some features are the \(x, y\) position of either the agent vehicle or other vehicles offset with the agent vehicle, \(x_{velocity}, y_{velocity}\) of the vehicles.
% info can be found: https://highway-env.farama.org/observations/#kinematics
A Multilayer perceptron (MLP) neural network will be used to take in this state and get the Q-values of the available actions.
A MLP can learn the nonlinear relationships between data as it uses multiple layers of neurons with non-linear activation functions (RelU).
Then using the Q-Values of the network we get the action with the highest Q-Value.
However, as the output is vehicle order dependent, it might not be able to generalize the network towards vehicles in different orders.

This is the reason for Grayscale Images, where we process the scenario of the environment with the road and the vehicles.
This provides a more realistic and accurate representations of the state.
Therefore, a Convolutional Neural Network (CNN) is used to process the grayscale image of the environment instead of getting the features of nearby vehicles.
Moreover, the CNN can also take channels of the image normally represented as \(r,g,b\), however as we are using grayscale we can use a set number of previous frames as the channels for a broader representation of the state.
Nonetheless, this will be computationally heavier as it will take more dimensions and more data than an array.

Furthermore, the inputs for the networks are the states.
DQN processes the states of the by first storing them into a memory replay circular buffer to be accessed after each time step.
This is so that it can have a equal distribution of all the states that the agent has seen, so that it wouldn't forget any previously seen states.
Nonetheless, the buffer still has a limit and so old states will be overwritten when new ones come in and the buffer is full.
As the agent needs to learn at the start of the training, the agent will traverse through the environment an just store the state relevant information in there.

As the agent learn from the memory replay it uses the Bellman equation on the target network versus the trained network for the best possible action against the action chosen and its discount value.
Denote with the equation (not sure if it should be put)
Then used the mean square error loss as the Huber Loss (maybe it should be L1 smooth loss) to get how far the best action is from the action chosen and update the q training network's parameters using gradient descent and backpropagation to output an action for similar states.
Furthermore, to prevent exploding gradients I have clipped the gradients to \([-1,1]\)~\cite{mnih_human-level_2015} for more stable training.



Epsilon-Greedy Policy:
This uses epsilon decay to slowly decrease epsilon which decreases the amount of randomness that the actor performs.
Instead of having a constant that goes down slowly regardless of the total amount of episodes defined where the epsilon might be big before ending the training.
Linear annealing schedule was used to decrease the epsilon depending on the total amount of episodes.
A float (0 to 1) will be assigned to decide if the actor should reach the minimum epsilon during 1 to 100\% of the training


Furthermore, the following extensions for Rainbow DQN were implemented based on the results it showed during the evaluation of the model and the metrics.
The metrics are stored using tensorboard to keep track of the average rewards, exploration rate (epsilon), episode length and the average loss of the episode.
Moreover, due to the limitation of our personal computers, Google Colab was used to train the model.
As Google Colab can't render the environment for displaying purposes, saving and loading the parameters and buffers of the neural networks.
As the rendering of the model is done locally, loading of the model was done locally and the videos were recorded.
I could also import a package to automatically record a video of the render, but do time and priorities, it wasn't achieved,

\section{Rainbow DQN}
Rainbow DQN refers to various extensions on top of the basic DQN to improve the accuracy, speed,  and efficiency of the agent to reach the most optimal policy~\cite{hessel_rainbow_2017}.
The extensions N-Steps, Double, Noisy Network, Prioritized Experience Replay (PER) and Dueling Network were implemented.

N-steps is an extension to the Bellman update from the Replay Memory~\cite{garcia_understanding_2019}.
Depending on the n, we retrieve n states after the chosen one to capture the future information of the states and then aggregated those Q values into the selected state.
This increase the accuracy of the representation of the memory by reducing the variance and learns more about the states with less episodes.

Moreover, the max operator of the actions values tends to prefer overestimated (higher) values instead of the lower ones.
Therefore, it would continuously select the highest action value instead of getting the values of the best action, learning to a suboptimal policy.
Double DQN then takes the best actions from the trained network and uses it to get the probability (Q values) of that action in the target network loss to learn the best action in the state~\cite{hasselt_deep_2015}.
This will stop the target value when overestimating the action values by separating the action selection and evaluation.

The Noisy Network is a Network or a Layer that is used for explorations instead of epsilon-greedy policy which is predetermined decrease of randomly selecting actions which is suitable for simpler environments.
This paper~\cite{fortunato_noisy_2019}, adds noise to the weights (sigma and mu) to the weights of a network for the network to slowly adapt to the noisy over a period of time resulting in more adaptive exploration.
The intensity of the noise is the epsilon which is initialized in the beginning of the network and sampled using the Factorized Gaussian Distribution.
Put the equation here:

Noise was implemented using the two weights (sigma and mu) as two vectors being of the size of the input and output respectively.
Factorized is used to decrease the sample amount and also ensuring non-negative values to make it more scalable


PER is a new Memory Replay for storing and sampling memories based on there importance, which how much loss they generate.~\cite{schaul_prioritized_2016}.
Instead of randomly sampling experiences, PER is used to sample a proportion of the states based on how much it influences the agent.
The storage uses a Sum Segment Tree to store the indexes of the memories and their corresponding priorities to be retrieved as the leafs and the parent nodes as the sums.
The leafs are stored as (2 * capacity) + index and then propagate upwards to update the sum all stored in an array.
Then the average sum of the priorities in the tree is used to get the index with the highest priority.
These indexes retrieve the transitions and its equivalent weights to how much should the loss be punished to be updated.
This utilizes importance sampling \((1/N * 1/P(i))^{\beta}\) to compensate for when beta is 1 for non-uniform probabilities.
The retrieval of the elements are based on dividing the index by two to get the sum of it.

Prioritized replay introduces bias because it changes this
distribution in an uncontrolled fashion, and therefore changes the solution that the estimates will
converge to (even if the policy and state distribution are fixed). 

Correct bias using importance-sampling (IS) 
hat fully compensates for the non-uniform probabilities \(P(i) if \beta = 1\). 
% These weights can be folded into the Q-learning update by using w_i * δ_i instead of δ_i (this is thus weighted IS, not ordinary IS, see e.g. Mahmood et al., 2014).

Dueling Network consists of 2 additional layers that separate the estimation of the state value and the action advantage function~\cite{wang_dueling_2016}.
The action advantage function is the difference between the Q-values for a state.
The state value (V) and the advantage value (A) are then aggregated layer to learn the Q value of the state without considering its actions, and weighting how important it truly is.
I take the mean of the advantage layer to normalize the output so V(s) doesn't contain information about the action values.
The benefits during training is that because it uses the TD error, it separates updating the state value function and action advantage function to reordering the best actions.
Furthermore, only one output neural (the action chosen) is being updated at a time, so the separation leads to more stable learning and generalizability across different states and actions.

%################################################################################
\section{Proximal Policy Optimization (PPO)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Method (2 Page)
% A description of the method(s) used to solve your chosen problem, an explanation of how these methods work (in your own words), and an explanation of why you chose these specific methods.

% Students should demonstrate a good understanding of their chosen method(s) and give reasonable justifications for their algorithmic choices. This should include demonstrating a reasonable appreciation of the strengths and weaknesses of alternative methods.(60\%)

% Students may show evidence of creativity in their algorithmic approach and analysis. Students should demonstrate a deep understanding of their chosen method(s), as well as possible alternative approaches. (70\%)

\subsection{Policy Network}
Actor-Critic RL approaches are implemented in PPO \cite{konda_actor-critic_1999}. Two networks are trained concurrently: \texttt{Critic} learns to approximate the state value function; \texttt{Actor} accordingly maps from states to actions. The structures of two networks are different for different observations ( CNN policy and MLP policy). For MLP, both \texttt{critic} and \texttt{actor} are constructed with 3 fully connected layers. For CNN, the observation is first pass by a shared convolutional neural network to extract the feature of the image, and then passed to \texttt{critic} or \texttt{actor} where each is structured as a 1 full-connected layer. The structure of the shared convolutional neural network is the same as it was in DQN, as it has been adjust to have the best performance in understanding the CNN input. 

Hyperbolic Tangent (Tanh) was selected as the activation function after all layers in both policies. It normalize the outputs between $-1$ and $1$, which allows the network to have a more stable learning process and faster convergence. Also, layers are initiated  with orthogonal matrices for a stable gradient flow. Where the gain of that in \texttt{actor} is $0.01$ for ..., and $1$ in \texttt{critic} for ...

% PPO pseudocode
\subsection{Training}
Overall, the training process for PPO follows an iterative approach\cite{schulman_proximal_2017}. In each iteration, the current policy interacts with the environment to collect data over a fixed number of steps. Using these data, generalized advantage estimates are computed and used to optimize a surrogate loss function over multiple epochs. Minibatches was implemented for stable and efficient updates. After each update, the new policy replaces the old one and the process repeats for subsequent iterations. 

To collect data, a class \texttt{train\_data} is used to record all necessary data for each step, including state, the chosen action, value of the state, reward, log of probability, and whether the episode end.

\subsubsection{Generalized Advantage Estimation (GAE)}
Similar top Dueling Network in DQN, the advantage function is used to measure how much better or worse an action is compared to value of state, and it is generalized and normalized to balance bias and variance. Since policy are updated with minibatch, there exist situations that epoch ends with a non-terminal state of an episode. In this situation, the value of the last state is estimated using the value network.

\subsubsection{Update Policy}


\section{Results (1 Page)}
To evaluate the quantitative results, the tensorboard's results display how well the agent trains for the environment.

Display the different Extension of the Rainbow as well as MLP and CNN.

A presentation of your results, showing how quickly and how well your agent(s) learn (i.e. improve their policies).

nclude informative baselines for comparison 
best agent
human agent
random agent

% TODO: Have a table that shows the average rewards after certain amount of iterations:


\section{Discussion (< 2 Page)}
An evaluation of how well you solved your chosen problem.

evaluate the performance of their agent(s),and include comparisons with key baselines and some alternative approaches.(60\%)



perform an in-depth evaluation of their chosen method(s) and include comparisons to and evaluations of additional baselines and alternative methods.(70\%)

Monte Carlo

DQN
The implementation of Rainbow DQN showed that MLP wasn't the best policy since it didn't have as much information as with CNN.
Therefore, as most of the features were implemented using the CNN policy, it would result in a better outcome.

TODO: talk about the difference between the different stuff

PPO

Overall, it still does seem that this environment is still more suitable for humans to control as the human agent performed the best and got the best results.
So, it can be said that self-driving cars would require either as many senses as humans and be trained for tremendous amounts of times to reach a level that humans are at.




\section{Future Work (0.5 Page)}
A discussion of potential future work you would complete if you had more time.

Potential future work that could be completed with more time is to finish PPO and make it better than DQN as shown in some other results.



\section{Personal Experience (0.5 Page)}
A discussion of your personal experience with the project, such as difficulties or pleasant surprises you encountered while completing it.

There were various difficulties with the implementation of Rainbow DQN.
Firstly, the neural networks used (MLP and CNN) were initially used softmax when getting the probabilities, however this proved that large values for the CNN at the start of training proved that it would converge to a policy that would produce the same results as softmax makes the probabilities the same and would always choose the first one during argmax.
Therefore, to debug the neural network, the CNN was simplified and removed the softmax to fix issues regarding the actions chosen from the policy.

\newpage
\bibliographystyle{unsrt} 
\bibliography{references} 

% \normalsize
% \newpage
% \section*{Appendices}
% If you have additional content that you would like to include in the appendices, please do so here.
% There is no limit to the length of your appendices, but we are not obliged to read them in their entirety while marking. The main body of your report should contain all essential information, and the content in the appendices should be clearly referenced where it's needed elsewhere.
% \subsection*{Appendix A: Example Appendix 1}
% \subsection*{Appendix B: Example Appendix 2}

\end{document}
