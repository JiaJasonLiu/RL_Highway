
@inproceedings{konda_actor-critic_1999,
	title = {Actor-{Critic} {Algorithms}},
	volume = {12},
	url = {https://proceedings.neurips.cc/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html},
	abstract = {We  propose  and  analyze  a  class  of  actor-critic  algorithms  for  simulation-based  optimization  of  a  Markov  decision  process  over  a  parameterized  family  of randomized  stationary  policies.  These  are two-time-scale  algorithms in  which  the critic uses TD learning  with  a  linear approximation architecture and the actor is  updated  in  an  approximate  gradient  direction  based  on  information  pro(cid:173) vided by the critic.  We  show that the features for  the critic should  span a subspace prescribed by the choice of parameterization of the  actor.  We  conclude by discussing convergence properties and some  open problems.},
	urldate = {2025-01-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Konda, Vijay and Tsitsiklis, John},
	year = {1999},
}

@misc{hessel_rainbow_2017,
	title = {Rainbow: {Combining} {Improvements} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Rainbow},
	url = {http://arxiv.org/abs/1710.02298},
	doi = {10.48550/arXiv.1710.02298},
	abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
	urldate = {2025-01-03},
	publisher = {arXiv},
	author = {Hessel, Matteo and Modayil, Joseph and Hasselt, Hado van and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	month = oct,
	year = {2017},
	note = {arXiv:1710.02298 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2025-01-03},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science},
	pages = {529--533},
}

@misc{schaul_prioritized_2016,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	doi = {10.48550/arXiv.1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = feb,
	year = {2016},
	note = {arXiv:1511.05952 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wang_dueling_2016,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.06581},
	doi = {10.48550/arXiv.1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado van and Lanctot, Marc and Freitas, Nando de},
	month = apr,
	year = {2016},
	note = {arXiv:1511.06581 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{fortunato_noisy_2019,
	title = {Noisy {Networks} for {Exploration}},
	url = {http://arxiv.org/abs/1706.10295},
	doi = {10.48550/arXiv.1706.10295},
	abstract = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and \${\textbackslash}epsilon\$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
	month = jul,
	year = {2019},
	note = {arXiv:1706.10295 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	doi = {10.48550/arXiv.1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv:1509.06461 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{hernandez-garcia_understanding_2019,
	title = {Understanding {Multi}-{Step} {Deep} {Reinforcement} {Learning}: {A} {Systematic} {Study} of the {DQN} {Target}},
	shorttitle = {Understanding {Multi}-{Step} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1901.07510},
	doi = {10.48550/arXiv.1901.07510},
	abstract = {Multi-step methods such as Retrace(\${\textbackslash}lambda\$) and \$n\$-step \$Q\$-learning have become a crucial component of modern deep reinforcement learning agents. These methods are often evaluated as a part of bigger architectures and their evaluations rarely include enough samples to draw statistically significant conclusions about their performance. This type of methodology makes it difficult to understand how particular algorithmic details of multi-step methods influence learning. In this paper we combine the \$n\$-step action-value algorithms Retrace, \$Q\$-learning, Tree Backup, Sarsa, and \$Q({\textbackslash}sigma)\$ with an architecture analogous to DQN. We test the performance of all these algorithms in the mountain car environment; this choice of environment allows for faster training times and larger sample sizes. We present statistical analyses on the effects of the off-policy correction, the backup length parameter \$n\$, and the update frequency of the target network on the performance of these algorithms. Our results show that (1) using off-policy correction can have an adverse effect on the performance of Sarsa and \$Q({\textbackslash}sigma)\$; (2) increasing the backup length \$n\$ consistently improved performance across all the different algorithms; and (3) the performance of Sarsa and \$Q\$-learning was more robust to the effect of the target network update frequency than the performance of Tree Backup, \$Q({\textbackslash}sigma)\$, and Retrace in this particular task.},
	urldate = {2025-01-02},
	publisher = {arXiv},
	author = {Hernandez-Garcia, J. Fernando and Sutton, Richard S.},
	month = feb,
	year = {2019},
	note = {arXiv:1901.07510 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lillicrap_continuous_2019,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	doi = {10.48550/arXiv.1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = jul,
	year = {2019},
	note = {arXiv:1509.02971 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{leurent_social_2019,
	title = {Social {Attention} for {Autonomous} {Decision}-{Making} in {Dense} {Traffic}},
	url = {http://arxiv.org/abs/1911.12250},
	doi = {10.48550/arXiv.1911.12250},
	abstract = {We study the design of learning architectures for behavioural planning in a dense traffic setting. Such architectures should deal with a varying number of nearby vehicles, be invariant to the ordering chosen to describe them, while staying accurate and compact. We observe that the two most popular representations in the literature do not fit these criteria, and perform badly on an complex negotiation task. We propose an attention-based architecture that satisfies all these properties and explicitly accounts for the existing interactions between the traffic participants. We show that this architecture leads to significant performance gains, and is able to capture interactions patterns that can be visualised and qualitatively interpreted. Videos and code are available at https://eleurent.github.io/social-attention/.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Leurent, Edouard and Mercat, Jean},
	month = nov,
	year = {2019},
	note = {arXiv:1911.12250 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bellotti_explaining_2023,
	title = {Explaining a {Deep} {Reinforcement} {Learning} ({DRL})-{Based} {Automated} {Driving} {Agent} in {Highway} {Simulations}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10077125/},
	doi = {10.1109/ACCESS.2023.3259544},
	urldate = {2024-12-17},
	journal = {IEEE Access},
	author = {Bellotti, Francesco and Lazzaroni, Luca and Capello, Alessio and Cossu, Marianna and De Gloria, Alessandro and Berta, Riccardo},
	year = {2023},
	pages = {28522--28550},
}

@inproceedings{bellotti_designing_2023,
	address = {Cham},
	title = {Designing an {Interpretability} {Analysis} {Framework} for {Deep} {Reinforcement} {Learning} ({DRL}) {Agents} in {Highway} {Automated} {Driving} {Simulation}},
	isbn = {978-3-031-26066-7},
	doi = {10.1007/978-3-031-26066-7_37},
	abstract = {Explainability is a key requirement for users to effectively understand, trust, and manage artificial intelligence applications, especially those concerning safety. We present the design of a framework aimed at supporting a quantitative explanation of the behavioural planning performed in automated driving (AD) highway simulations by a high-level decision making agent trained through deep reinforcement learning (DRL). The framework relies on the computation of SHAP values and keeps into consideration a neural architecture featuring an attention layer. The framework is particularly devoted to study the relationship between attention and interpretability, and how to represent, analyze and compare attention and SHAP values in a 2D spatial highway environment. The framework features three main visualization areas, that are obtained by processing quantities such as attention, SHAP values, vehicular observations: Episode view, plotting quantities on an episode’s timeline; Frame view, reporting the measurement values step by step; Aggregated view, showing, also on 2D maps, statistical values from the aggregation of several simulation episodes. To the best of our knowledge, this is the first in-depth explainability analysis of a DRL-based decision-making AD system, also including a statistical analysis. We hope that the upcoming open source release of the designed framework will be useful to further advance research in the field.},
	language = {en},
	booktitle = {Proceedings of {SIE} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Bellotti, Francesco and Lazzaroni, Luca and Capello, Alessio and Cossu, Marianna and De Gloria, Alessandro and Berta, Riccardo},
	editor = {Cocorullo, Giuseppe and Crupi, Felice and Limiti, Ernesto},
	year = {2023},
	pages = {239--244},
}

@misc{noauthor_abstract_nodate,
	title = {Abstract},
	url = {https://eleurent.github.io/social-attention/},
	abstract = {Edouard Leurent, Jean Mercat},
	language = {en-US},
	urldate = {2024-12-17},
	journal = {Social Attention for Autonomous Decision-Making in Dense Traffic},
}

@inproceedings{sutton_policy_1999,
	title = {Policy {Gradient} {Methods} for {Reinforcement} {Learning} with {Function} {Approximation}},
	volume = {12},
	url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html},
	abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
	urldate = {2024-12-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
	year = {1999},
}

@article{forneris_implementing_2023,
	title = {Implementing {Deep} {Reinforcement} {Learning} ({DRL})-based {Driving} {Styles} for {Non}-{Player} {Vehicles}},
	volume = {10},
	copyright = {Copyright (c) 2023 Luca Forneris, Alessandro Pighetti, Luca Lazzaroni, Francesco Bellotti, Alessio Capello, Marianna Cossu, Riccardo Berta},
	issn = {2384-8766},
	url = {https://journal.seriousgamessociety.org/index.php/IJSG/article/view/638},
	doi = {10.17083/ijsg.v10i4.638},
	abstract = {We propose a new, hierarchical architecture for behavioral planning of vehicle models usable as realistic non-player vehicles in serious games related to traffic and driving. These agents, trained with deep reinforcement learning (DRL), decide their motion by taking high-level decisions, such as “keep lane”, “overtake” and “go to rightmost lane”. This is similar to a driver’s high-level reasoning and takes into account the availability of advanced driving assistance systems (ADAS) in current vehicles. Compared to a low-level decision making system, our model performs better both in terms of safety and speed. As a significant advantage, the proposed approach allows to reduce the number of training steps by more than one order of magnitude. This makes the development of new models much more efficient, which is key for implementing vehicles featuring different driving styles. We also demonstrate that, by simply tweaking the reinforcement learning (RL) reward function, it is possible to train agents characterized by different driving behaviors. We also employed the continual learning technique, starting the training procedure of a more specialized agent from a base model. This allowed significantly to reduce the number of training steps while keeping similar vehicular performance figures. However, the characteristics of the specialized agents are deeply influenced by the characteristics of the baseline agent.
\&nbsp;},
	language = {en},
	number = {4},
	urldate = {2024-12-17},
	journal = {International Journal of Serious Games},
	author = {Forneris, Luca and Pighetti, Alessandro and Lazzaroni, Luca and Bellotti, Francesco and Capello, Alessio and Cossu, Marianna and Berta, Riccardo},
	month = nov,
	year = {2023},
	note = {Number: 4},
	keywords = {Automotive Driving, Autonomous Agents, Decision Making, Driving Styles, PPO, Racing Games, Reinforcement Learning, Serious Games},
	pages = {153--170},
}

@inproceedings{pighetti_investigating_2024,
	address = {Cham},
	title = {Investigating {Adversarial} {Policy} {Learning} for {Robust} {Agents} in {Automated} {Driving} {Highway} {Simulations}},
	isbn = {978-3-031-48121-5},
	doi = {10.1007/978-3-031-48121-5_18},
	abstract = {This research explores an emerging approach, the adversarial policy learning paradigm, that aims to increase safety and robustness in deep reinforcement learning models for automated driving. We propose an iterative procedure to train an adversarial agent acting in a highway-simulated environment to attack a victim agent that is to be improved. Each training iteration consists of two phases. The adversarial agent is first trained to disrupt the victim-agent policy. The victim model is then trained to overcome the defects observed by the attack from the adversarial agent. The experimental results demonstrate that the victim agent trained with adversarial attacks outperforms the original agent.},
	language = {en},
	booktitle = {Applications in {Electronics} {Pervading} {Industry}, {Environment} and {Society}},
	publisher = {Springer Nature Switzerland},
	author = {Pighetti, Alessandro and Bellotti, Francesco and Oh, Changjae and Lazzaroni, Luca and Forneris, Luca and Fresta, Matteo and Berta, Riccardo},
	editor = {Bellotti, Francesco and Grammatikakis, Miltos D. and Mansour, Ali and Ruo Roch, Massimo and Seepold, Ralf and Solanas, Agusti and Berta, Riccardo},
	year = {2024},
	pages = {124--129},
}

@inproceedings{capello_investigating_2023,
	address = {Cham},
	title = {Investigating {High}-{Level} {Decision} {Making} for {Automated} {Driving}},
	isbn = {978-3-031-30333-3},
	doi = {10.1007/978-3-031-30333-3_41},
	abstract = {As the quality of perception systems available for automated driving (AD) increases, we investigate the development of an AD agent based on Reinforcement Learning which exploits underlying systems for longitudinal and lateral control. The goal is addressed by designing high-level actions, trying to imitate the commands of a real driver. The proposed agent is trained in a simulated motorway environment and compared to an agent which outputs low-level actions. Our preliminary results show similar performance results, a more pronounced human-like behaviour and a huge reduction in needed training time because of the higher-level of the available actions.},
	language = {en},
	booktitle = {Applications in {Electronics} {Pervading} {Industry}, {Environment} and {Society}},
	publisher = {Springer Nature Switzerland},
	author = {Capello, Alessio and Forneris, Luca and Pighetti, Alessandro and Bellotti, Francesco and Lazzaroni, Luca and Cossu, Marianna and De Gloria, Alessandro and Berta, Riccardo},
	editor = {Berta, Riccardo and De Gloria, Alessandro},
	year = {2023},
	pages = {307--311},
}

@inproceedings{pighetti_high-level_2022,
	address = {Cham},
	title = {High-{Level} {Decision}-{Making} {Non}-player {Vehicles}},
	isbn = {978-3-031-22124-8},
	doi = {10.1007/978-3-031-22124-8_22},
	abstract = {Availability of realistic driver models, also able to represent various driving styles, is key to add traffic in serious games on automotive driving. We propose a new architecture for behavioural planning of vehicles, that decide their motion taking high-level decisions, such as “keep lane”, “overtake” and “go to rightmost lane”. This is similar to a driver’s high-level reasoning and takes into account the availability of ever more sophisticated Advanced Driving Assistance Systems (ADAS) in current vehicles. Compared to a low-level decision making system, our model performs better both in terms of safety and average speed. As a significant advantage, the hierarchical approach allows to reduce the number of training steps, which is critical for ML models, by more than one order of magnitude. The developed agent seems to show a more realistic behaviour. We also showed feasibility of training models able to differentiate their performance in a way similar to the driving styles. We believe that such agents could be profitably employed in state of the art SGs for driving, improving the realism of single NPVs and overall traffic.},
	language = {en},
	booktitle = {Games and {Learning} {Alliance}},
	publisher = {Springer International Publishing},
	author = {Pighetti, Alessandro and Forneris, Luca and Lazzaroni, Luca and Bellotti, Francesco and Capello, Alessio and Cossu, Marianna and De Gloria, Alessandro and Berta, Riccardo},
	editor = {Kiili, Kristian and Antti, Koskinen and de Rosa, Francesca and Dindar, Muhterem and Kickmeier-Rust, Michael and Bellotti, Francesco},
	year = {2022},
	pages = {223--233},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{leurent_environment_2018,
	title = {An {Environment} for {Autonomous} {Driving} {Decision}-{Making}},
	url = {https://github.com/eleurent/highway-env},
	publisher = {GitHub},
	author = {Leurent, Edouard},
	year = {2018},
	note = {Publication Title: GitHub repository},
}
