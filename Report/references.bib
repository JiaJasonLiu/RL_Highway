
@misc{leurent_social_2019,
	title = {Social {Attention} for {Autonomous} {Decision}-{Making} in {Dense} {Traffic}},
	url = {http://arxiv.org/abs/1911.12250},
	doi = {10.48550/arXiv.1911.12250},
	abstract = {We study the design of learning architectures for behavioural planning in a dense traffic setting. Such architectures should deal with a varying number of nearby vehicles, be invariant to the ordering chosen to describe them, while staying accurate and compact. We observe that the two most popular representations in the literature do not fit these criteria, and perform badly on an complex negotiation task. We propose an attention-based architecture that satisfies all these properties and explicitly accounts for the existing interactions between the traffic participants. We show that this architecture leads to significant performance gains, and is able to capture interactions patterns that can be visualised and qualitatively interpreted. Videos and code are available at https://eleurent.github.io/social-attention/.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Leurent, Edouard and Mercat, Jean},
	month = nov,
	year = {2019},
	note = {arXiv:1911.12250 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bellotti_explaining_2023,
	title = {Explaining a {Deep} {Reinforcement} {Learning} ({DRL})-{Based} {Automated} {Driving} {Agent} in {Highway} {Simulations}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10077125/},
	doi = {10.1109/ACCESS.2023.3259544},
	urldate = {2024-12-17},
	journal = {IEEE Access},
	author = {Bellotti, Francesco and Lazzaroni, Luca and Capello, Alessio and Cossu, Marianna and De Gloria, Alessandro and Berta, Riccardo},
	year = {2023},
	pages = {28522--28550},
}

@misc{noauthor_explaining_nodate,
	title = {Explaining a {Deep} {Reinforcement} {Learning} ({DRL})-{Based} {Automated} {Driving} {Agent} in {Highway} {Simulations} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/10077125},
	urldate = {2024-12-17},
}

@inproceedings{bellotti_designing_2023,
	address = {Cham},
	title = {Designing an {Interpretability} {Analysis} {Framework} for {Deep} {Reinforcement} {Learning} ({DRL}) {Agents} in {Highway} {Automated} {Driving} {Simulation}},
	isbn = {978-3-031-26066-7},
	doi = {10.1007/978-3-031-26066-7_37},
	abstract = {Explainability is a key requirement for users to effectively understand, trust, and manage artificial intelligence applications, especially those concerning safety. We present the design of a framework aimed at supporting a quantitative explanation of the behavioural planning performed in automated driving (AD) highway simulations by a high-level decision making agent trained through deep reinforcement learning (DRL). The framework relies on the computation of SHAP values and keeps into consideration a neural architecture featuring an attention layer. The framework is particularly devoted to study the relationship between attention and interpretability, and how to represent, analyze and compare attention and SHAP values in a 2D spatial highway environment. The framework features three main visualization areas, that are obtained by processing quantities such as attention, SHAP values, vehicular observations: Episode view, plotting quantities on an episode’s timeline; Frame view, reporting the measurement values step by step; Aggregated view, showing, also on 2D maps, statistical values from the aggregation of several simulation episodes. To the best of our knowledge, this is the first in-depth explainability analysis of a DRL-based decision-making AD system, also including a statistical analysis. We hope that the upcoming open source release of the designed framework will be useful to further advance research in the field.},
	language = {en},
	booktitle = {Proceedings of {SIE} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Bellotti, Francesco and Lazzaroni, Luca and Capello, Alessio and Cossu, Marianna and De Gloria, Alessandro and Berta, Riccardo},
	editor = {Cocorullo, Giuseppe and Crupi, Felice and Limiti, Ernesto},
	year = {2023},
	pages = {239--244},
}

@misc{noauthor_abstract_nodate,
	title = {Abstract},
	url = {https://eleurent.github.io/social-attention/},
	abstract = {Edouard Leurent, Jean Mercat},
	language = {en-US},
	urldate = {2024-12-17},
	journal = {Social Attention for Autonomous Decision-Making in Dense Traffic},
}

@inproceedings{sutton_policy_1999,
	title = {Policy {Gradient} {Methods} for {Reinforcement} {Learning} with {Function} {Approximation}},
	volume = {12},
	url = {https://proceedings.neurips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html},
	abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
	urldate = {2024-12-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
	year = {1999},
}

@article{forneris_implementing_2023,
	title = {Implementing {Deep} {Reinforcement} {Learning} ({DRL})-based {Driving} {Styles} for {Non}-{Player} {Vehicles}},
	volume = {10},
	copyright = {Copyright (c) 2023 Luca Forneris, Alessandro Pighetti, Luca Lazzaroni, Francesco Bellotti, Alessio Capello, Marianna Cossu, Riccardo Berta},
	issn = {2384-8766},
	url = {https://journal.seriousgamessociety.org/index.php/IJSG/article/view/638},
	doi = {10.17083/ijsg.v10i4.638},
	abstract = {We propose a new, hierarchical architecture for behavioral planning of vehicle models usable as realistic non-player vehicles in serious games related to traffic and driving. These agents, trained with deep reinforcement learning (DRL), decide their motion by taking high-level decisions, such as “keep lane”, “overtake” and “go to rightmost lane”. This is similar to a driver’s high-level reasoning and takes into account the availability of advanced driving assistance systems (ADAS) in current vehicles. Compared to a low-level decision making system, our model performs better both in terms of safety and speed. As a significant advantage, the proposed approach allows to reduce the number of training steps by more than one order of magnitude. This makes the development of new models much more efficient, which is key for implementing vehicles featuring different driving styles. We also demonstrate that, by simply tweaking the reinforcement learning (RL) reward function, it is possible to train agents characterized by different driving behaviors. We also employed the continual learning technique, starting the training procedure of a more specialized agent from a base model. This allowed significantly to reduce the number of training steps while keeping similar vehicular performance figures. However, the characteristics of the specialized agents are deeply influenced by the characteristics of the baseline agent.
\&nbsp;},
	language = {en},
	number = {4},
	urldate = {2024-12-17},
	journal = {International Journal of Serious Games},
	author = {Forneris, Luca and Pighetti, Alessandro and Lazzaroni, Luca and Bellotti, Francesco and Capello, Alessio and Cossu, Marianna and Berta, Riccardo},
	month = nov,
	year = {2023},
	note = {Number: 4},
	keywords = {Automotive Driving, Autonomous Agents, Decision Making, Driving Styles, PPO, Racing Games, Reinforcement Learning, Serious Games},
	pages = {153--170},
}

@inproceedings{pighetti_investigating_2024,
	address = {Cham},
	title = {Investigating {Adversarial} {Policy} {Learning} for {Robust} {Agents} in {Automated} {Driving} {Highway} {Simulations}},
	isbn = {978-3-031-48121-5},
	doi = {10.1007/978-3-031-48121-5_18},
	abstract = {This research explores an emerging approach, the adversarial policy learning paradigm, that aims to increase safety and robustness in deep reinforcement learning models for automated driving. We propose an iterative procedure to train an adversarial agent acting in a highway-simulated environment to attack a victim agent that is to be improved. Each training iteration consists of two phases. The adversarial agent is first trained to disrupt the victim-agent policy. The victim model is then trained to overcome the defects observed by the attack from the adversarial agent. The experimental results demonstrate that the victim agent trained with adversarial attacks outperforms the original agent.},
	language = {en},
	booktitle = {Applications in {Electronics} {Pervading} {Industry}, {Environment} and {Society}},
	publisher = {Springer Nature Switzerland},
	author = {Pighetti, Alessandro and Bellotti, Francesco and Oh, Changjae and Lazzaroni, Luca and Forneris, Luca and Fresta, Matteo and Berta, Riccardo},
	editor = {Bellotti, Francesco and Grammatikakis, Miltos D. and Mansour, Ali and Ruo Roch, Massimo and Seepold, Ralf and Solanas, Agusti and Berta, Riccardo},
	year = {2024},
	pages = {124--129},
}

@inproceedings{capello_investigating_2023,
	address = {Cham},
	title = {Investigating {High}-{Level} {Decision} {Making} for {Automated} {Driving}},
	isbn = {978-3-031-30333-3},
	doi = {10.1007/978-3-031-30333-3_41},
	abstract = {As the quality of perception systems available for automated driving (AD) increases, we investigate the development of an AD agent based on Reinforcement Learning which exploits underlying systems for longitudinal and lateral control. The goal is addressed by designing high-level actions, trying to imitate the commands of a real driver. The proposed agent is trained in a simulated motorway environment and compared to an agent which outputs low-level actions. Our preliminary results show similar performance results, a more pronounced human-like behaviour and a huge reduction in needed training time because of the higher-level of the available actions.},
	language = {en},
	booktitle = {Applications in {Electronics} {Pervading} {Industry}, {Environment} and {Society}},
	publisher = {Springer Nature Switzerland},
	author = {Capello, Alessio and Forneris, Luca and Pighetti, Alessandro and Bellotti, Francesco and Lazzaroni, Luca and Cossu, Marianna and De Gloria, Alessandro and Berta, Riccardo},
	editor = {Berta, Riccardo and De Gloria, Alessandro},
	year = {2023},
	pages = {307--311},
}

@inproceedings{pighetti_high-level_2022,
	address = {Cham},
	title = {High-{Level} {Decision}-{Making} {Non}-player {Vehicles}},
	isbn = {978-3-031-22124-8},
	doi = {10.1007/978-3-031-22124-8_22},
	abstract = {Availability of realistic driver models, also able to represent various driving styles, is key to add traffic in serious games on automotive driving. We propose a new architecture for behavioural planning of vehicles, that decide their motion taking high-level decisions, such as “keep lane”, “overtake” and “go to rightmost lane”. This is similar to a driver’s high-level reasoning and takes into account the availability of ever more sophisticated Advanced Driving Assistance Systems (ADAS) in current vehicles. Compared to a low-level decision making system, our model performs better both in terms of safety and average speed. As a significant advantage, the hierarchical approach allows to reduce the number of training steps, which is critical for ML models, by more than one order of magnitude. The developed agent seems to show a more realistic behaviour. We also showed feasibility of training models able to differentiate their performance in a way similar to the driving styles. We believe that such agents could be profitably employed in state of the art SGs for driving, improving the realism of single NPVs and overall traffic.},
	language = {en},
	booktitle = {Games and {Learning} {Alliance}},
	publisher = {Springer International Publishing},
	author = {Pighetti, Alessandro and Forneris, Luca and Lazzaroni, Luca and Bellotti, Francesco and Capello, Alessio and Cossu, Marianna and De Gloria, Alessandro and Berta, Riccardo},
	editor = {Kiili, Kristian and Antti, Koskinen and de Rosa, Francesca and Dindar, Muhterem and Kickmeier-Rust, Michael and Bellotti, Francesco},
	year = {2022},
	pages = {223--233},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{leurent_environment_2018,
	title = {An {Environment} for {Autonomous} {Driving} {Decision}-{Making}},
	url = {https://github.com/eleurent/highway-env},
	publisher = {GitHub},
	author = {Leurent, Edouard},
	year = {2018},
	note = {Publication Title: GitHub repository},
}
